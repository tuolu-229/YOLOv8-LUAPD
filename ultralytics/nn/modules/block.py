# Ultralytics YOLO ğŸš€, AGPL-3.0 license
"""
Block modules
"""
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

from ultralytics.nn.modules.conv import Conv, DWConv, GhostConv, LightConv, RepConv
from ultralytics.nn.modules.transformer import TransformerBlock

from einops import rearrange
from ultralytics.nn.modules.attention import *
import numpy as np
from ultralytics.nn.modules.orepa import OREPA, OREPA_LargeConv, RepVGGBlock_OREPA
import torchvision.ops

#from torchvision.ops import deform_conv2d

__all__ = ('DFL', 'HGBlock', 'HGStem', 'SPP', 'SPPF', 'C1', 'C2', 'C3', 'C2f', 'C3x', 'C3TR', 'C3Ghost',
           'GhostBottleneck', 'Bottleneck', 'BottleneckCSP', 'Proto', 'RepC3', 'LAWDS', 'RFAConv2', 'RFCAConv',
           'Fusion', 'C2f_SCConv', 'SCConv', 'C2f_DCNv2', 'DCNv2', 'CSPStage', 'DCNv2_Dynamic', 'C2f_DCNv2_Dynamic',
           'C2f_FocusedLinearAttention', 'RepBlock', 'BiFusion',
           # ä¸‹é¢ä¸€è¡Œæ˜¯gold-yoloçš„ç»“æ„
           'SimFusion_3in', 'SimFusion_4in', 'IFM','InjectionMultiSum_Auto_pool', 'PyramidPoolAgg', 'AdvPoolFusion', 'TopBasicLayer',
           'ELAN_OPERA', 'C2f_OREPA', 'C2f_test', 'RepNCSPELAN4','SPDConv','CBFuse','CBLinear','Silence',
           'ContextGuidedBlock_Down','ADown', 'V7DownSampling', 'ScConv', 'C2f_ScConv', 'down_sample','EMSConv',
           'C2f_EMSCP','C2f_EMSC','EMSConv_down', 'DGCST', 'CGNet_GELAN', 'MSBlock','ResNet_RepNCSPELAN4')


class DFL(nn.Module):
    """
    Integral module of Distribution Focal Loss (DFL).
    Proposed in Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391
    """

    # è¾“å…¥é€šé“æ•°c1ï¼Œé»˜è®¤ä¸º16
    def __init__(self, c1=16):
        """Initialize a convolutional layer with a given number of input channels."""
        super().__init__()
        self.conv = nn.Conv2d(c1, 1, 1, bias=False).requires_grad_(False)   # è¾“å…¥é€šé“æ•°c1ï¼Œè¾“å‡ºé€šé“ä¸º1ï¼Œå·ç§¯æ ¸å¤§å°ä¸º 1x1ï¼Œä¸ä½¿ç”¨åç½®é¡¹
        x = torch.arange(c1, dtype=torch.float)
        self.conv.weight.data[:] = nn.Parameter(x.view(1, c1, 1, 1))
        self.c1 = c1

    def forward(self, x):
        """Applies a transformer layer on input tensor 'x' and returns a tensor."""
        b, c, a = x.shape  # batch, channels, anchors
        return self.conv(x.view(b, 4, self.c1, a).transpose(2, 1).softmax(1)).view(b, 4, a)
        # return self.conv(x.view(b, self.c1, 4, a).softmax(1)).view(b, 4, a)


class Proto(nn.Module):
    """YOLOv8 mask Proto module for segmentation models."""

    def __init__(self, c1, c_=256, c2=32):  # ch_in, number of protos, number of masks
        super().__init__()
        self.cv1 = Conv(c1, c_, k=3)
        self.upsample = nn.ConvTranspose2d(c_, c_, 2, 2, 0, bias=True)  # nn.Upsample(scale_factor=2, mode='nearest')
        self.cv2 = Conv(c_, c_, k=3)
        self.cv3 = Conv(c_, c2)

    def forward(self, x):
        """Performs a forward pass through layers using an upsampled input image."""
        return self.cv3(self.cv2(self.upsample(self.cv1(x))))


class HGStem(nn.Module):
    """StemBlock of PPHGNetV2 with 5 convolutions and one maxpool2d.
    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """

    def __init__(self, c1, cm, c2):
        super().__init__()
        self.stem1 = Conv(c1, cm, 3, 2, act=nn.ReLU())
        self.stem2a = Conv(cm, cm // 2, 2, 1, 0, act=nn.ReLU())
        self.stem2b = Conv(cm // 2, cm, 2, 1, 0, act=nn.ReLU())
        self.stem3 = Conv(cm * 2, cm, 3, 2, act=nn.ReLU())
        self.stem4 = Conv(cm, c2, 1, 1, act=nn.ReLU())
        self.pool = nn.MaxPool2d(kernel_size=2, stride=1, padding=0, ceil_mode=True)

    def forward(self, x):
        """Forward pass of a PPHGNetV2 backbone layer."""
        x = self.stem1(x)
        x = F.pad(x, [0, 1, 0, 1])
        x2 = self.stem2a(x)
        x2 = F.pad(x2, [0, 1, 0, 1])
        x2 = self.stem2b(x2)
        x1 = self.pool(x)
        x = torch.cat([x1, x2], dim=1)
        x = self.stem3(x)
        x = self.stem4(x)
        return x


class HGBlock(nn.Module):
    """HG_Block of PPHGNetV2 with 2 convolutions and LightConv.
    https://github.com/PaddlePaddle/PaddleDetection/blob/develop/ppdet/modeling/backbones/hgnet_v2.py
    """

    def __init__(self, c1, cm, c2, k=3, n=6, lightconv=False, shortcut=False, act=nn.ReLU()):
        super().__init__()
        block = LightConv if lightconv else Conv
        self.m = nn.ModuleList(block(c1 if i == 0 else cm, cm, k=k, act=act) for i in range(n))
        self.sc = Conv(c1 + n * cm, c2 // 2, 1, 1, act=act)  # squeeze conv
        self.ec = Conv(c2 // 2, c2, 1, 1, act=act)  # excitation conv
        self.add = shortcut and c1 == c2

    def forward(self, x):
        """Forward pass of a PPHGNetV2 backbone layer."""
        y = [x]
        y.extend(m(y[-1]) for m in self.m)
        y = self.ec(self.sc(torch.cat(y, 1)))
        return y + x if self.add else y


class SPP(nn.Module):
    """Spatial Pyramid Pooling (SPP) layer https://arxiv.org/abs/1406.4729."""

    def __init__(self, c1, c2, k=(5, 9, 13)):
        """Initialize the SPP layer with input/output channels and pooling kernel sizes."""
        super().__init__()
        c_ = c1 // 2  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)
        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])

    def forward(self, x):
        """Forward pass of the SPP layer, performing spatial pyramid pooling."""
        x = self.cv1(x)
        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))


class SPPF(nn.Module):
    """Spatial Pyramid Pooling - Fast (SPPF) layer for YOLOv5 by Glenn Jocher."""

    def __init__(self, c1, c2, k=5):  # kæ˜¯æŒ‡æ± åŒ–æ ¸çš„å¤§å°   equivalent to SPP(k=(5, 9, 13))
        super().__init__()
        c_ = c1 // 2  # éšè—é€šé“æ•°c_
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c_ * 4, c2, 1, 1)
        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)  # æœ€å¤§æ± åŒ–ï¼Œæ± åŒ–æ ¸å¤§å°ä¸º k

    def forward(self, x):
        """Forward pass through Ghost Convolution block."""
        x = self.cv1(x)
        y1 = self.m(x)
        y2 = self.m(y1)
        y3 = self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))
        # (b,c,h,w)=y3.size()
        # print(b,c,h,w)
        return y3  # å°†å››ä¸ªé€šé“æ‹¼æ¥èµ·æ¥ï¼Œç»è¿‡ç¬¬ä¸€ä¸ªå·ç§¯æ ¸çš„çŸ©é˜µxã€xçš„ä¸‰æ¬¡è¿ç»­æ± åŒ–


class C1(nn.Module):
    """CSP Bottleneck with 1 convolution."""

    def __init__(self, c1, c2, n=1):  # ch_in, ch_out, number
        super().__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.m = nn.Sequential(*(Conv(c2, c2, 3) for _ in range(n)))

    def forward(self, x):
        """Applies cross-convolutions to input in the C3 module."""
        y = self.cv1(x)
        return self.m(y) + y


class C2(nn.Module):
    """CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv(2 * self.c, c2, 1)  # optional act=FReLU(c2)
        # self.attention = ChannelAttention(2 * self.c)  # or SpatialAttention()
        self.m = nn.Sequential(*(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through the CSP bottleneck with 2 convolutions."""
        a, b = self.cv1(x).chunk(2, 1)
        return self.cv2(torch.cat((self.m(a), b), 1))


class C2f(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):  # è¾“å…¥ï¼Œè¾“å‡ºï¼Œé‡å¤æ¬¡æ•°nï¼Œæ®‹å·®è¿æ¥shortcutï¼Œgæ˜¯å·ç§¯æ ¸çš„å¤§å°ï¼Œeæ˜¯ç¼©æ”¾
        super().__init__()
        self.c = int(c2 * e)  # hidden channels e=0.5,å¯¹è¾“å‡ºé€šé“è¿›è¡Œå¹³åˆ†ã€‚
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)   # å®šä¹‰c2fçš„ç¬¬ä¸€ä¸ªå·ç§¯çš„å‚æ•°ï¼Œè¾“å…¥é€šé“æ•°c1ï¼Œè¾“å‡ºé€šé“æ•°2 * self.cï¼Œå·ç§¯æ ¸å¤§å°1*1ï¼ˆæ‰€ä»¥è¾“å‡ºå›¾ç‰‡çš„å¤§å°ä¸å˜ï¼‰ï¼Œæ­¥é•¿ä¸º1
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # å®šä¹‰c2fçš„ç»“å°¾çš„å·ç§¯å‚æ•°ï¼Œè¾“å…¥é€šé“æ•°2 * self.cï¼Œè¾“å‡ºé€šé“æ•°c2ï¼Œå·ç§¯æ ¸å¤§å°1*1
        # nä¸ªBottleneckç»„æˆçš„ModuleList,å¯ä»¥æŠŠmçœ‹åšæ˜¯ä¸€ä¸ªå¯è¿­ä»£å¯¹è±¡
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

    def forward(self, x):
        """Forward pass through C2f layer."""
        # å…ˆæ˜¯å¯¹è¾“å…¥xåšå·ç§¯æ“ä½œï¼Œæ—¨åœ¨æ”¹å˜è¾“å‡ºé€šé“çš„æ•°ç›®
        # cv1çš„å¤§å°æ˜¯(b,c2,w,h)ï¼Œå¯¹cv1åœ¨ç»´åº¦1ç­‰åˆ†æˆä¸¤ä»½ï¼ˆå‡è®¾åˆ†åˆ«æ˜¯aå’Œbï¼‰ï¼Œaå’Œbçš„å¤§å°å‡æ˜¯(b,c2/2,w,h)ã€‚æ­¤æ—¶y=[a,b]ã€‚
        # list(...)ï¼šå°†ç»“æœï¼ˆå—å…ƒç»„ï¼‰è½¬æ¢ä¸º Python åˆ—è¡¨ã€‚
        # æ€»ç»“ï¼šå°†å¾—åˆ°ä¸€ä¸ª Python åˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«ç”±å·ç§¯æ“ä½œå’Œåç»­åˆ†å‰²äº§ç”Ÿçš„ä¸¤ä¸ªå¼ é‡ã€‚ åˆ—è¡¨ä¸­çš„æ¯ä¸ªå¼ é‡å¯¹åº”äºé€šè¿‡æ²¿ç¬¬äºŒç»´åˆ†å‰²å·ç§¯ç»“æœè·å¾—çš„å—ä¹‹ä¸€
        y = list(self.cv1(x).chunk(2, 1))
        # self.m é‡Œé¢æœ‰å¾ªç¯nå»æ§åˆ¶æ¨¡å—çš„æ¬¡æ•°     y[-1]ï¼šæŒ‡yä¸­çš„æœ€åä¸€ä¸ªå…ƒç´ 
        # m(y[-1]) å°†æ¯ä¸ªæ¨¡å— m åº”ç”¨äºåˆ—è¡¨ y çš„æœ€åä¸€ä¸ªå…ƒç´ ã€‚
        # ç„¶åæŠŠcä¹ŸåŠ å…¥yä¸­ã€‚æ­¤æ—¶y=[a,b,c]
        # é‡å¤ä¸Šè¿°æ“ä½œnæ¬¡ï¼ˆå› ä¸ºæ˜¯nä¸ªbottleneckï¼‰ï¼Œæœ€ç»ˆå¾—åˆ°çš„yåˆ—è¡¨ä¸­ä¸€å…±æœ‰n+2ä¸ªå…ƒç´ 
        # y.extend(...) é€šè¿‡é™„åŠ ç”±ç”Ÿæˆå™¨è¡¨è¾¾å¼åˆ›å»ºçš„å¯è¿­ä»£å¯¹è±¡ä¸­çš„å…ƒç´ æ¥æ‰©å±•åˆ—è¡¨ y
        y.extend(m(y[-1]) for m in self.m)
        # å¯¹åˆ—è¡¨yä¸­çš„å¼ é‡åœ¨ç»´åº¦ 1 è¿›è¡Œè¿æ¥ï¼Œå¾—åˆ°çš„å¼ é‡å¤§å°æ˜¯(b,(n+2)*c2/2,w,h)
        # æœ€ç»ˆé€šè¿‡å·ç§¯cv2,è¾“å‡ºå¼ é‡çš„å¤§å°æ˜¯(b,c2,w,h)
        return self.cv2(torch.cat(y, 1))

    def forward_split(self, x):
        """Forward pass using split() instead of chunk().  ä½¿ç”¨ split() è€Œä¸æ˜¯ chunk() è¿›è¡Œå‰å‘ä¼ æ’­ """
        # self.cv1(x) çš„è¾“å‡ºæ²¿ç¬¬äºŒä¸ªç»´åº¦åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†çš„å¤§å°ä¸º self.c
        # list(...)ï¼šå°†åˆ†å‰²ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))


class C3(nn.Module):
    """CSP Bottleneck with 3 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))

    def forward(self, x):
        """Forward pass through the CSP bottleneck with 2 convolutions."""
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))


class C3x(C3):
    """C3 module with cross-convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize C3TR instance and set default parameters."""
        super().__init__(c1, c2, n, shortcut, g, e)
        self.c_ = int(c2 * e)
        self.m = nn.Sequential(*(Bottleneck(self.c_, self.c_, shortcut, g, k=((1, 3), (3, 1)), e=1) for _ in range(n)))


class RepC3(nn.Module):
    """Rep C3."""

    def __init__(self, c1, c2, n=3, e=1.0):
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c1, c2, 1, 1)
        self.m = nn.Sequential(*[RepConv(c_, c_) for _ in range(n)])
        self.cv3 = Conv(c_, c2, 1, 1) if c_ != c2 else nn.Identity()

    def forward(self, x):
        """Forward pass of RT-DETR neck layer."""
        return self.cv3(self.m(self.cv1(x)) + self.cv2(x))


class C3TR(C3):
    """C3 module with TransformerBlock()."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize C3Ghost module with GhostBottleneck()."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)
        self.m = TransformerBlock(c_, c_, 4, n)


class C3Ghost(C3):
    """C3 module with GhostBottleneck()."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        """Initialize 'SPP' module with various pooling sizes for spatial pyramid pooling."""
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))


class GhostBottleneck(nn.Module):
    """Ghost Bottleneck https://github.com/huawei-noah/ghostnet."""

    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride
        super().__init__()
        c_ = c2 // 2
        self.conv = nn.Sequential(
            GhostConv(c1, c_, 1, 1),  # pw
            DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw
            GhostConv(c_, c2, 1, 1, act=False))  # pw-linear
        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1,
                                                                            act=False)) if s == 2 else nn.Identity()

    def forward(self, x):
        """Applies skip connection and concatenation to input tensor."""
        return self.conv(x) + self.shortcut(x)

# è¿™æ˜¯c2fä¼ è¿›æ¥çš„å‚æ•° self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0ï¼Œæ‰€ä»¥å¯¹ç€onnxçœ‹æ—¶ï¼Œå¾—çœ‹è¿™ä¸ªå‚æ•°
class Bottleneck(nn.Module):
    """Standard bottleneck."""
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # è¾“å…¥é€šé“æ•°ï¼Œè¾“å‡ºé€šé“æ•°ï¼Œæ˜¯å¦æ®‹å·®è¿æ¥ï¼Œç»„æ•°ï¼Œå·ç§¯æ ¸çš„å¤§å°ï¼Œç¼©æ”¾å€ç‡e
        super().__init__()
        c_ = int(c2 * e)  # hidden channels æŒ‰ç…§e=0.5ï¼Œåˆ™c_çš„é€šé“æ•°åº”è¯¥æ˜¯c2çš„ä¸€åŠ
        self.cv1 = Conv(c1, c_, k[0], 1)  # è¾“å…¥é€šé“: c1, è¾“å‡ºé€šé“ï¼šc_ , å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)  # è¾“å…¥é€šé“ï¼šc_ , è¾“å‡ºé€šé“c2, å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        #self.cv1 = Conv(c1, c_, k[0], 1)  # è¾“å…¥é€šé“: c1, è¾“å‡ºé€šé“ï¼šc_ , å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        #self.cv2 = RFCAConv(c_, c2)  # è¾“å…¥é€šé“ï¼šc_ , è¾“å‡ºé€šé“c2, å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        self.add = shortcut and c1 == c2   # shortcut and c1 == c2 è¡¨ç¤ºå¦‚æœåŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼Œself.add çš„å€¼ä¸º Trueï¼ŒåŒæ—¶ä½¿ç”¨æ®‹å·®è¿æ¥

    def forward(self, x):
        """'forward()' applies the YOLOv5 FPN to input data."""
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x)) # å¦‚æœ self.add ä¸º Trueï¼Œè¾“å‡ºçš„æ˜¯xç»è¿‡ä¸¤ä¸ªå·ç§¯åçš„å€¼å’Œxç›¸åŠ ï¼›å¦‚æœå¦‚æœ self.add ä¸ºFalseï¼Œè¾“å‡ºxç»è¿‡ä¸¤ä¸ªå·ç§¯åçš„å€¼


class BottleneckCSP(nn.Module):
    """CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks."""

    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)
        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)
        self.cv4 = Conv(2 * c_, c2, 1, 1)
        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)
        self.act = nn.SiLU()
        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        """Applies a CSP bottleneck with 3 convolutions."""
        y1 = self.cv3(self.m(self.cv1(x)))
        y2 = self.cv2(x)
        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))


######################################## é­”é­çš„LAWDS(ä¸€ä¸ªå¯ä»¥ä»£æ›¿å·ç§¯çš„æ¨¡å—) begin ########################################

class LAWDS(nn.Module):
    # Light Adaptive-weight downsampling
    # å¯¹ä½¿ç”¨åˆ°çš„æ¨¡å—è¿›è¡Œåˆå§‹åŒ–å®šä¹‰
    def __init__(self, ch, group=16) -> None:   # chè¡¨ç¤ºè¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°ï¼Œgroupæ˜¯åˆ†ç»„å·ç§¯ä¸­çš„åˆ†ç»„æ•°
        super().__init__()

        self.softmax = nn.Softmax(dim=-1)   # åˆ›å»ºä¸€ä¸ªsoftmaxæ¿€æ´»å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ¯ä¸ªä½ç½®çš„æƒé‡
        # è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå±‚çš„Sequentialæ¨¡å—,ç»è¿‡è¿™ä¸ªæ¨¡å—çš„ç‰¹å¾å›¾ç»´åº¦ä¸å‘ç”Ÿå˜åŒ–
        # nn.AvgPool2dï¼šä¸€ä¸ªå¹³å‡æ± åŒ–å±‚ï¼Œå¯¹è¾“å…¥ç‰¹å¾å›¾è¿›è¡Œå‡å€¼æ± åŒ–ï¼Œä½¿ç”¨3x3çš„å·ç§¯æ ¸ï¼Œstrideä¸º1ï¼Œpaddingä¸º1ï¼ˆä¾‹å¦‚3*3çš„çŸ©é˜µå…ˆæ‰©å……ä¸º4*4ï¼‰
        # Conv(ch, ch, k=1)ï¼šä¸€ä¸ª1x1å·ç§¯å±‚ï¼Œç”¨äºè°ƒæ•´é€šé“æ•°ï¼Œè¾“å…¥é€šé“æ•°å’Œè¾“å‡ºé€šé“æ•°éƒ½æ˜¯chï¼Œkæ˜¯å·ç§¯æ ¸ä¸ªæ•°
        self.attention = nn.Sequential(
            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),
            Conv(ch, ch, k=1)
        )
        # å®ç°yolov5çš„focusæ¨¡å—çš„ç‰¹å¾
        # ç»è¿‡è¿™ä¸ªæ¨¡å—çš„ç‰¹å¾å›¾hå’Œwå˜æˆ 1/2
        # self.ds_convï¼šè¿™æ˜¯ä¸€ä¸ªå·ç§¯å±‚ï¼Œç”¨äºæ‰§è¡Œä¸‹é‡‡æ ·æ“ä½œã€‚å®ƒé‡‡ç”¨chä¸ªè¾“å…¥é€šé“ï¼Œå°†å…¶è½¬æ¢ä¸ºch * 4ä¸ªè¾“å‡ºé€šé“ï¼Œä½¿ç”¨3x3çš„å·ç§¯æ ¸ï¼Œstrideä¸º2ï¼Œåˆ†ç»„å·ç§¯çš„ç»„æ•°ä¸ºch // group
        self.ds_conv = Conv(ch, ch * 4, k=3, s=2, g=(ch // group))

    def forward(self, x):
        # bs, ch, 2*h, 2*w => bs, ch, h, w, 4    å¯¹4è¿™ä¸ªç»´åº¦è¿›è¡Œsoftmax
        # é¦–å…ˆï¼Œé€šè¿‡self.attentionæ¨¡å—è®¡ç®—å±€éƒ¨å‡å€¼æ± åŒ–å’Œå­¦ä¹ åˆ°çš„æƒé‡ã€‚è¿™æ˜¯é€šè¿‡å¯¹è¾“å…¥xè¿›è¡Œå‡å€¼æ± åŒ–ï¼Œç„¶åé‡æ’æ“ä½œï¼ˆrearrangeï¼‰æ¥å®ç°çš„ï¼Œå°†å½¢çŠ¶ä»bs, ch, h, wå˜ä¸ºbs, ch, h, w, 4ï¼Œè¿™é‡Œçš„4è¡¨ç¤ºæ¯ä¸ªä½ç½®å¯¹åº”çš„4ä¸ªæƒé‡ã€‚æ¥ç€ï¼Œä½¿ç”¨softmaxå‡½æ•°å¯¹è¿™äº›æƒé‡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ç¡®ä¿å®ƒä»¬çš„æ€»å’Œä¸º1ã€‚
        att = rearrange(self.attention(x), 'bs ch (s1 h) (s2 w) -> bs ch h w (s1 s2)', s1=2, s2=2)
        # (b,c,h,w,d)=att.size()
        # print(b,c,h,w,d)
        att = self.softmax(att)

        # bs, 4 * ch, h, w => bs, ch, h, w, 4
        # é€šè¿‡self.ds_convæ¨¡å—è¿›è¡Œä¸‹é‡‡æ ·ã€‚è¿™æ˜¯é€šè¿‡å·ç§¯æ“ä½œï¼Œä»¥åŠå°†å½¢çŠ¶ä»bs, 4*ch, h, wå˜ä¸ºbs, ch, h, w, 4æ¥å®ç°çš„ï¼Œè¿™é‡Œçš„4è¡¨ç¤ºæ¯ä¸ªä½ç½®å¯¹åº”çš„4ä¸ªé€šé“ã€‚
        x = rearrange(self.ds_conv(x), 'bs (s ch) h w -> bs ch h w s', s=4)
        # å°†ä¸‹é‡‡æ ·çš„ç»“æœä¸æƒé‡ç›¸ä¹˜ï¼Œç„¶åæ²¿ç€æœ€åä¸€ä¸ªç»´åº¦ï¼ˆ4ç»´ï¼‰å¯¹å®ƒä»¬è¿›è¡Œæ±‚å’Œï¼Œå¾—åˆ°æœ€ç»ˆçš„ä¸‹é‡‡æ ·ç»“æœ
        x = torch.sum(x * att, dim=-1)
        return x

######################################## LAWDS end ########################################


######################################## RACFconvçš„å„ä¸ªå·ç§¯ï¼ˆåªå®éªŒäº†RFCAConvä»£æ›¿bottlenecké‡Œçš„ç¬¬äºŒä¸ªå·ç§¯ï¼‰ begin ########################################

class h_sigmoid(nn.Module):
    def __init__(self, inplace=True):
        super(h_sigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace=inplace)

    def forward(self, x):
        return self.relu(x + 3) / 6

class h_swish(nn.Module):
    def __init__(self, inplace=True):
        super(h_swish, self).__init__()
        self.sigmoid = h_sigmoid(inplace=inplace)

    def forward(self, x):
        return x * self.sigmoid(x)

class RFCAConv(nn.Module):
    # åœ¨bottleneckçš„ç¬¬äºŒä¸ªå·ç§¯ä¸­ï¼Œkernel_size, strideå‡ä¸º1
    def __init__(self, inp, oup, kernel_size, stride, reduction=32):
        super(RFCAConv, self).__init__()
        self.kernel_size = kernel_size
        self.generate = nn.Sequential(nn.Conv2d(inp, inp * (kernel_size ** 2), kernel_size, padding=kernel_size // 2,
                                                stride=stride, groups=inp,
                                                bias=False),
                                      nn.BatchNorm2d(inp * (kernel_size ** 2)),
                                      nn.ReLU()
                                      )
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        mip = max(8, inp // reduction)

        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(mip)
        self.act = h_swish()

        self.conv_h = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)
        self.conv_w = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)
        self.conv = nn.Sequential(nn.Conv2d(inp, oup, kernel_size, stride=kernel_size))

    def forward(self, x):
        b, c = x.shape[0:2]
        generate_feature = self.generate(x)
        h, w = generate_feature.shape[2:]
        generate_feature = generate_feature.view(b, c, self.kernel_size ** 2, h, w)

        generate_feature = rearrange(generate_feature, 'b c (n1 n2) h w -> b c (h n1) (w n2)', n1=self.kernel_size,
                                     n2=self.kernel_size)

        x_h = self.pool_h(generate_feature)
        x_w = self.pool_w(generate_feature).permute(0, 1, 3, 2)

        y = torch.cat([x_h, x_w], dim=2)
        y = self.conv1(y)
        y = self.bn1(y)
        y = self.act(y)

        h, w = generate_feature.shape[2:]
        x_h, x_w = torch.split(y, [h, w], dim=2)
        x_w = x_w.permute(0, 1, 3, 2)

        a_h = self.conv_h(x_h).sigmoid()
        a_w = self.conv_w(x_w).sigmoid()
        return self.conv(generate_feature * a_w * a_h)


class RFAConv2(nn.Module):  # åŸºäºGroup Convå®ç°çš„RFAConv
    def __init__(self, in_channel, out_channel, kernel_size, stride=1):
        super().__init__()
        self.kernel_size = kernel_size  # å°†å‚æ•°å­˜å‚¨åœ¨æ¨¡å—ä¸­

        self.get_weight = nn.Sequential(nn.AvgPool2d(kernel_size=kernel_size, padding=kernel_size // 2, stride=stride),
                                        nn.Conv2d(in_channel, in_channel * (kernel_size ** 2), kernel_size=1,
                                                  groups=in_channel, bias=False))
        self.generate_feature = nn.Sequential(
            nn.Conv2d(in_channel, in_channel * (kernel_size ** 2), kernel_size=kernel_size, padding=kernel_size // 2,
                      stride=stride, groups=in_channel, bias=False),
            nn.BatchNorm2d(in_channel * (kernel_size ** 2)),
            nn.ReLU())

        self.conv = nn.Sequential(nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, stride=kernel_size),
                                  nn.BatchNorm2d(out_channel),
                                  nn.ReLU())

    def forward(self, x):
        b, c = x.shape[0:2]
        weight = self.get_weight(x)
        h, w = weight.shape[2:]
        weighted = weight.view(b, c, self.kernel_size ** 2, h, w).softmax(2)  # b c*kernel**2,h,w ->  b c k**2 h w
        feature = self.generate_feature(x).view(b, c, self.kernel_size ** 2, h,
                                                w)  # b c*kernel**2,h,w ->  b c k**2 h w   è·å¾—æ„Ÿå—é‡ç©ºé—´ç‰¹å¾
        weighted_data = feature * weighted
        conv_data = rearrange(weighted_data, 'b c (n1 n2) h w -> b c (h n1) (w n2)', n1=self.kernel_size,
                              # b c k**2 h w ->  b c h*k w*k
                              n2=self.kernel_size)
        return self.conv(conv_data)

######################################## RACFconvçš„å„ä¸ªå·ç§¯  end ########################################

######################################## BIFPN begin ########################################
# fusionç›¸å½“äºä¸åŒå¤§å°çš„ç‰¹å¾å›¾æ•´æˆä¸€æ ·å¤§å°çš„
class Fusion(nn.Module):
    # inc_listï¼šè¾“å…¥é€šé“æ•°       fusion:èåˆçš„æ–¹å¼ï¼Œæœ‰å››ç§ï¼Œ'weight', 'adaptive', 'concat', 'bifpn'
    def __init__(self, inc_list, fusion='bifpn') -> None:
        super().__init__()

        assert fusion in ['weight', 'adaptive', 'concat', 'bifpn']
        self.fusion = fusion

        if self.fusion == 'bifpn':
            # å®šä¹‰ä¸€ä¸ªåä¸ºâ€œfusion_weightâ€çš„å‚æ•°ï¼ˆï¼‰ï¼ŒPyTorch ä¸­çš„å‚æ•°æ˜¯åœ¨è®­ç»ƒæœŸé—´ä¼˜åŒ–çš„å¯å­¦ä¹ å¼ é‡ï¼Œnn.å‚æ•°å‡½æ•°ç”¨äºåˆ›å»ºæ­¤ç±»å‚æ•°
            # len(inc_list)ç”¨äºç¡®å®šåˆ—è¡¨ä¸­å…ƒç´ çš„æ•°é‡
            # orch.onesæ˜¯æŒ‡ä½¿ç”¨ 1 çš„å¼ é‡è¿›è¡Œåˆå§‹åŒ–ï¼Œå¼ é‡çš„é•¿åº¦ç”±inc_listä¸­çš„å…ƒç´ æ•°å†³å®šï¼Œè¿™è¡¨æ˜å…¶ä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½æœ‰ä¸€ä¸ªå…³è”çš„æƒé‡
            self.fusion_weight = nn.Parameter(torch.ones(len(inc_list), dtype=torch.float32), requires_grad=True)
            self.relu = nn.ReLU()
            # è¾ƒå°çš„ epsilon å€¼ç”¨äºé˜²æ­¢è¢«é›¶é™¤æˆ–ä¸ºæŸäº›æ“ä½œå¢åŠ æ•°å€¼ç¨³å®šæ€§ï¼Œç›¸å½“äºæ”¾åœ¨åˆ†æ¯çš„ä½ç½®
            self.epsilon = 1e-4
        else:
            # nn.ModuleListï¼šè¿™æ˜¯ä¸€ä¸ªåŒ…å«å­æ¨¡å—åˆ—è¡¨çš„å®¹å™¨æ¨¡å—
            self.fusion_conv = nn.ModuleList([Conv(inc, inc, 1) for inc in inc_list])

            if self.fusion == 'adaptive':
                self.fusion_adaptive = Conv(sum(inc_list), len(inc_list), 1)

    def forward(self, x):
        if self.fusion in ['weight', 'adaptive']:
            for i in range(len(x)):
                x[i] = self.fusion_conv[i](x[i])
        if self.fusion == 'weight':
            return torch.sum(torch.stack(x, dim=0), dim=0)
        elif self.fusion == 'adaptive':
            fusion = torch.softmax(self.fusion_adaptive(torch.cat(x, dim=1)), dim=1)
            x_weight = torch.split(fusion, [1] * len(x), dim=1)
            return torch.sum(torch.stack([x_weight[i] * x[i] for i in range(len(x))], dim=0), dim=0)
        elif self.fusion == 'concat':
            return torch.cat(x, dim=1)
        elif self.fusion == 'bifpn':
            fusion_weight = self.relu(self.fusion_weight.clone())
            fusion_weight = fusion_weight / (torch.sum(fusion_weight, dim=0))
            return torch.sum(torch.stack([fusion_weight[i] * x[i] for i in range(len(x))], dim=0), dim=0)

######################################## BIFPN end ########################################

######################################## SCConv begin ########################################

# CVPR 2020 http://mftp.mmcheng.net/Papers/20cvprSCNet.pdf
# SCConv æ¨¡å—é€šè¿‡è¿™ä¸‰ä¸ªåˆ†æ”¯çš„æ“ä½œï¼Œå®ç°äº†é€‰æ‹©æ€§çš„ç‰¹å¾èåˆï¼Œå…¶ä¸­ k2 åˆ†æ”¯ç”¨äºè·å–å…¨å±€ä¿¡æ¯ï¼Œk3 åˆ†æ”¯ç”¨äºå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œå¾®è°ƒï¼Œè€Œ k4 åˆ†æ”¯ç”¨äºäº§ç”Ÿæœ€ç»ˆçš„è¾“å‡ºã€‚è¿™æœ‰åŠ©äºç½‘ç»œé€‰æ‹©æ€§åœ°èåˆå’Œå¼ºè°ƒä¸åŒçš„ç‰¹å¾ï¼Œä»¥æé«˜æ¨¡å‹æ€§èƒ½
class SCConv(nn.Module):
    # https://github.com/MCG-NKU/SCNet/blob/master/scnet.py
    #  sï¼šæ­¥é•¿ d:ç©ºæ´å·ç§¯
    def __init__(self, c1, c2, s=1, d=1, g=1, pooling_r=4): # å°‘äº†paddingå’Œnorm_layerçš„åˆå§‹å€¼çš„è®¾ç½®
        super(SCConv, self).__init__()
        self.k2 = nn.Sequential(
                    nn.AvgPool2d(kernel_size=pooling_r, stride=pooling_r),      # å…ˆæ˜¯å¯¹è¾“å…¥è¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–ï¼Œå°†è¾“å…¥ç‰¹å¾å›¾å°ºå¯¸ç¼©å°ï¼Œæ­¥é•¿strideä¸º pooling_rï¼Œç›¸å½“äºç‰¹å¾å›¾ç¼©å°pooling_rå€
                    Conv(c1, c2, k=3, d=d, g=g, act=False)  # ç©ºæ´å·ç§¯çš„ç‡ä¸ºdï¼Œä¸åº”ç”¨æ¿€æ´»å‡½æ•°ï¼ˆact=Falseï¼‰  ä¸è¾“å…¥paddingï¼Œåˆ™ä¸ºnone   è¿™é‡Œè¾“å…¥è¿›convçš„é¡ºåºå’Œå®šä¹‰çš„é¡ºåºä¸ä¸€æ ·
                    )
        self.k3 = Conv(c1, c2, k=3, d=d, g=g, act=False)    # ä¸ k2 åˆ†æ”¯ç±»ä¼¼ï¼Œä½†æ²¡æœ‰æ± åŒ–
        self.k4 = Conv(c1, c2, k=3, s=s, d=d, g=g, act=False)
        #self.k5 = Conv(c1, c2, k=3, s=2, p=1, d=1, g=1)

    def forward(self, x):
        identity = x    # é¦–å…ˆå°†è¾“å…¥ x å¤åˆ¶åˆ° identity å˜é‡ä¸­ï¼Œä»¥ä¾¿åé¢è¿›è¡Œæ®‹å·®è¿æ¥
        # self.k2(x) è¿”å› k2 åˆ†æ”¯çš„è¾“å‡º    F.interpolate(self.k2(x), identity.size()[2:])ï¼šé€šè¿‡æ’å€¼æ“ä½œï¼Œå°† k2 åˆ†æ”¯çš„è¾“å‡ºè°ƒæ•´ä¸ºä¸ identity çš„ç›¸åŒå°ºå¯¸
        # torch.add(identity, ...)ï¼šæ¥ä¸‹æ¥ï¼Œå°† identityï¼ˆå³è¾“å…¥ x çš„å‰¯æœ¬ï¼‰ä¸å‰ä¸€æ­¥ä¸­çš„æ’å€¼ç»“æœç›¸åŠ ã€‚è¿™æ˜¯å…¸å‹çš„æ®‹å·®è¿æ¥æ“ä½œï¼Œå®ƒæœ‰åŠ©äºå°†åŸå§‹ç‰¹å¾ä¸æ–°ç‰¹å¾ç›¸èåˆ
        # torch.sigmoid(...)ï¼šæœ€åï¼Œå°†ç›¸åŠ åçš„ç»“æœé€šè¿‡ sigmoid å‡½æ•°è¿›è¡Œæ¿€æ´»ã€‚è¿™ä¸ªæ“ä½œäº§ç”Ÿäº†ä¸€ä¸ªèŒƒå›´åœ¨ 0 åˆ° 1 ä¹‹é—´çš„å¼ é‡ï¼Œå…¶ä¸­å€¼æ¥è¿‘ 1 è¡¨ç¤ºæ¥è‡ª k2 åˆ†æ”¯çš„ä¿¡æ¯å¯¹æœ€ç»ˆè¾“å‡ºçš„è´¡çŒ®è¾ƒå¤§ï¼Œå€¼æ¥è¿‘ 0 è¡¨ç¤ºè´¡çŒ®è¾ƒå°
        out = torch.sigmoid(torch.add(identity, F.interpolate(self.k2(x), identity.size()[2:]))) # sigmoid(identity + k2)
        out = torch.mul(self.k3(x), out) # k3 * sigmoid(identity + k2)
        out = self.k4(out) # k4
        #out = self.k5(out)  # k4
        return out

class Bottleneck_SCConv(Bottleneck):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)    # å‡å¦‚è¿™è¦æ›¿æ¢æˆSCConvï¼Œå»æ‰k[0], 1ï¼Œ
        self.cv2 = SCConv(c_, c2, g=g)

class C3_SCConv(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_SCConv(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))

class C2f_SCConv(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_SCConv(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## SCConv end ########################################

######################################## down_sample end ########################################
class down_sample(nn.Module):
    # Efficient Multi-Scale Conv Plus
    def __init__(self, channel_in, channel_out, kernels=[1, 3, 5, 7]):
        super().__init__()
        min_ch = channel_in // 4
        self.convs = nn.ModuleList([])
        for ks in kernels:
            self.convs.append(nn.Sequential(nn.Conv2d(min_ch, min_ch*2, kernel_size=(1, ks), stride=(1, 2), padding=(0, ks//2)),
                                nn.Conv2d(min_ch*2, min_ch*2, kernel_size=(ks, 1), stride=(2, 1), padding=(ks//2, 0))))
        self.conv_1x1 = nn.Sequential(
            Conv(channel_out, channel_out, 1),
            Conv(channel_out, channel_out, 1)
        )

    def channel_shuffle(self, x, groups):
        batchsize, num_channels, height, width = x.size()
        channels_per_group = num_channels // groups
        x = x.view(batchsize, groups, channels_per_group, height, width)
        x = torch.transpose(x, 1, 2).contiguous()
        x = x.view(batchsize, -1, height, width)
        return x

    def forward(self, x):
        x_group = rearrange(x, 'bs (g ch) h w -> bs ch h w g', g=4)
        x_convs = torch.stack([self.convs[i](x_group[..., i]) for i in range(len(self.convs))])
        x_convs = rearrange(x_convs, 'g bs ch h w -> bs (g ch) h w')
        # å®Œæˆæ´—ç‰Œçš„æ“ä½œ
        x_convs = self.channel_shuffle(x_convs, 2)
        return x_convs + self.conv_1x1(x_convs)
######################################## down_sample end ########################################


######################################## C3 C2f DCNV2 start ########################################
def autopad(k, p=None, d=1):  # kernel(å·ç§¯æ ¸çš„å¤§å°ï¼Œç±»å‹å¯èƒ½æ˜¯ä¸€ä¸ªintä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªåºåˆ—), padding(å¡«å……), dilation(æ‰©å¼ ï¼Œæ™®é€šå·ç§¯çš„æ‰©å¼ ç‡ä¸º1ï¼Œç©ºæ´å·ç§¯çš„æ‰©å¼ ç‡å¤§äº1)
    """Pad to 'same' shape outputs."""
    if d > 1:                         # åŠ å…¥ç©ºæ´å·ç§¯ä»¥åçš„å®é™…å·ç§¯æ ¸ä¸åŸå§‹å·ç§¯æ ¸ä¹‹é—´çš„å…³ç³»å¦‚ä¸‹.è¿›å…¥ä¸‹é¢çš„è¯­å¥ï¼Œè¯´æ˜è¯´æ˜æœ‰æ‰©å¼ æ“ä½œï¼Œéœ€è¦æ ¹æ®æ‰©å¼ ç³»æ•°æ¥è®¡ç®—çœŸæ­£çš„å·ç§¯æ ¸å¤§å°ï¼Œifè¯­å¥ä¸­å¦‚æœkæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ™åˆ†åˆ«è®¡ç®—å‡ºæ¯ä¸ªç»´åº¦çš„çœŸå®å·ç§¯æ ¸å¤§å°ï¼š[d * (x - 1) + 1 for x in k]
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]
    if p is None:                       # ä¸‹é¢çš„//æ˜¯æŒ‡å‘ä¸‹å–æ•´
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # è‡ªåŠ¨è®¡ç®—å¡«å……çš„å¤§å°ã€‚ifè¯­å¥ä¸­ï¼Œå¦‚æœkæ˜¯ä¸€ä¸ªæ•´æ•°ï¼Œåˆ™k//2ï¼ˆisinstanceå°±æ˜¯åˆ¤æ–­kæ˜¯å¦æ˜¯intæ•´æ•°ï¼‰
    return p

class DCNv2(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=None, groups=1, dilation=1, act=True, deformable_groups=1):
        super(DCNv2, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = (kernel_size, kernel_size)   # å·ç§¯æ ¸çš„å¤§å°ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ•´æ•°ï¼ˆè¡¨ç¤ºæ­£æ–¹å½¢å·ç§¯æ ¸ï¼‰æˆ–è€…æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ˆè¡¨ç¤ºé•¿å®½ä¸åŒçš„çŸ©å½¢å·ç§¯æ ¸ï¼‰
        self.stride = (stride, stride)
        padding = autopad(kernel_size, padding, dilation)   # æ ¹æ®å·ç§¯æ ¸å¤§å°å’Œæ‰©å¼ ç‡ï¼ˆdilationï¼‰é‡æ–°è®¡ç®—å¡«å……å¤§å°
        self.padding = (padding, padding)
        self.dilation = (dilation, dilation)    #  å·ç§¯æ ¸çš„æ‰©å¼ ï¼ˆæˆ–è†¨èƒ€ï¼‰ç‡ï¼Œç”¨äºæ§åˆ¶å·ç§¯æ ¸ä¸­å…ƒç´ ä¹‹é—´çš„ç©ºé—´é—´éš”
        self.groups = groups
        self.deformable_groups = deformable_groups  # å¯å˜å½¢å·ç§¯çš„ç»„æ•°ï¼Œç”¨äºæ§åˆ¶å¯å˜å½¢å·ç§¯ä¸­çš„åˆ†ç»„æ–¹å¼

        # æƒé‡å’Œåç½®å‚æ•°åˆå§‹åŒ–
        # self.weight: å·ç§¯å±‚çš„æƒé‡ï¼›self.bias: å·ç§¯å±‚çš„åç½®ï¼Œéƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°
        # æƒé‡å’Œåç½®é€šè¿‡ nn.Parameter å°è£…ï¼Œè¡¨ç¤ºå®ƒä»¬æ˜¯æ¨¡å‹å¯å­¦ä¹ çš„å‚æ•°ã€‚è¿™ä¸¤ä¸ªå‚æ•°çš„å½¢çŠ¶ä¸å·ç§¯æ ¸å¤§å°ã€è¾“å…¥é€šé“æ•°å’Œè¾“å‡ºé€šé“æ•°ç›¸å…³
        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, *self.kernel_size)
        )
        self.bias = nn.Parameter(torch.empty(out_channels))

        # out_channels_offset_mask æ˜¯åç§»å’Œæ©ç çš„é€šé“æ•°ï¼Œä¸å¯å˜å½¢å·ç§¯çš„è®¾ç½®ç›¸å…³
        # åœ¨å¯å˜å½¢å·ç§¯ä¸­ï¼Œæ¯ä¸ªç©ºé—´ä½ç½®éƒ½éœ€è¦ç”Ÿæˆä¸€ä¸ªåç§»å‘é‡å’Œä¸€ä¸ªæ©ç ï¼Œè€Œè¿™äº›å‘é‡å’Œæ©ç çš„ç»´åº¦å°±ç”± out_channels_offset_mask å†³å®š
        # deformable_groups: å¯å˜å½¢å·ç§¯ä¸­çš„å˜å½¢ç»„æ•°   3: æ¯ä¸ªä½ç½®éœ€è¦ç”Ÿæˆçš„åç§»å‘é‡å’Œæ©ç çš„ç»´åº¦
        # kernel_size[0] * kernel_size[1]: å·ç§¯æ ¸çš„å¤§å°
        out_channels_offset_mask = (self.deformable_groups * 3 *
                                    self.kernel_size[0] * self.kernel_size[1])
        # self.conv_offset_mask: ç”¨äºç”Ÿæˆå¯å˜å½¢å·ç§¯çš„åç§»å’Œæ©ç çš„å·ç§¯å±‚ã€‚
        self.conv_offset_mask = nn.Conv2d(
            self.in_channels,
            out_channels_offset_mask,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            bias=True,
        )
        self.bn = nn.BatchNorm2d(out_channels)
        self.act = Conv.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
        # self.reset_parameters(): è°ƒç”¨è¯¥æ–¹æ³•å¯¹æƒé‡ã€åç½®ã€åç§»å’Œæ©ç çš„å‚æ•°è¿›è¡Œåˆå§‹åŒ–ã€‚è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„æ–¹æ³•ï¼Œé€šå¸¸ç”¨äºåˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°
        self.reset_parameters()

    def forward(self, x):
        # è¾“å…¥ç»è¿‡conv_offset_mask å¤„ç†
        offset_mask = self.conv_offset_mask(x)
        # ç”Ÿæˆåç§»é‡ offset å’Œæ©è†œ maskï¼ŒåŒæ—¶å°†å®ƒä»¬åˆ†æˆä¸¤éƒ¨åˆ†ï¼ˆo1 å’Œ o2ï¼‰å’Œ mask
        o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)
        # å°† o1 å’Œ o2 è¿›è¡Œæ‹¼æ¥ï¼Œç»„æˆåç§»é‡ offset
        offset = torch.cat((o1, o2), dim=1)
        # mask ç»è¿‡ sigmoid å‡½æ•°è¿›è¡Œå½’ä¸€åŒ–
        mask = torch.sigmoid(mask)
        # ä½¿ç”¨åç§»é‡ offsetã€æ©è†œ mask å’Œæƒé‡ self.weight å¯¹è¾“å…¥ x è¿›è¡Œ deformable convolution è¿ç®—ï¼Œå¾—åˆ°è¾“å‡ºç‰¹å¾å›¾   torchvision.ops
        x = torch.ops.torchvision.deform_conv2d(
            x,
            self.weight,
            offset,
            mask,
            self.bias,
            self.stride[0], self.stride[1],
            self.padding[0], self.padding[1],
            self.dilation[0], self.dilation[1],
            self.groups,
            self.deformable_groups,
            True
        )
        # ä¸‹é¢è¿™ç§æ–¹æ³•ä¹Ÿè½¬ä¸å‡ºonnx
        # x = deform_conv2d(
        #     x,
        #     offset,
        #     self.weight,
        #
        #
        #     self.bias,
        #     self.stride[0],
        #     self.padding[0],
        #     self.dilation[0],
        #     mask,
        # )
        x = self.bn(x)
        x = self.act(x)
        return x

    # é‡ç½®ç½‘ç»œå‚æ•°çš„å‡½æ•°
    #
    def reset_parameters(self):
        # è®¡ç®—è¾“å…¥é€šé“æ•° self.in_channels ä¸æ¯ä¸ªå·ç§¯æ ¸å¤§å° self.kernel_size çš„ä¹˜ç§¯ï¼Œå¾—åˆ°åˆå§‹æƒé‡å‚æ•°å…ƒç´ çš„æ•°é‡ n
        n = self.in_channels
        for k in self.kernel_size:
            n *= k
        # è®¡ç®—æ ‡å‡†å·® std
        std = 1. / math.sqrt(n)
        # ä½¿ç”¨å‡åŒ€åˆ†å¸ƒç”Ÿæˆä¸€ä¸ªèŒƒå›´åœ¨ -std åˆ° std ä¹‹é—´çš„éšæœºå€¼æ¥åˆå§‹åŒ–æƒé‡æ•°æ® self.weight.data
        self.weight.data.uniform_(-std, std)
        # å°†åç½®é¡¹ self.bias.data åˆå§‹åŒ–ä¸ºå…¨é›¶
        self.bias.data.zero_()
        # å°† conv_offset_mask æ“ä½œçš„æƒé‡å’Œåç½®é¡¹å‡åˆå§‹åŒ–ä¸ºå…¨é›¶
        self.conv_offset_mask.weight.data.zero_()
        self.conv_offset_mask.bias.data.zero_()

class Bottleneck_DCNV2(Bottleneck):
    """Standard bottleneck with DCNV2."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv2 = DCNv2(c_, c2, k[1], 1)

class C3_DCNv2(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_DCNV2(c_, c_, shortcut, g, k=(1, 3), e=1.0) for _ in range(n)))

class C2f_DCNv2(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_DCNV2(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## C3 C2f DCNV2 end ########################################


######################################## DAMO-YOLO GFPN start ########################################
# æ„å»ºRepconvå’Œconvçš„æ®‹å·®ç»“æ„
class BasicBlock_3x3_Reverse(nn.Module):    # ç±»ä¼¼äºå®šä¹‰äº†bottleneckæ¨¡å—ï¼Œé¡ºåºæ˜¯å…ˆRepconvï¼Œå†conv
    def __init__(self,
                 ch_in,
                 ch_hidden_ratio,   # ç”¨äºè®¡ç®—éšè—é€šé“æ•°çš„æ¯”ç‡ï¼Œç›¸å½“äºä¸¤ä¸ªå·ç§¯ï¼Œç¬¬ä¸€ä¸ªå·ç§¯è¾“å‡ºçš„é€šé“æ•°æ˜¯ch_in * ch_hidden_ratio
                 ch_out,
                 shortcut=True):    # ä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦åŒ…å«å¿«æ·æ–¹å¼è¿æ¥
        super(BasicBlock_3x3_Reverse, self).__init__()
        assert ch_in == ch_out  # # åˆ¤æ–­æ˜¯å¦åŒ…å«å¿«æ·æ–¹å¼è¿æ¥ï¼Œå‡å¦‚è¾“å…¥ä¸ç­‰äºè¾“å‡ºï¼Œä¼šå‡ºç°AssertionError
        ch_hidden = int(ch_in * ch_hidden_ratio)    # æ ¹æ®è¾“å…¥é€šé“æ•°å’ŒæŒ‡å®šæ¯”ç‡è®¡ç®—ç¬¬ä¸€ä¸ªå·ç§¯conv2çš„è¾“å‡ºé€šé“æ•°ï¼Œç¬¬äºŒä¸ªå·ç§¯conv1çš„è¾“å…¥é€šé“æ•°
        self.conv1 = Conv(ch_hidden, ch_out, 3, s=1)    # åˆ›å»ºä¸€ä¸ªæ™®é€šå·ç§¯å±‚Convï¼Œå†…æ ¸å¤§å°ä¸º 3x3 ä¸”æ­¥å¹…ä¸º 1ï¼Œæ­¤æ—¶è¾“å…¥çš„é«˜å®½=è¾“å‡ºçš„
        self.conv2 = RepConv(ch_in, ch_hidden, 3, s=1)  # åˆ›å»ºä¸€ä¸ªRepConvï¼Œé¡ºåºæ˜¯å…ˆRepconvï¼Œå†conv
        self.shortcut = shortcut    # å°†å‚æ•°çš„å€¼åˆ†é…ç»™å®ä¾‹å˜é‡ã€‚æ­¤å˜é‡ç”¨äºç¡®å®šæ˜¯å¦åº”æ·»åŠ å¿«æ·æ–¹å¼è¿æ¥

    # å…ˆæ˜¯repconv->conv,å¹¶ä¸”é»˜è®¤æ˜¯æ®‹å·®çš„ç»“æ„
    def forward(self, x):
        y = self.conv2(x)
        y = self.conv1(y)
        # æ®‹å·®ç»“æ„çš„é€‰æ‹©
        if self.shortcut:
            # å¦‚æœå¯ç”¨ï¼Œåˆ™å‰å‘ä¼ é€’è¿”å›è¾“å…¥å’Œè¾“å‡ºçš„æ€»å’Œã€‚è¿™æ˜¯æ®‹å·®å—çš„å…¸å‹ç»“æ„ï¼Œå…¶ä¸­è¾“å…¥è¢«æ·»åŠ åˆ°è¾“å‡ºä¸­ï¼Œæ³¨æ„è¿™é‡Œæ˜¯å¯¹é«˜å’Œå®½å¯¹åº”ä½ç½®çš„åƒç´ å€¼è¿›è¡Œç›¸åŠ 
            return x + y
        else:
            return y

# è¯¥æ¨¡å—çš„æ€»ä½“ç›®çš„æ˜¯ä½¿ç”¨ç©ºé—´é‡‘å­—å¡”æ± åŒ–ä»è¾“å…¥ç‰¹å¾å›¾ä¸­æ•è·å¤šå°ºåº¦ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨å·ç§¯å±‚å¯¹å…¶è¿›è¡Œè¿›ä¸€æ­¥å¤„ç†
class SPP(nn.Module):
    def __init__(
        self,
        ch_in,
        ch_out,
        k,  # ç©ºé—´é‡‘å­—å¡”ä¸­çš„å±‚æ•°ã€‚å®ƒå†³å®šäº†ç©ºé—´é‡‘å­—å¡”æ± å°†ä½¿ç”¨å¤šå°‘ä¸ªä¸åŒçš„æ¯”ä¾‹
        pool_size   # ç©ºé—´é‡‘å­—å¡”ä¸­æ¯ä¸ªçº§åˆ«çš„æ± åŒ–åŒºåŸŸçš„å¤§å°ã€‚å®ƒå¯ä»¥æ˜¯æ­£æ–¹å½¢åŒºåŸŸçš„å•ä¸ªå€¼ï¼Œä¹Ÿå¯ä»¥æ˜¯çŸ©å½¢åŒºåŸŸçš„å…ƒç»„ï¼ˆé«˜åº¦ã€å®½åº¦ï¼‰
    ):
        super(SPP, self).__init__()
        self.pool = []  # åˆå§‹åŒ–åœ¨ SPP æ¨¡å—ä¸­è°ƒç”¨çš„ç©ºåˆ—è¡¨ã€‚æ­¤åˆ—è¡¨å°†ç”¨äºå­˜å‚¨å›¾å±‚
        for i, size in enumerate(pool_size):    # éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆå¤§å°ï¼‰åŠå…¶ç´¢å¼•
            pool = nn.MaxPool2d(kernel_size=size,   # MaxPool2dä½¿ç”¨æŒ‡å®šå‚æ•°åˆ›å»ºå›¾å±‚    kernel_sizeï¼šæ± åŒ–çª—å£çš„å¤§å°
                                stride=1,   # strideï¼šæ± åŒ–æ“ä½œçš„æ­¥å¹…
                                padding=size // 2,  # paddingï¼šåœ¨åº”ç”¨æ± åŒ–ä¹‹å‰æ·»åŠ åˆ°è¾“å…¥çš„å¡«å……
                                ceil_mode=False)    # ceil_modeï¼šæ˜¯å¦ä½¿ç”¨ ceil å‡½æ•°è®¡ç®—è¾“å‡ºå¤§å°ï¼ˆè®¾ç½®ä¸º Falseï¼‰
            self.add_module('pool{}'.format(i), pool)   # å°†åˆ›å»ºçš„å›¾å±‚ä½œä¸ºå­æ¨¡å—æ·»åŠ åˆ° SPP æ¨¡å—ä¸­ï¼Œå¹¶æ ¹æ®ç´¢å¼•å°†å…¶å‘½åä¸º 'pool0'ã€'pool1' ç­‰
            self.pool.append(pool)  # å°†åˆ›å»ºçš„å›¾å±‚è¿½åŠ åˆ°åˆ—è¡¨ä¸­ä»¥ä¾›å°†æ¥å‚è€ƒ
        self.conv = Conv(ch_in, ch_out, k)

    # å‰å‘ä¼ é€’æ¶‰åŠå¯¹è¾“å…¥å¼ é‡åº”ç”¨å¤šä¸ª max-pooling æ“ä½œï¼Œè¿æ¥ç»“æœï¼Œç„¶ååº”ç”¨å·ç§¯è¿ç®—ã€‚æ­¤è¿‡ç¨‹ä½¿ SPP æ¨¡å—èƒ½å¤Ÿä»è¾“å…¥å¼ é‡ä¸­æ•è·å¤šå°ºåº¦ä¿¡æ¯
    def forward(self, x):
        # outs = [x]ï¼šä½¿ç”¨è¾“å…¥å¼ é‡åˆå§‹åŒ–åˆ—è¡¨ã€‚æ­¤åˆ—è¡¨å°†ç”¨äºå­˜å‚¨ max-pooling æ“ä½œçš„ç»“æœ
        outs = [x]

        for pool in self.pool:  # éå†åˆ—è¡¨ä¸­å­˜å‚¨çš„å›¾å±‚
            outs.append(pool(x))    # å°†æ¯ä¸ª max-pooling æ“ä½œåº”ç”¨äºè¾“å…¥å¼ é‡ï¼Œå¹¶å°†ç»“æœé™„åŠ åˆ°åˆ—è¡¨ä¸­
        y = torch.cat(outs, axis=1)     # æ²¿é€šé“ç»´åº¦ï¼ˆaxis=1ï¼‰è¿æ¥åˆ—è¡¨ä¸­çš„æ‰€æœ‰å¼ é‡ã€‚è¿™ç§ä¸²è”å½¢æˆäº†ç©ºé—´é‡‘å­—å¡”æ± åŒ–çš„è¾“å‡ºå¼ é‡ã€‚

        y = self.conv(y)    # å°†å·ç§¯è¿ç®— ï¼ˆï¼‰ åº”ç”¨äºä¸²è”çš„å¼ é‡ã€‚æ­¤å·ç§¯è¿ç®—æ˜¯ SPP æ¨¡å—çš„ä¸€éƒ¨åˆ†
        return y

# å‡å¦‚è¾“å…¥æ˜¯1*256*20*20
class CSPStage(nn.Module):
    def __init__(self,
                 ch_in,
                 ch_out,
                 n,     # æŒ‡å®šå—å‡½æ•°çš„é‡å¤æ¬¡æ•° ï¼ˆblock_fn)
                 block_fn='BasicBlock_3x3_Reverse',     # è¦ä½¿ç”¨çš„å—å‡½æ•°çš„ç±»å‹ã€‚ä¸Šé¢æœ‰æ¨¡å—çš„å…·ä½“è§£é‡Š
                 ch_hidden_ratio=1.0,   #  ç”¨äºç¡®å®šå—ä¸­éšè—é€šé“æ•°çš„æ¯”ç‡
                 act='silu',
                 spp=False):    # æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨ç©ºé—´é‡‘å­—å¡”æ±  ï¼ˆSPPï¼‰ çš„å¸ƒå°”å€¼
        super(CSPStage, self).__init__()

        split_ratio = 2     # å®šä¹‰ç”¨äºå°†è¾“å‡ºé€šé“åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†çš„åˆ†æµæ¯”
        ch_first = int(ch_out // split_ratio)   # æ ¹æ®åˆ†æµæ¯”è®¡ç®—ç¬¬ä¸€éƒ¨åˆ†çš„é€šé“æ•° 128/2=64
        ch_mid = int(ch_out - ch_first)     # è®¡ç®—ç¬¬äºŒéƒ¨åˆ†çš„é€šé“æ•°    64
        # conv1å’Œconv2æ˜¯ä¸¤ä¸ªå¹¶è¡Œçš„1*1å·ç§¯ï¼Œä¸¤ä¸ªå·ç§¯è¿›è¡Œé™ç»´ï¼Œç„¶åä»£è¡¨çš„ä¸¤æ¡æ”¯è·¯æ‹¼æ¥åˆ°ä¸€èµ·
        self.conv1 = Conv(ch_in, ch_first, 1)
        self.conv2 = Conv(ch_in, ch_mid, 1)

        self.convs = nn.Sequential()    # æ„Ÿè§‰æ˜¯åˆ›å»ºä¸€ä¸ªç©ºç™½çš„æ¨¡å— æ˜¯ä¸€ä¸ªå®¹å™¨ï¼Œå¯ä»¥å®¹çº³æ¨¡å—ï¼ˆå±‚ï¼‰çš„æœ‰åºåºåˆ—ã€‚å®ƒå…è®¸æ‚¨ä»¥ç´§å‡‘çš„æ–¹å¼å®šä¹‰ä¸€ç³»åˆ—æ“ä½œ

        next_ch_in = ch_mid   # ä¸‹ä¸€ä¸ªå·ç§¯çš„è¾“å…¥é€šé“æ•°
        for i in range(n):      # å¾ªç¯è¿­ä»£æ¬¡æ•°    # å¯¹äºæ¯æ¬¡è¿­ä»£ï¼Œéƒ½ä¼šå°†ä¸€ä¸ªå—æ·»åŠ åˆ°é¡ºåºå®¹å™¨
            # å—ç±»å‹ç”±block_fnå‚æ•°ã€‚å¦‚æœè®¾ç½®ä¸º ï¼Œåˆ™'BasicBlock_3x3_Reverse'BasicBlock_3x3_Reverseç±»è¢«åˆ›å»ºå¹¶æ·»åŠ åˆ°å®¹å™¨ä¸­
            if block_fn == 'BasicBlock_3x3_Reverse':
                #  BasicBlock_3x3_Reverseç±»è¢«æ·»åŠ åˆ°self.convsï¼Œç›¸å½“äºåœ¨nn.Sequential()é‡Œé¢åŠ ä¸€äº›æ¨¡å—
                self.convs.add_module(
                    # The str(i)ç”¨ä½œæ·»åŠ åˆ°â€œselfâ€çš„æ¯ä¸ªå—çš„åç§°self.convs sequence. In Python, str(i)è½¬æ¢iåˆ°å…¶å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ã€‚
                    # ç„¶åï¼Œæ­¤å­—ç¬¦ä¸²è¡¨ç¤ºå½¢å¼ç”¨ä½œ â€nn.Sequential container (self.convs).
                    # For example, when  is 0, the first block added will be named '0'. When iiä¸º 1ï¼Œæ·»åŠ çš„ç¬¬äºŒä¸ªå—å°†å‘½åä¸º '1'ï¼Œ
                    # ä¾æ­¤ç±»æ¨ã€‚æ­¤å‘½åçº¦å®šæœ‰åŠ©äºå”¯ä¸€æ ‡è¯†æ¯ä¸ªå—nn.Sequential container.
                    str(i),
                    BasicBlock_3x3_Reverse(next_ch_in,
                                           ch_hidden_ratio,
                                           ch_mid,
                                           shortcut=True))
            else:
                # ç”¨äºæŒ‡ç¤ºå°šæœªæ”¯æŒæˆ–å®ç°ç‰¹å®šåŠŸèƒ½æˆ–å®ç°ã€‚æ‰§è¡Œæ­¤è¯­å¥æ—¶ï¼Œå®ƒä¼šå¼•å‘å¼‚å¸¸ï¼Œå‘å¼€å‘äººå‘˜æˆ–ç”¨æˆ·å‘å‡ºä¿¡å·ï¼Œè¡¨æ˜ä»–ä»¬å°è¯•ä½¿ç”¨çš„åŠŸèƒ½ä¸å¯ç”¨
                raise NotImplementedError
            # å¦‚æœæ»¡è¶³ä¸‹é¢çš„ä¸¤ä¸ªæ¡ä»¶
            if i == (n - 1) // 2 and spp:
                # åœ¨å·ç§¯convsåæ·»åŠ SPPæ¨¡å—
                self.convs.add_module('spp', SPP(ch_mid * 4, ch_mid, 1, [5, 9, 13]))
            next_ch_in = ch_mid
        # åœ¨å¾ªç¯ä¹‹åï¼Œå®ƒå‘æ¨¡å—æ·»åŠ å¦ä¸€ä¸ªå·ç§¯å±‚ ï¼ˆï¼‰ã€‚è¯¥å±‚ä¼¼ä¹é€šè¿‡å°†å‰ä¸€ä¸ªå— ï¼ˆï¼‰ çš„è¾“å‡ºé€šé“ä¸åˆå§‹é€šé“ ï¼ˆï¼‰ ç»„åˆåœ¨ä¸€èµ·æ¥èšåˆæ¥è‡ªå—çš„ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆå¸¦æœ‰é€šé“çš„è¾“å‡ºã€‚
        # æ­¤å·ç§¯çš„å†…æ ¸å¤§å°ä¸º 1ï¼Œè¿™æ„å‘³ç€å®ƒæ˜¯ 1x1 å·ç§¯ï¼Œ è¿›è¡Œé€šé“æ•°çš„é™ç»´
        self.conv3 = Conv(ch_mid * n + ch_first, ch_out, 1)

    def forward(self, x):
        # y1å’Œy2åˆ†åˆ«ç”±è¾“å…¥xç»è¿‡ä¸åŒçš„å·ç§¯å¾—åˆ°
        y1 = self.conv1(x)
        y2 = self.conv2(x)

        # åˆå§‹åŒ–ä¸ºåŒ…å«y1ç»“æœçš„åˆ—è¡¨
        mid_out = [y1]
        # ä»£ç å¾ªç¯è®¿é—®è¯¥æ¨¡å—ï¼Œè¯¥æ¨¡å—å¯èƒ½åŒ…å«BasicBlock_3x3_Reverseæ¨¡å—ï¼Œå¯èƒ½è¿˜åŒ…å«ä¸€ä¸ª SPP æ¨¡å—
        for conv in self.convs:
            y2 = conv(y2)
            # å¯¹äºæ¯æ¬¡è¿­ä»£ï¼Œå®ƒå°†å½“å‰å·ç§¯å—çš„ç»“æœé™„åŠ åˆ°mid_out
            # ç›¸å½“äºç¬¬ä¸€ä¸ªBasicBlock_3x3_Reverseçš„è¾“å‡ºæ‰ä¼šæ¥åˆ°ç»“å°¾
            mid_out.append(y2)
        # æ²¿ç»´åº¦ä¸º1å°† mid_outè¿åˆ°ä¸€èµ·
        y = torch.cat(mid_out, axis=1)
        y = self.conv3(y)
        return y

######################################## DAMO-YOLO GFPN end ########################################

######################################## DCNV2_Dynamic start ########################################

class MPCA(nn.Module):
    # MultiPath Coordinate Attention
    def __init__(self, channels) -> None:
        super().__init__()

        self.gap = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            Conv(channels, channels)
        )
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        self.conv_hw = Conv(channels, channels, (3, 1))
        self.conv_pool_hw = Conv(channels, channels, 1)

    def forward(self, x):
        _, _, h, w = x.size()
        x_pool_h, x_pool_w, x_pool_ch = self.pool_h(x), self.pool_w(x).permute(0, 1, 3, 2), self.gap(x)
        x_pool_hw = torch.cat([x_pool_h, x_pool_w], dim=2)
        x_pool_hw = self.conv_hw(x_pool_hw)
        x_pool_h, x_pool_w = torch.split(x_pool_hw, [h, w], dim=2)
        x_pool_hw_weight = self.conv_pool_hw(x_pool_hw).sigmoid()
        x_pool_h_weight, x_pool_w_weight = torch.split(x_pool_hw_weight, [h, w], dim=2)
        x_pool_h, x_pool_w = x_pool_h * x_pool_h_weight, x_pool_w * x_pool_w_weight
        x_pool_ch = x_pool_ch * torch.mean(x_pool_hw_weight, dim=2, keepdim=True)
        return x * x_pool_h.sigmoid() * x_pool_w.permute(0, 1, 3, 2).sigmoid() * x_pool_ch.sigmoid()


class DCNv2_Offset_Attention(nn.Module):
    def __init__(self, in_channels, kernel_size, stride, deformable_groups=1) -> None:
        super().__init__()

        padding = autopad(kernel_size, None, 1)
        self.out_channel = (deformable_groups * 3 * kernel_size * kernel_size)

        # # å…ˆæ³¨æ„åŠ›åå·ç§¯
        self.attention = li(in_channels)

        # # å…ˆå·ç§¯åæ³¨æ„åŠ›
        #self.attention = li(self.out_channel)

        self.conv_offset_mask = nn.Conv2d(in_channels, self.out_channel, kernel_size, stride, padding, bias=True)
        #self.attention = MPCA(self.out_channel)
        #self.attention = CA(self.out_channel)


    def forward(self, x):

        # æ³¨æ„åŠ›æœºåˆ¶ï¼Œåå·ç§¯
        conv_offset_mask = self.attention(x)
        conv_offset_mask = self.conv_offset_mask(conv_offset_mask)

        # (b,c,h,w)=x.size()
        # print(b,c,h,w)
        # #å…ˆæ˜¯å·ç§¯ï¼Œåœ¨æ˜¯æ³¨æ„åŠ›æœºåˆ¶
        # conv_offset_mask = self.conv_offset_mask(x)
        # conv_offset_mask = self.attention(conv_offset_mask)

        # b1,c1,h1,w1 = conv_offset_mask.size()
        # print(1,b1,c1,h1,w1)
        # conv_offset_mask = self.attention(x)
        # conv_offset_mask = self.conv_offset_mask(conv_offset_mask)
        # b1,c1,h1,w1 = conv_offset_mask.size()
        # print(2,b1,c1,h1,w1)
        return conv_offset_mask


class DCNv2_Dynamic(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1,
                 padding=None, groups=1, dilation=1, act=True, deformable_groups=1):
        super(DCNv2_Dynamic, self).__init__()

        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = (kernel_size, kernel_size)
        self.stride = (stride, stride)
        padding = autopad(kernel_size, padding, dilation)
        self.padding = (padding, padding)
        self.dilation = (dilation, dilation)
        self.groups = groups
        self.deformable_groups = deformable_groups

        self.weight = nn.Parameter(
            torch.empty(out_channels, in_channels, *self.kernel_size)
        )
        self.bias = nn.Parameter(torch.empty(out_channels))

        self.conv_offset_mask = DCNv2_Offset_Attention(in_channels, kernel_size, stride, deformable_groups)

        self.bn = nn.BatchNorm2d(out_channels)
        self.act = Conv.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
        self.reset_parameters()

    def forward(self, x):
        offset_mask = self.conv_offset_mask(x)
        # b1,c1,h1,w1 = offset_mask.size()
        # print(1,b1,c1,h1,w1)
        o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)
        offset = torch.cat((o1, o2), dim=1)
        mask = torch.sigmoid(mask)
        # x = torch.ops.torchvision.deform_conv2d(
        #     x,
        #     self.weight,
        #     offset,
        #     mask,
        #     self.bias,
        #     self.stride[0], self.stride[1],
        #     self.padding[0], self.padding[1],
        #     self.dilation[0], self.dilation[1],
        #     self.groups,
        #     self.deformable_groups,
        #     True
        # )
        # x = self.bn(x)
        # x = self.act(x)
        x = torchvision.ops.deform_conv2d(input=x,
                                          offset=offset,
                                          weight=self.weight,
                                          bias=self.bias,
                                          padding=self.padding,
                                          mask=mask,
                                          stride=self.stride)
        return x

    def reset_parameters(self):
        n = self.in_channels
        for k in self.kernel_size:
            n *= k
        std = 1. / math.sqrt(n)
        self.weight.data.uniform_(-std, std)
        self.bias.data.zero_()
        self.conv_offset_mask.conv_offset_mask.weight.data.zero_()
        self.conv_offset_mask.conv_offset_mask.bias.data.zero_()

class Bottleneck_DCNV2_Dynamic(Bottleneck):
    """Standard bottleneck with DCNV2."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv2 = DCNv2_Dynamic(c_, c2, k[1], 1)

class C3_DCNv2_Dynamic(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_DCNV2_Dynamic(c_, c_, shortcut, g, k=(1, 3), e=1.0) for _ in range(n)))

class C2f_DCNv2_Dynamic(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_DCNV2_Dynamic(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))


######################################## DCNV2_Dynamic end ########################################


# ######################################## DCNV2_Dynamic start ########################################
#
# class MPCA(nn.Module):
#     # MultiPath Coordinate Attention
#     def __init__(self, channels) -> None:
#         super().__init__()
#
#         self.gap = nn.Sequential(
#             nn.AdaptiveAvgPool2d((1, 1)),
#             Conv(channels, channels)
#         )
#         self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
#         self.pool_w = nn.AdaptiveAvgPool2d((1, None))
#         self.conv_hw = Conv(channels, channels, (3, 1))
#         self.conv_pool_hw = Conv(channels, channels, 1)
#
#     def forward(self, x):
#         _, _, h, w = x.size()
#         x_pool_h, x_pool_w, x_pool_ch = self.pool_h(x), self.pool_w(x).permute(0, 1, 3, 2), self.gap(x)
#         x_pool_hw = torch.cat([x_pool_h, x_pool_w], dim=2)
#         x_pool_hw = self.conv_hw(x_pool_hw)
#         x_pool_h, x_pool_w = torch.split(x_pool_hw, [h, w], dim=2)
#         x_pool_hw_weight = self.conv_pool_hw(x_pool_hw).sigmoid()
#         x_pool_h_weight, x_pool_w_weight = torch.split(x_pool_hw_weight, [h, w], dim=2)
#         x_pool_h, x_pool_w = x_pool_h * x_pool_h_weight, x_pool_w * x_pool_w_weight
#         x_pool_ch = x_pool_ch * torch.mean(x_pool_hw_weight, dim=2, keepdim=True)
#         return x * x_pool_h.sigmoid() * x_pool_w.permute(0, 1, 3, 2).sigmoid() * x_pool_ch.sigmoid()
#
#
# class DCNv2_Offset_Attention(nn.Module):
#     # deformable_groups:å¯å˜å½¢å·ç§¯ç»„æ•°
#     def __init__(self, in_channels, kernel_size, stride, deformable_groups=1) -> None:
#         super().__init__()
#
#         padding = autopad(kernel_size, None, 1)
#         self.out_channel = (deformable_groups * 3 * kernel_size * kernel_size)  # 27
#         self.conv_offset_mask = nn.Conv2d(in_channels, self.out_channel, kernel_size, stride, padding, bias=True)
#         self.attention = CA(self.out_channel)
#
#     def forward(self, x):
#         # åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œå°† conv_offset_mask å±‚åº”ç”¨äºè¾“å…¥ xï¼Œç„¶åå°†è¾“å‡ºé€šè¿‡æ³¨æ„åŠ›æœºåˆ¶ (self.attention)ã€‚æœ€ç»ˆç»“æœè¢«è¿”å›
#         conv_offset_mask = self.conv_offset_mask(x)
#         conv_offset_mask = self.attention(conv_offset_mask)
#         b, c, h, w = conv_offset_mask.size()
#         print(1,b, c, h, w)
#         return conv_offset_mask
#
#
# class DCNv2_Dynamic(nn.Module):
#     def __init__(self, in_channels, out_channels, kernel_size, stride=1,
#                  padding=None, groups=1, dilation=1, act=True, deformable_groups=1):
#         super(DCNv2_Dynamic, self).__init__()
#
#         self.in_channels = in_channels
#         self.out_channels = out_channels
#         self.kernel_size = (kernel_size, kernel_size)
#         self.stride = (stride, stride)
#         # ç®€åŒ–paddingçš„é…ç½®è¿‡ç¨‹ï¼Œä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®ï¼Œä½†æ­¤å·ç§¯è¾“å‡ºç‰¹å¾å›¾å¤§å°ä¸ä¸€å®šå’Œè¾“å…¥ç›¸åŒ
#         padding = autopad(kernel_size, padding, dilation)
#         self.padding = (padding, padding)
#         self.dilation = (dilation, dilation)
#         self.groups = groups
#         self.deformable_groups = deformable_groups
#
#         self.weight = nn.Parameter(
#             torch.empty(out_channels, in_channels, *self.kernel_size)
#         )
#         self.bias = nn.Parameter(torch.empty(out_channels))
#
#         self.conv_offset_mask = DCNv2_Offset_Attention(in_channels, kernel_size, stride, deformable_groups)
#         self.bn = nn.BatchNorm2d(out_channels)
#         self.act = Conv.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
#         self.reset_parameters()
#
#     def forward(self, x):
#         offset_mask = self.conv_offset_mask(x)
#         o1, o2, mask = torch.chunk(offset_mask, 3, dim=1)
#         offset = torch.cat((o1, o2), dim=1)
#         mask = torch.sigmoid(mask)
#         x = torch.ops.torchvision.deform_conv2d(
#             x,
#             self.weight,
#             offset,
#             mask,
#             self.bias,
#             self.stride[0], self.stride[1],
#             self.padding[0], self.padding[1],
#             self.dilation[0], self.dilation[1],
#             self.groups,
#             self.deformable_groups,
#             True
#         )
#         x = self.bn(x)
#         x = self.act(x)
#         return x
#
#     def reset_parameters(self):
#         n = self.in_channels
#         for k in self.kernel_size:
#             n *= k
#         std = 1. / math.sqrt(n)
#         self.weight.data.uniform_(-std, std)
#         self.bias.data.zero_()
#         self.conv_offset_mask.conv_offset_mask.weight.data.zero_()
#         self.conv_offset_mask.conv_offset_mask.bias.data.zero_()
#
# class Bottleneck_DCNV2_Dynamic(Bottleneck):
#     """Standard bottleneck with DCNV2."""
#
#     def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
#         super().__init__(c1, c2, shortcut, g, k, e)
#         c_ = int(c2 * e)  # hidden channels
#         self.cv2 = DCNv2_Dynamic(c_, c2, k[1], 1)
#
# class C3_DCNv2_Dynamic(C3):
#     def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
#         super().__init__(c1, c2, n, shortcut, g, e)
#         c_ = int(c2 * e)  # hidden channels
#         self.m = nn.Sequential(*(Bottleneck_DCNV2_Dynamic(c_, c_, shortcut, g, k=(1, 3), e=1.0) for _ in range(n)))
#
# class C2f_DCNv2_Dynamic(C2f):
#     def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
#         super().__init__(c1, c2, n, shortcut, g, e)
#         self.m = nn.ModuleList(Bottleneck_DCNV2_Dynamic(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))
#
#
# ######################################## DCNV2_Dynamic end ########################################

######################################## C3 C2f FocusedLinearAttention end ########################################

class Bottleneck_FocusedLinearAttention(Bottleneck):
    """Standard bottleneck with FocusedLinearAttention."""

    def __init__(self, c1, c2, fmapsize, shortcut=True, g=1, k=(3, 3),
                 e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.attention = FocusedLinearAttention(c2, fmapsize)

    def forward(self, x):
        return x + self.attention(self.cv2(self.cv1(x))) if self.add else self.attention(self.cv2(self.cv1(x)))


class C3_FocusedLinearAttention(C3):
    def __init__(self, c1, c2, n=1, fmapsize=None, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(
            *(Bottleneck_FocusedLinearAttention(c_, c_, fmapsize, shortcut, g, k=(1, 3), e=1.0) for _ in range(n)))


class C2f_FocusedLinearAttention(C2f):
    def __init__(self, c1, c2, n=1, fmapsize=None, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(
            Bottleneck_FocusedLinearAttention(self.c, self.c, fmapsize, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## C3 C2f FocusedLinearAttention end ########################################


######################################## GOLD-YOLO start ########################################
# conv_bn åˆ›å»ºconvå’Œbnæ¨¡å—ï¼Œç”¨äºRepVGGBlockä¸­
def conv_bn(in_channels, out_channels, kernel_size, stride, padding, groups=1, bias=False):
    '''Basic cell for rep-style block, including conv and bn'''
    result = nn.Sequential()
    result.add_module('conv', nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                                        kernel_size=kernel_size, stride=stride, padding=padding, groups=groups,
                                        bias=bias))
    result.add_module('bn', nn.BatchNorm2d(num_features=out_channels))
    return result

# å…·ä½“çš„ç»“æ„çœ‹å›¾ï¼Œä»£ç åˆè‡­åˆé•¿
class RepVGGBlock(nn.Module):
    '''RepVGGBlock is a basic rep-style block, including training and deploy status
    This code is based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py
    '''

    def __init__(self, in_channels, out_channels, kernel_size=3,
                 stride=1, padding=1, dilation=1, groups=1, padding_mode='zeros', deploy=False, use_se=False):
        super(RepVGGBlock, self).__init__()
        """ Initialization of the class.
        Args:
            in_channels (int): Number of channels in the input image
            out_channels (int): Number of channels produced by the convolution
            kernel_size (int or tuple): Size of the convolving kernel
            stride (int or tuple, optional): Stride of the convolution. Default: 1
            padding (int or tuple, optional): Zero-padding added to both sides of
                the input. Default: 1
            dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
            groups (int, optional): Number of blocked connections from input
                channels to output channels. Default: 1
            padding_mode (string, optional): Default: 'zeros'
            deploy: Whether to be deploy status or training status. Default: False
            use_se: Whether to use se. Default: False
        """
        self.deploy = deploy
        self.groups = groups
        self.in_channels = in_channels
        self.out_channels = out_channels

        assert kernel_size == 3
        assert padding == 1

        padding_11 = padding - kernel_size // 2

        self.nonlinearity = nn.ReLU()

        if use_se:
            raise NotImplementedError("se block not supported yet")
        else:
            self.se = nn.Identity()

        if deploy:
            self.rbr_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,
                                         stride=stride,
                                         padding=padding, dilation=dilation, groups=groups, bias=True,
                                         padding_mode=padding_mode)

        else:
            self.rbr_identity = nn.BatchNorm2d(
                num_features=in_channels) if out_channels == in_channels and stride == 1 else None
            self.rbr_dense = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,
                                     stride=stride, padding=padding, groups=groups)
            self.rbr_1x1 = conv_bn(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride,
                                   padding=padding_11, groups=groups)

    def forward(self, inputs):
        '''Forward process'''
        if hasattr(self, 'rbr_reparam'):
            return self.nonlinearity(self.se(self.rbr_reparam(inputs)))

        if self.rbr_identity is None:
            id_out = 0
        else:
            id_out = self.rbr_identity(inputs)

        return self.nonlinearity(self.se(self.rbr_dense(inputs) + self.rbr_1x1(inputs) + id_out))

    def get_equivalent_kernel_bias(self):
        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.rbr_dense)
        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.rbr_1x1)
        kernelid, biasid = self._fuse_bn_tensor(self.rbr_identity)
        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid

    def _pad_1x1_to_3x3_tensor(self, kernel1x1):
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])

    def _fuse_bn_tensor(self, branch):
        if branch is None:
            return 0, 0
        if isinstance(branch, nn.Sequential):
            kernel = branch.conv.weight
            running_mean = branch.bn.running_mean
            running_var = branch.bn.running_var
            gamma = branch.bn.weight
            beta = branch.bn.bias
            eps = branch.bn.eps
        else:
            assert isinstance(branch, nn.BatchNorm2d)
            if not hasattr(self, 'id_tensor'):
                input_dim = self.in_channels // self.groups
                kernel_value = np.zeros((self.in_channels, input_dim, 3, 3), dtype=np.float32)
                for i in range(self.in_channels):
                    kernel_value[i, i % input_dim, 1, 1] = 1
                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std

    def switch_to_deploy(self):
        if hasattr(self, 'rbr_reparam'):
            return
        kernel, bias = self.get_equivalent_kernel_bias()
        self.rbr_reparam = nn.Conv2d(in_channels=self.rbr_dense.conv.in_channels,
                                     out_channels=self.rbr_dense.conv.out_channels,
                                     kernel_size=self.rbr_dense.conv.kernel_size, stride=self.rbr_dense.conv.stride,
                                     padding=self.rbr_dense.conv.padding, dilation=self.rbr_dense.conv.dilation,
                                     groups=self.rbr_dense.conv.groups, bias=True)
        self.rbr_reparam.weight.data = kernel
        self.rbr_reparam.bias.data = bias
        for para in self.parameters():
            para.detach_()
        self.__delattr__('rbr_dense')
        self.__delattr__('rbr_1x1')
        if hasattr(self, 'rbr_identity'):
            self.__delattr__('rbr_identity')
        if hasattr(self, 'id_tensor'):
            self.__delattr__('id_tensor')
        self.deploy = True
# ä¸Šé¢æ˜¯RepVGGBlock

# è‡ªé€‚åº”å¹³å‡æ± åŒ–  output_sizeï¼šè¾“å‡ºå¤§å°
def onnx_AdaptiveAvgPool2d(x, output_size):
    # x.shape[-2:]ï¼šæå–è¾“å…¥å¼ é‡ x å½¢çŠ¶çš„æœ€åä¸¤ä¸ªç»´åº¦hå’Œw
    # np.array(...)ï¼šå°†æå–çš„ç»´åº¦è½¬æ¢ä¸º NumPy æ•°ç»„
    # np.floor(...)ï¼šåº”ç”¨ä¸‹å–æ•´å‡½æ•°å°†é™¤æ³•ç»“æœå‘ä¸‹èˆå…¥åˆ°æœ€æ¥è¿‘çš„æ•´æ•°ã€‚ è¿™ç¡®ä¿äº†æ­¥å¹…æ˜¯æ•´æ•°å€¼
    # .astype(np.int32)ï¼šå°†ç»“æœè½¬æ¢ä¸ºint32æ•°æ®ç±»å‹ã€‚ è¿™ä¸€æ­¥æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºæ­¥å¹…åº”è¯¥æ˜¯æ•´æ•°
    stride_size = np.floor(np.array(x.shape[-2:]) / output_size).astype(np.int32)
    kernel_size = np.array(x.shape[-2:]) - (output_size - 1) * stride_size
    # ä½¿ç”¨è®¡ç®—å‡ºçš„å†…æ ¸å¤§å°å’Œæ­¥å¹…å¤§å°åˆ›å»º nn.AvgPool2d
    avg = nn.AvgPool2d(kernel_size=list(kernel_size), stride=list(stride_size))
    x = avg(x)
    return x


def get_avg_pool():
    if torch.onnx.is_in_onnx_export():
        avg_pool = onnx_AdaptiveAvgPool2d
    else:
        avg_pool = nn.functional.adaptive_avg_pool2d
    return avg_pool

# ä»¥ç¬¬äºŒä¸ªé€šé“çš„å¤§å°ä¸ºæ ‡å‡† è¿™ä¸ªæ¨¡å—å’ŒSimFusion_4inå¾ˆåƒï¼Œåªæ˜¯ä¸‹é¢è¿™ä¸ªåªæœ‰ä¸‰ä¸ªè¾“å…¥é€šé“
class SimFusion_3in(nn.Module):
    def __init__(self, in_channel_list, out_channels):
        super().__init__()
        # ä»¥ä¸‹çš„ä¸‰ä¸ªå·ç§¯ç”¨äºåˆ¤æ–­in_channel_listæ˜¯å¦å¯¹äºout_channelsï¼Œå‡å¦‚ä¸ç­‰äºï¼Œéœ€è¦ç»è¿‡ä¸€ä¸ªå·ç§¯ï¼Œä½¿å¾—è¾“å…¥é€šé“æ•°=è®¾å®šçš„out_channels
        self.cv1 = Conv(in_channel_list[0], out_channels, act=nn.ReLU()) if in_channel_list[
                                                                                0] != out_channels else nn.Identity()
        self.cv2 = Conv(in_channel_list[1], out_channels, act=nn.ReLU()) if in_channel_list[
                                                                                1] != out_channels else nn.Identity()
        self.cv3 = Conv(in_channel_list[2], out_channels, act=nn.ReLU()) if in_channel_list[
                                                                                2] != out_channels else nn.Identity()
        self.cv_fuse = Conv(out_channels * 3, out_channels, act=nn.ReLU())
        self.downsample = nn.functional.adaptive_avg_pool2d

    def forward(self, x):
        N, C, H, W = x[1].shape
        output_size = (H, W)

        if torch.onnx.is_in_onnx_export():
            self.downsample = onnx_AdaptiveAvgPool2d
            output_size = np.array([H, W])

        # x0ä¸‹é‡‡æ ·åï¼Œçœ‹è¾“å…¥é€šé“æ•°æ˜¯å¦å¯¹äºè®¾å®šå€¼ï¼Œä¸ç­‰äºå°±ç»è¿‡ä¸€ä¸ªå·ç§¯æ“ä½œ
        x0 = self.cv1(self.downsample(x[0], output_size))
        x1 = self.cv2(x[1])
        x2 = self.cv3(F.interpolate(x[2], size=(H, W), mode='bilinear', align_corners=False))
        return self.cv_fuse(torch.cat((x0, x1, x2), dim=1))


# ä»ç»“æ„å›¾æ¥è¯´ï¼Œå¯¹å«æœ‰å››ä¸ªä¸åŒè¾“å…¥çš„xï¼Œå°†å…¶Hå’ŒWæ•´åˆ°ä¸€æ ·å¤§å°ï¼ˆä»¥ç¬¬ä¸‰ä¸ªé€šé“çš„å¤§å°ä¸ºæ ‡å‡†ï¼‰ï¼Œç„¶åè¿›è¡Œcontact
class SimFusion_4in(nn.Module):
    def __init__(self):
        super().__init__()
        # adaptive_avg_pool2d  è¯¥å‡½æ•°å¯¹è¾“å…¥å¼ é‡æ‰§è¡Œè‡ªé€‚åº”å¹³å‡æ± åŒ–
        self.avg_pool = nn.functional.adaptive_avg_pool2d

    #
    def forward(self, x):
        # x_l, x_m, x_s, x_n å°±å¯¹ç”¨yamlæ–‡ä»¶ä¸­çš„[2, 4, 6, 9]--ä¸åŒé€šé“çš„è¾“å‡ºï¼Œè¿™å››ä¸ªçš„channelæ˜¯ä¸ä¸€æ ·çš„ï¼Œæ˜¯æ»¡è¶³contactçš„è¦æ±‚çš„
        # [2, 4, 6, 9]å¯¹åº”çš„hå’ŒWåˆ†åˆ«æ˜¯160*160ï¼Œ80*80ï¼Œ40*40ï¼Œ20*20
        x_l, x_m, x_s, x_n = x
        B, C, H, W = x_s.shape
        # è¾“å‡ºçš„å¤§å°å–ç¬¬ä¸‰ä¸ªè¾“å…¥çš„hå’Œwçš„å¤§å°
        output_size = np.array([H, W])

        # é»˜è®¤yamlä¸‹ï¼Œä¸è¿›å…¥ifé‡Œé¢çš„è¯­å¥
        # æ‚¨æä¾›çš„ä»£ç æ£€æŸ¥ PyTorch æ¨¡å‹å½“å‰æ˜¯å¦æ­£åœ¨å¯¼å‡ºä¸º ONNX æ ¼å¼ã€‚
        # å¦‚æœæ˜¯ï¼Œå®ƒä½¿ç”¨è‡ªå®šä¹‰å‡½æ•° onnx_AdaptiveAvgPool2d è¿›è¡Œè‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼Œ
        # è€Œä¸æ˜¯æ ‡å‡† PyTorch nn.function.adaptive_avg_pool2d
        if torch.onnx.is_in_onnx_export():
            self.avg_pool = onnx_AdaptiveAvgPool2d

        # å¯¹x_lå’Œx_mè¿›è¡Œè‡ªåŠ¨å¹³å‡æ± åŒ–ï¼Œè¾“å‡ºçš„Hå’ŒWå¤§å°ä¸ºoutput_size
        x_l = self.avg_pool(x_l, output_size)
        x_m = self.avg_pool(x_m, output_size)
        x_n = F.interpolate(x_n, size=(H, W), mode='bilinear', align_corners=False)

        out = torch.cat([x_l, x_m, x_s, x_n], 1)
        return out


# åœ¨yamlæ–‡ä»¶ä¸­ï¼Œç±»ä¼¼äºc2fçš„ä½œç”¨
# oucåœ¨yamlæ–‡ä»¶ä¸­é»˜è®¤æ˜¯[64, 32]ï¼Œæ‰€ä»¥ä¸‹é¢æ‰ç”¨sum(ouc)     fuse_block_numæ˜¯RepVGGBlockçš„é‡å¤æ¬¡æ•°
class IFM(nn.Module):
    def __init__(self, inc, ouc, embed_dim_p=96, fuse_block_num=3) -> None:
        super().__init__()

        # å…ˆæ˜¯å·ç§¯ï¼Œå¤šä¸ªRepVGGBlockï¼Œå·ç§¯
        self.conv = nn.Sequential(
            Conv(inc, embed_dim_p),
            *[RepVGGBlock(embed_dim_p, embed_dim_p) for _ in range(fuse_block_num)],
            Conv(embed_dim_p, sum(ouc)),
        )

    def forward(self, x):
        return self.conv(x)


class h_sigmoid(nn.Module):
    def __init__(self, inplace=True):
        super(h_sigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace=inplace)

    def forward(self, x):
        return self.relu(x + 3) / 6


class InjectionMultiSum_Auto_pool(nn.Module):
    def __init__(
            self,
            inp: int,
            oup: int,
            global_inp: list,
            flag: int
    ) -> None:
        super().__init__()
        self.global_inp = global_inp
        self.flag = flag
        self.local_embedding = Conv(inp, oup, 1, act=False)
        self.global_embedding = Conv(global_inp[self.flag], oup, 1, act=False)
        self.global_act = Conv(global_inp[self.flag], oup, 1, act=False)
        self.act = h_sigmoid()

    def forward(self, x):
        '''
        x_g: global features
        x_l: local features
        '''
        x_l, x_g = x
        B, C, H, W = x_l.shape
        g_B, g_C, g_H, g_W = x_g.shape
        use_pool = H < g_H

        gloabl_info = x_g.split(self.global_inp, dim=1)[self.flag]

        local_feat = self.local_embedding(x_l)

        global_act = self.global_act(gloabl_info)
        global_feat = self.global_embedding(gloabl_info)

        if use_pool:
            avg_pool = get_avg_pool()
            output_size = np.array([H, W])

            sig_act = avg_pool(global_act, output_size)
            global_feat = avg_pool(global_feat, output_size)

        else:
            sig_act = F.interpolate(self.act(global_act), size=(H, W), mode='bilinear', align_corners=False)
            global_feat = F.interpolate(global_feat, size=(H, W), mode='bilinear', align_corners=False)

        out = local_feat * sig_act + global_feat
        return out


def get_shape(tensor):
    shape = tensor.shape
    if torch.onnx.is_in_onnx_export():
        shape = [i.cpu().numpy() for i in shape]
    return shape


class PyramidPoolAgg(nn.Module):
    def __init__(self, inc, ouc, stride, pool_mode='torch'):
        super().__init__()
        self.stride = stride
        if pool_mode == 'torch':
            self.pool = nn.functional.adaptive_avg_pool2d
        elif pool_mode == 'onnx':
            self.pool = onnx_AdaptiveAvgPool2d
        self.conv = Conv(inc, ouc)

    def forward(self, inputs):
        B, C, H, W = get_shape(inputs[-1])
        H = (H - 1) // self.stride + 1
        W = (W - 1) // self.stride + 1

        output_size = np.array([H, W])

        if not hasattr(self, 'pool'):
            self.pool = nn.functional.adaptive_avg_pool2d

        if torch.onnx.is_in_onnx_export():
            self.pool = onnx_AdaptiveAvgPool2d

        out = [self.pool(inp, output_size) for inp in inputs]

        return self.conv(torch.cat(out, dim=1))


def drop_path(x, drop_prob: float = 0., training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).
    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.
    """
    if drop_prob == 0. or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = Conv(in_features, hidden_features, act=False)
        self.dwconv = nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features)
        self.act = nn.ReLU6()
        self.fc2 = Conv(hidden_features, out_features, act=False)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.dwconv(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class GOLDYOLO_Attention(torch.nn.Module):
    def __init__(self, dim, key_dim, num_heads, attn_ratio=4):
        super().__init__()
        self.num_heads = num_heads
        self.scale = key_dim ** -0.5
        self.key_dim = key_dim
        self.nh_kd = nh_kd = key_dim * num_heads  # num_head key_dim
        self.d = int(attn_ratio * key_dim)
        self.dh = int(attn_ratio * key_dim) * num_heads
        self.attn_ratio = attn_ratio

        self.to_q = Conv(dim, nh_kd, 1, act=False)
        self.to_k = Conv(dim, nh_kd, 1, act=False)
        self.to_v = Conv(dim, self.dh, 1, act=False)

        self.proj = torch.nn.Sequential(nn.ReLU6(), Conv(self.dh, dim, act=False))

    def forward(self, x):  # x (B,N,C)
        B, C, H, W = get_shape(x)

        qq = self.to_q(x).reshape(B, self.num_heads, self.key_dim, H * W).permute(0, 1, 3, 2)
        kk = self.to_k(x).reshape(B, self.num_heads, self.key_dim, H * W)
        vv = self.to_v(x).reshape(B, self.num_heads, self.d, H * W).permute(0, 1, 3, 2)

        attn = torch.matmul(qq, kk)
        attn = attn.softmax(dim=-1)  # dim = k

        xx = torch.matmul(attn, vv)

        xx = xx.permute(0, 1, 3, 2).reshape(B, self.dh, H, W)
        xx = self.proj(xx)
        return xx


class top_Block(nn.Module):

    def __init__(self, dim, key_dim, num_heads, mlp_ratio=4., attn_ratio=2., drop=0.,
                 drop_path=0.):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.mlp_ratio = mlp_ratio

        self.attn = GOLDYOLO_Attention(dim, key_dim=key_dim, num_heads=num_heads, attn_ratio=attn_ratio)

        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)

    def forward(self, x1):
        x1 = x1 + self.drop_path(self.attn(x1))
        x1 = x1 + self.drop_path(self.mlp(x1))
        return x1


class TopBasicLayer(nn.Module):
    def __init__(self, embedding_dim, ouc_list, block_num=2, key_dim=8, num_heads=4,
                 mlp_ratio=4., attn_ratio=2., drop=0., attn_drop=0., drop_path=0.):
        super().__init__()
        self.block_num = block_num

        self.transformer_blocks = nn.ModuleList()
        for i in range(self.block_num):
            self.transformer_blocks.append(top_Block(
                embedding_dim, key_dim=key_dim, num_heads=num_heads,
                mlp_ratio=mlp_ratio, attn_ratio=attn_ratio,
                drop=drop, drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path))
        self.conv = nn.Conv2d(embedding_dim, sum(ouc_list), 1)

    def forward(self, x):
        # token * N
        for i in range(self.block_num):
            x = self.transformer_blocks[i](x)
        return self.conv(x)


class AdvPoolFusion(nn.Module):
    def forward(self, x):
        x1, x2 = x
        if torch.onnx.is_in_onnx_export():
            self.pool = onnx_AdaptiveAvgPool2d
        else:
            self.pool = nn.functional.adaptive_avg_pool2d

        N, C, H, W = x2.shape
        output_size = np.array([H, W])
        x1 = self.pool(x1, output_size)

        return torch.cat([x1, x2], 1)

######################################## GOLD-YOLO end ########################################


######################################## yolov6 EfficientRepBiPAN start ########################################
# å¯¹è¾“å…¥ä½¿ç”¨è½¬ç½®å·ç§¯ï¼Œä¹Ÿå°±æ˜¯ä¸Šé‡‡æ ·
class Transpose(nn.Module):
    '''Normal Transpose, default for upsampling'''

    # kernel_size=2, stride=2å‚æ•°ä¸‹ï¼Œb,c,h,w  ->  b,c,2h,2w
    def __init__(self, in_channels, out_channels, kernel_size=2, stride=2):
        super().__init__()
        self.upsample_transpose = torch.nn.ConvTranspose2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            stride=stride,
            bias=True
        )

    def forward(self, x):
        return self.upsample_transpose(x)

# å¯¹è¾“å…¥é€šé“æ•°ä¸ä¸€æ ·çš„xè¿›è¡Œä¸Šã€ä¸‹é‡‡æ ·æ“ä½œï¼Œä½¿å¾—channelä¸€æ ·
class BiFusion(nn.Module):
    '''BiFusion Block in PAN'''

    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.cv1 = Conv(in_channels[1], out_channels, 1, 1)
        self.cv2 = Conv(in_channels[2], out_channels, 1, 1)
        self.cv3 = Conv(out_channels * 3, out_channels, 1, 1)

        # ä¸Šé‡‡æ ·ï¼Œhå’Œwæ‰©å……ä¸€å€
        self.upsample = Transpose(
            in_channels=out_channels,
            out_channels=out_channels,
        )
        # ä¸‹é‡‡æ ·ï¼Œhå’Œwç¼©å‡ä¸€å€
        self.downsample = Conv(
            out_channels,
            out_channels,
            3,
            2
        )

    def forward(self, x):
        # æ„Ÿè§‰æ˜¯è¾“å…¥xæœ‰ä¸‰ä¸ªï¼Œæœ‰ä¸åŒçš„channelï¼Œéœ€è¦é€šè¿‡ä¸‹é¢çš„æ“ä½œå°†channelæ•´åˆ°å’Œx[1]ä¸€æ ·
        # x[0]æŒ‡ç¬¬ä¸€ä¸ªè¾“å…¥å¼ é‡
        x0 = self.upsample(x[0])
        x1 = self.cv1(x[1])
        x2 = self.downsample(self.cv2(x[2]))
        return self.cv3(torch.cat((x0, x1, x2), dim=1))

# ä½¿ç”¨RepVGGBlockè¿›è¡Œæ„å»ºbottleneckï¼Œå¹¶ä¸”åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æ®‹å·®ç»“æ„
class BottleRep(nn.Module):
    def __init__(self, in_channels, out_channels, basic_block=RepVGGBlock, weight=False):
        super().__init__()
        self.conv1 = basic_block(in_channels, out_channels)
        self.conv2 = basic_block(out_channels, out_channels)
        # å‡å¦‚è¾“å…¥é€šé“æ•°=è¾“å‡ºé€šé“æ•°ï¼Œè¿›è¡Œæ®‹å·®è¿æ¥
        if in_channels != out_channels:
            self.shortcut = False
        else:
            self.shortcut = True
        # å‡å¦‚weight=trueï¼Œåˆ™å®šä¹‰äº†ä¸€ä¸ªå¯å­¦ä¹ å‚æ•°alphaï¼Œtorch.ones(1)æŒ‡åˆå§‹åŒ–ä¸º1.
        if weight:
            self.alpha = nn.Parameter(torch.ones(1))
        else:
            self.alpha = 1.0

    def forward(self, x):
        outputs = self.conv1(x)
        outputs = self.conv2(outputs)
        # å‡å¦‚è¾“å…¥é€šé“æ•°=è¾“å‡ºé€šé“æ•°ï¼Œè¿›è¡Œæ®‹å·®è¿æ¥ï¼Œå…¶ä¸­xçš„æ”¯è·¯éœ€è¦æˆä¸€ä¸ªç³»æ•°
        return outputs + self.alpha * x if self.shortcut else outputs

# yolov6è®ºæ–‡é‡Œé¢æœ‰
class RepBlock(nn.Module):
    '''
        RepBlock is a stage block with rep-style basic block
    '''
    # å°±[-1, 12, RepBlock, [256]] # 12è€Œè¨€ï¼Œå½“æ¨¡å‹ä¸ºnæ—¶ï¼Œn=4  block=RepVGGBlock
    def __init__(self, in_channels, out_channels, n=1, block=BottleRep, basic_block=RepVGGBlock):
        super().__init__()

        self.conv1 = block(in_channels, out_channels)
        # å½“n>1ï¼Œåˆ™æ„å»ºnn.Sequentialï¼Œå¦åˆ™self.block = none
        # for _ in range(n - 1)ï¼šè¿™æ˜¯ä¸€ä¸ªè¿­ä»£ n æ¬¡çš„å¾ªç¯
        # å®ƒåˆ›å»º n ä¸ªRepVGGBlockï¼ˆç‹¬ç«‹çš„å‡½æ•°è¡¨ç¤ºçš„æ¶æ„ï¼‰ï¼Œå…¶ä¸­ out_channels ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºé€šé“ã€‚
        #  * è¿ç®—ç¬¦ç”¨äºå°†è¿™äº›å®ä¾‹è§£å‹ç¼©ä¸º nn.Sequential æ„é€ å‡½æ•°çš„å•ç‹¬å‚æ•°ï¼Œä»è€Œæœ‰æ•ˆåœ°åˆ›å»ºè¦æŒ‰é¡ºåºæ‰§è¡Œçš„å—åºåˆ—
        self.block = nn.Sequential(*(block(out_channels, out_channels) for _ in range(n - 1))) if n > 1 else None

        # if n > 1:
        #     print(21,n)
        #     self.block = nn.Sequential(*(block(out_channels, out_channels) for _ in range(n - 1)))
        #     print(22)
        # else:
        #     self.block = None
        #     print(23)

        # éœ€è¦å°†blockæ”¹ä¸ºBottleRepï¼Œæ‰ä¼šè¿›å…¥ä¸‹é¢çš„è¯­å¥
        if block == BottleRep:
            self.conv1 = BottleRep(in_channels, out_channels, basic_block=basic_block, weight=True)
            n = n // 2
            # ä¸‹é¢çš„è¯­å¥å°±å’Œä¸Šé¢çš„self.blockä¸€æ¨¡ä¸€æ ·
            self.block = nn.Sequential(
                *(BottleRep(out_channels, out_channels, basic_block=basic_block, weight=True) for _ in range(n - 1))) if n > 1 else None

    def forward(self, x):
        x = self.conv1(x)
        # æ ¹æ®blockçš„ç±»å‹é€‰æ‹©ä¸åŒçš„ç»“æ„
        if self.block is not None:
            x = self.block(x)
        return x

######################################## EfficientRepBiPAN end ########################################

######################################## C3 C2f OREPA start ########################################

class Bottleneck_OREPA(Bottleneck):
    """Standard bottleneck with OREPA."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        if k[0] == 1:
            self.cv1 = Conv(c1, c_)
        else:
            self.cv1 = OREPA(c1, c_, k[0])
        self.cv2 = OREPA(c_, c2, k[1], groups=g)

class C3_OREPA(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_OREPA(c_, c_, shortcut, g, k=(1, 3), e=1.0) for _ in range(n)))

class C2f_OREPA(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_OREPA(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## C3 C2f OREPA end ########################################

######################################## opera+E-ELAN start ########################################
# class Bottleneck_OREPA(Bottleneck):
#     """Standard bottleneck with OREPA."""
#
#     def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
#         super().__init__(c1, c2, shortcut, g, k, e)
#         c_ = int(c2 * e)  # hidden channels
#         if k[0] == 1:
#             self.cv1 = Conv(c1, c_)
#         else:
#             self.cv1 = OREPA(c1, c_, k[0])
#         self.cv2 = OREPA(c_, c2, k[1], groups=g)

class ELAN_OPERA(nn.Module):
    def __init__(self,  c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__()
        self.c = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, 2 * self.c, 1, 1)
        self.cv2 = Conv((2 + n) * self.c, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.ModuleList(Bottleneck(self.c, self.c, shortcut, g, k=((3, 3), (3, 3)), e=1.0) for _ in range(n))

    def forward(self, x):
        """Forward pass through C2f layer."""
        y = list(self.cv1(x).chunk(2, 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))

    def forward_split(self, x):
        """Forward pass using split() instead of chunk()."""
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in self.m)
        return self.cv2(torch.cat(y, 1))

######################################## opera+E-ELAN end ########################################


######################################## æµ‹è¯•c2fè¾“å‡º begin ########################################
class C2f_test(nn.Module):
    """Faster Implementation of CSP Bottleneck with 2 convolutions."""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__()
        self.c0 = int(c1 * e)
        self.c = int(c2 * e)
        self.cv1 = Conv(self.c0, 2 * self.c, 1, 1)
        self.cv2 = Conv(self.c0, 2 * self.c, 1, 1)
        self.cv3 = Conv((2 + n) * self.c * 2, c2, 1)
        self.m = nn.ModuleList(Bottleneck_OREPA(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

    def forward(self, x):
        # (b,c,h,w)=x.size()
        # print(1,b,c,h,w)

        x1, x2 = x.split((self.c0, self.c0), 1)

        y1 = list(self.cv1(x1).chunk(2, 1))
        y1.extend(m(y1[-1]) for m in self.m)
        z1 = torch.cat(y1, 1)

        y2 = list(self.cv2(x2).chunk(2, 1))
        y2.extend(m(y2[-1]) for m in self.m)
        z2 = torch.cat(y2, 1)

        output = self.cv3(torch.cat((z1, z2), dim = 1))

        return output

    def forward_split(self, x):
        # (b1,c1,h1,w1)=x.size()
        # print(2,b1,c1,h1,w1)
        # y = list(self.cv1(x).split((self.c, self.c), 1))
        # y.extend(m(y[-1]) for m in self.m)

        x1, x2 = x.split((self.c0, self.c0), 1)

        y1 = list(self.cv1(x1).split((self.c0, self.c0), 1))
        y1.extend(m(y1[-1]) for m in self.m)
        z1 = torch.cat(y1, 1)

        y2 = list(self.cv2(x2).split((self.c0, self.c0), 1))
        y2.extend(m(y2[-1]) for m in self.m)
        z2 = torch.cat(y2, 1)

        output = self.cv3(torch.cat((z1, z2), dim=1))

        return output

######################################## æµ‹è¯•c2fè¾“å‡º end ########################################


######################################## YOLOV9 end ########################################
# RepConvNæ˜¯å†²å‚æ•°æ¨¡å—repä¸­ä¸€ä¸ªå¾ˆåŸºç¡€çš„æ¨¡å—ï¼Œæ‰€ä»¥å¯ä»¥ç”¨åˆ«çš„é‡å‚æ•°æ¨¡å—å»æ›¿æ¢ï¼Œä¸‹é¢æœ‰ä¾‹å­
class RepConvN(nn.Module):
    """RepConv is a basic rep-style block, including training and deploy status
    This code is based on https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py
    """
    default_act = nn.SiLU()  # default activation

    def __init__(self, c1, c2, k=3, s=1, p=1, g=1, d=1, act=True, bn=False, deploy=False):
        super().__init__()
        assert k == 3 and p == 1
        self.g = g
        self.c1 = c1
        self.c2 = c2
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

        self.bn = None
        self.conv1 = Conv(c1, c2, k, s, p=p, g=g, act=False)
        self.conv2 = Conv(c1, c2, 1, s, p=(p - k // 2), g=g, act=False)

    def forward_fuse(self, x):
        """Forward process"""
        return self.act(self.conv(x))

    def forward(self, x):
        """Forward process"""
        id_out = 0 if self.bn is None else self.bn(x)
        return self.act(self.conv1(x) + self.conv2(x) + id_out)

    def get_equivalent_kernel_bias(self):
        kernel3x3, bias3x3 = self._fuse_bn_tensor(self.conv1)
        kernel1x1, bias1x1 = self._fuse_bn_tensor(self.conv2)
        kernelid, biasid = self._fuse_bn_tensor(self.bn)
        return kernel3x3 + self._pad_1x1_to_3x3_tensor(kernel1x1) + kernelid, bias3x3 + bias1x1 + biasid

    def _avg_to_3x3_tensor(self, avgp):
        channels = self.c1
        groups = self.g
        kernel_size = avgp.kernel_size
        input_dim = channels // groups
        k = torch.zeros((channels, input_dim, kernel_size, kernel_size))
        k[np.arange(channels), np.tile(np.arange(input_dim), groups), :, :] = 1.0 / kernel_size ** 2
        return k

    def _pad_1x1_to_3x3_tensor(self, kernel1x1):
        if kernel1x1 is None:
            return 0
        else:
            return torch.nn.functional.pad(kernel1x1, [1, 1, 1, 1])

    def _fuse_bn_tensor(self, branch):
        if branch is None:
            return 0, 0
        if isinstance(branch, Conv):
            kernel = branch.conv.weight
            running_mean = branch.bn.running_mean
            running_var = branch.bn.running_var
            gamma = branch.bn.weight
            beta = branch.bn.bias
            eps = branch.bn.eps
        elif isinstance(branch, nn.BatchNorm2d):
            if not hasattr(self, 'id_tensor'):
                input_dim = self.c1 // self.g
                kernel_value = np.zeros((self.c1, input_dim, 3, 3), dtype=np.float32)
                for i in range(self.c1):
                    kernel_value[i, i % input_dim, 1, 1] = 1
                self.id_tensor = torch.from_numpy(kernel_value).to(branch.weight.device)
            kernel = self.id_tensor
            running_mean = branch.running_mean
            running_var = branch.running_var
            gamma = branch.weight
            beta = branch.bias
            eps = branch.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std

    def fuse_convs(self):
        if hasattr(self, 'conv'):
            return
        kernel, bias = self.get_equivalent_kernel_bias()
        self.conv = nn.Conv2d(in_channels=self.conv1.conv.in_channels,
                              out_channels=self.conv1.conv.out_channels,
                              kernel_size=self.conv1.conv.kernel_size,
                              stride=self.conv1.conv.stride,
                              padding=self.conv1.conv.padding,
                              dilation=self.conv1.conv.dilation,
                              groups=self.conv1.conv.groups,
                              bias=True).requires_grad_(False)
        self.conv.weight.data = kernel
        self.conv.bias.data = bias
        for para in self.parameters():
            para.detach_()
        self.__delattr__('conv1')
        self.__delattr__('conv2')
        if hasattr(self, 'nm'):
            self.__delattr__('nm')
        if hasattr(self, 'bn'):
            self.__delattr__('bn')
        if hasattr(self, 'id_tensor'):
            self.__delattr__('id_tensor')

class RepNBottleneck(nn.Module):
    # Standard bottleneck
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, kernels, groups, expand
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = RepConvN(c1, c_, k[0], 1)
        self.cv2 = Conv(c_, c2, k[1], 1, g=g)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))

class DBBNBottleneck(RepNBottleneck):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = DiverseBranchBlock(c1, c_, k[0], 1)

class OREPANBottleneck(RepNBottleneck):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = OREPA(c1, c_, k[0], 1)

# DilatedReparamBlock å¯é‡å‚æ•°åŒ–çš„å¤§æ ¸æ¨¡å—
class DRBNBottleneck(RepNBottleneck):
    def __init__(self, c1, c2, kernel_size, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = DilatedReparamBlock(c1, kernel_size)

class RepNCSP(nn.Module):
    # CSP Bottleneck with 3 convolutions
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv(c1, c_, 1, 1)
        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)
        self.m = nn.Sequential(*(RepNBottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

    def forward(self, x):
        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))

class DBBNCSP(RepNCSP):
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(DBBNBottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

class OREPANCSP(RepNCSP):
    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(OREPANBottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))

class DRBNCSP(RepNCSP):
    def __init__(self, c1, c2, n=1, kernel_size=7, shortcut=True, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(DRBNBottleneck(c_, c_, kernel_size, shortcut, g, e=1.0) for _ in range(n)))

class RepNCSPELAN4(nn.Module):
    # csp-elan  384 256 128 64 1
    def __init__(self, c1, c2, c3, c4, c5=1):  # ch_in, ch_out, number, shortcut, groups, c5æ˜¯æŒ‡RepNCSPçš„é‡å¤æ¬¡æ•°
        super().__init__()
        self.c = c3//2
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv2 = nn.Sequential(RepNCSP(c3//2, c4, c5), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1))
        self.cv4 = Conv(c3+(2*c4), c2, 1, 1)

    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        # y[-1]å…ˆç»è¿‡self.cv2ï¼ˆç®—æ˜¯ä¸€ä¸ªé›†æˆçš„æ¨¡å—ï¼Œæ‰€ä»¥ç»è¿‡RepNCSPä¸ä¼šè¿æ¥åˆ°contactï¼Œåªæœ‰ç»è¿‡Convæ‰ä¼šcontactï¼‰ï¼Œå†ç»è¿‡self.cv3
        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))

    def forward_split(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))

class Bottleneck_RepNCSP(nn.Module):
    """Standard bottleneck."""
    def __init__(self,c3,c4,c5, shortcut=True):  # è¾“å…¥é€šé“æ•°ï¼Œè¾“å‡ºé€šé“æ•°ï¼Œæ˜¯å¦æ®‹å·®è¿æ¥ï¼Œç»„æ•°ï¼Œå·ç§¯æ ¸çš„å¤§å°ï¼Œç¼©æ”¾å€ç‡e
        super().__init__()
        #c_ = int(c2 * e)  # hidden channels æŒ‰ç…§e=0.5ï¼Œåˆ™c_çš„é€šé“æ•°åº”è¯¥æ˜¯c2çš„ä¸€åŠ
        self.c6 = c3//2
        self.cv1 = RepNCSP(self.c6, c4, c5)  # è¾“å…¥é€šé“: c1, è¾“å‡ºé€šé“ï¼šc_ , å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        self.cv2 = Conv(c4, c4, 3, 1)  # è¾“å…¥é€šé“ï¼šc_ , è¾“å‡ºé€šé“c2, å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        #self.cv1 = Conv(c1, c_, k[0], 1)  # è¾“å…¥é€šé“: c1, è¾“å‡ºé€šé“ï¼šc_ , å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        #self.cv2 = RFCAConv(c_, c2)  # è¾“å…¥é€šé“ï¼šc_ , è¾“å‡ºé€šé“c2, å·ç§¯æ ¸ï¼š3x3, æ­¥é•¿1
        self.add = shortcut and self.c6 == c4   # shortcut and c1 == c2 è¡¨ç¤ºå¦‚æœåŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ï¼Œself.add çš„å€¼ä¸º Trueï¼ŒåŒæ—¶ä½¿ç”¨æ®‹å·®è¿æ¥

    def forward(self, x):
        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))

#  RepNCSPå’Œconvç»„æˆæ®‹å·®ç»“æ„çš„ ResNet_RepNCSPELAN4
class ResNet_RepNCSPELAN4(nn.Module):
    # csp-elan  384 256 128 64 1
    def __init__(self, c1, c2, c3, c4, c5=1):  # ch_in, ch_out, number, shortcut, groups, c5æ˜¯æŒ‡RepNCSPçš„é‡å¤æ¬¡æ•°
        super().__init__()
        self.c = c3//2
        self.cv1 = Conv(c1, c3, 1, 1)
        self.cv2 = Bottleneck_RepNCSP(c3,c4,c5)
        self.cv3 = Bottleneck_RepNCSP(c3,c4,c5)
        # self.cv2 = nn.Sequential(RepNCSP(c3//2, c4, c5), Conv(c4, c4, 3, 1))
        # self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1))
        self.cv4 = Conv(c3+(2*c4), c2, 1, 1)

    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        # y[-1]å…ˆç»è¿‡self.cv2ï¼ˆç®—æ˜¯ä¸€ä¸ªé›†æˆçš„æ¨¡å—ï¼Œæ‰€ä»¥ç»è¿‡RepNCSPä¸ä¼šè¿æ¥åˆ°contactï¼Œåªæœ‰ç»è¿‡Convæ‰ä¼šcontactï¼‰ï¼Œå†ç»è¿‡self.cv3
        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))

    def forward_split(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))




class DBBNCSPELAN4(RepNCSPELAN4):
    def __init__(self, c1, c2, c3, c4, c5=1):
        super().__init__(c1, c2, c3, c4, c5)
        self.cv2 = nn.Sequential(DBBNCSP(c3//2, c4, c5), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(DBBNCSP(c4, c4, c5), Conv(c4, c4, 3, 1))

class OREPANCSPELAN4(RepNCSPELAN4):
    def __init__(self, c1, c2, c3, c4, c5=1):
        super().__init__(c1, c2, c3, c4, c5)
        self.cv2 = nn.Sequential(OREPANCSP(c3//2, c4, c5), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(OREPANCSP(c4, c4, c5), Conv(c4, c4, 3, 1))

# ä½¿ç”¨éœ€è¦åœ¨yamlæ–‡ä»¶ä¸­åŠ å…¥å·ç§¯æ ¸çš„å¤§å°
class DRBNCSPELAN4(RepNCSPELAN4):
    def __init__(self, c1, c2, c3, c4, c5=1, c6=7):
        super().__init__(c1, c2, c3, c4, c5)
        self.cv2 = nn.Sequential(DRBNCSP(c3//2, c4, c5, c6), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(DRBNCSP(c4, c4, c5, c6), Conv(c4, c4, 3, 1))


class ADown(nn.Module):
    def __init__(self, c1, c2):  # ch_in, ch_out, shortcut, kernels, groups, expand
        super().__init__()
        self.c = c2 // 2
        # 1æ˜¯æŒ‡å¡«å……ä¸º1ï¼Œç»è¿‡è¿™ä¸ªå·ç§¯æ˜¯ä¸‹é‡‡æ ·å·ç§¯
        self.cv1 = Conv(c1 // 2, self.c, 3, 2, 1)
        self.cv2 = Conv(c1 // 2, self.c, 1, 1, 0)

    # ä»¥160*160çš„ç‰¹å¾å±‚ä¸‹é‡‡æ ·å˜æˆ80*80ä¸ºä¾‹å­
    def forward(self, x):
        # 2 32 160 160
        # (b,c,h,w)=x.size()
        # print(1,b,c,h,w)
        # å¯¹è¾“å…¥å¼ é‡ x è¿›è¡Œå¤§å°ä¸º 2x2 çš„å¹³å‡æ± åŒ–æ“ä½œï¼Œæ­¥å¹…ä¸º 1ï¼Œå¡«å……ä¸º 0
        #False: ceil_modeï¼Œå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦ä½¿ç”¨å‘ä¸Šå–æ•´çš„æ–¹å¼è®¡ç®—è¾“å‡ºå½¢çŠ¶ã€‚è¿™é‡Œä¸º Falseï¼Œè¡¨ç¤ºä½¿ç”¨å‘ä¸‹å–æ•´ã€‚
        #True: count_include_padï¼Œå¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦åŒ…å«å¡«å……å€¼åœ¨å†…è¿›è¡Œè®¡ç®—ã€‚è¿™é‡Œä¸º Trueï¼Œè¡¨ç¤ºåŒ…å«å¡«å……å€¼åœ¨å†…
        # 2 32 159 159
        x = torch.nn.functional.avg_pool2d(x, 2, 1, 0, False, True)
        # (b1,c1,h1,w1)=x.size()
        # print(2,b1,c1,h1,w1)
        # x1å’Œx2æ˜¯è¾“å…¥å¼ é‡xåœ¨ç»´åº¦1ä¸Šå¹³å‡åˆ†å‰²åå¾—åˆ°çš„ä¸¤ä¸ªå¼ é‡
        # x1ï¼š2 16 159 159
        x1,x2 = x.chunk(2, 1)
        # (b1,c1,h1,w1)=x1.size()
        # print(3,b1,c1,h1,w1)
        # å·ç§¯åï¼š2 32 80 80
        x1 = self.cv1(x1)
        # (b1,c1,h1,w1)=x1.size()
        # print(4,b1,c1,h1,w1)
        # å¯¹è¾“å…¥å¼ é‡ x2 è¿›è¡Œå¤§å°ä¸º 3x3 çš„æœ€å¤§æ± åŒ–æ“ä½œï¼Œæ­¥å¹…ä¸º 2ï¼Œå¡«å……ä¸º 1
        # æ± åŒ–åï¼š2 16 80 80   å·ç§¯åï¼š2 32 80 80
        x2 = torch.nn.functional.max_pool2d(x2, 3, 2, 1)
        # (b1,c1,h1,w1)=x2.size()
        # print(5,b1,c1,h1,w1)
        x2 = self.cv2(x2)
        # (b1,c1,h1,w1)=x2.size()
        # print(6,b1,c1,h1,w1)
        return torch.cat((x1, x2), 1)

class CBLinear(nn.Module):
    def __init__(self, c1, c2s, k=1, s=1, p=None, g=1):  # ch_in, ch_outs, kernel, stride, padding, groups
        super(CBLinear, self).__init__()
        self.c2s = c2s
        self.conv = nn.Conv2d(c1, sum(c2s), k, s, autopad(k, p), groups=g, bias=True)

    def forward(self, x):
        outs = self.conv(x).split(self.c2s, dim=1)
        return outs

class CBFuse(nn.Module):
    def __init__(self, idx):
        super(CBFuse, self).__init__()
        self.idx = idx

    def forward(self, xs):
        target_size = xs[-1].shape[2:]
        res = [F.interpolate(x[self.idx[i]], size=target_size, mode='nearest') for i, x in enumerate(xs[:-1])]
        out = torch.sum(torch.stack(res + xs[-1:]), dim=0)
        return out


class Silence(nn.Module):
    def __init__(self, channels):
        super(Silence, self).__init__()
        self.a11=channels

    def forward(self, x):
        return x


######################################## YOLOV9 end ########################################


######################################## UniRepLKNetBlock, DilatedReparamBlock start ########################################
import torch.utils.checkpoint as checkpoint

from ultralytics.nn.backbone.UniRepLKNet import get_bn, get_conv2d, NCHWtoNHWC, GRNwithNHWC, SEBlock, NHWCtoNCHW, fuse_bn, merge_dilated_into_large_kernel
class DilatedReparamBlock(nn.Module):
    """
    Dilated Reparam Block proposed in UniRepLKNet (https://github.com/AILab-CVC/UniRepLKNet)
    We assume the inputs to this block are (N, C, H, W)
    """
    def __init__(self, channels, kernel_size, deploy=False, use_sync_bn=False, attempt_use_lk_impl=True):
        super().__init__()
        self.lk_origin = get_conv2d(channels, channels, kernel_size, stride=1,
                                    padding=kernel_size//2, dilation=1, groups=channels, bias=deploy,
                                    attempt_use_lk_impl=attempt_use_lk_impl)
        self.attempt_use_lk_impl = attempt_use_lk_impl

        #   Default settings. We did not tune them carefully. Different settings may work better.
        if kernel_size == 17:
            self.kernel_sizes = [5, 9, 3, 3, 3]
            self.dilates = [1, 2, 4, 5, 7]
        elif kernel_size == 15:
            self.kernel_sizes = [5, 7, 3, 3, 3]
            self.dilates = [1, 2, 3, 5, 7]
        elif kernel_size == 13:
            self.kernel_sizes = [5, 7, 3, 3, 3]
            self.dilates = [1, 2, 3, 4, 5]
        elif kernel_size == 11:
            self.kernel_sizes = [5, 5, 3, 3, 3]
            self.dilates = [1, 2, 3, 4, 5]
        elif kernel_size == 9:
            self.kernel_sizes = [5, 5, 3, 3]
            self.dilates = [1, 2, 3, 4]
        elif kernel_size == 7:
            self.kernel_sizes = [5, 3, 3]
            self.dilates = [1, 2, 3]
        elif kernel_size == 5:
            self.kernel_sizes = [3, 3]
            self.dilates = [1, 2]
        else:
            raise ValueError('Dilated Reparam Block requires kernel_size >= 5')

        if not deploy:
            self.origin_bn = get_bn(channels, use_sync_bn)
            for k, r in zip(self.kernel_sizes, self.dilates):
                self.__setattr__('dil_conv_k{}_{}'.format(k, r),
                                 nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=k, stride=1,
                                           padding=(r * (k - 1) + 1) // 2, dilation=r, groups=channels,
                                           bias=False))
                self.__setattr__('dil_bn_k{}_{}'.format(k, r), get_bn(channels, use_sync_bn=use_sync_bn))

    def forward(self, x):
        if not hasattr(self, 'origin_bn'):      # deploy mode
            return self.lk_origin(x)
        out = self.origin_bn(self.lk_origin(x))
        for k, r in zip(self.kernel_sizes, self.dilates):
            conv = self.__getattr__('dil_conv_k{}_{}'.format(k, r))
            bn = self.__getattr__('dil_bn_k{}_{}'.format(k, r))
            out = out + bn(conv(x))
        return out

    def switch_to_deploy(self):
        if hasattr(self, 'origin_bn'):
            origin_k, origin_b = fuse_bn(self.lk_origin, self.origin_bn)
            for k, r in zip(self.kernel_sizes, self.dilates):
                conv = self.__getattr__('dil_conv_k{}_{}'.format(k, r))
                bn = self.__getattr__('dil_bn_k{}_{}'.format(k, r))
                branch_k, branch_b = fuse_bn(conv, bn)
                origin_k = merge_dilated_into_large_kernel(origin_k, branch_k, r)
                origin_b += branch_b
            merged_conv = get_conv2d(origin_k.size(0), origin_k.size(0), origin_k.size(2), stride=1,
                                    padding=origin_k.size(2)//2, dilation=1, groups=origin_k.size(0), bias=True,
                                    attempt_use_lk_impl=self.attempt_use_lk_impl)
            merged_conv.weight.data = origin_k
            merged_conv.bias.data = origin_b
            self.lk_origin = merged_conv
            self.__delattr__('origin_bn')
            for k, r in zip(self.kernel_sizes, self.dilates):
                self.__delattr__('dil_conv_k{}_{}'.format(k, r))
                self.__delattr__('dil_bn_k{}_{}'.format(k, r))


class UniRepLKNetBlock(nn.Module):
    def __init__(self,
                 dim,
                 kernel_size,
                 drop_path=0.,
                 layer_scale_init_value=1e-6,
                 deploy=False,
                 attempt_use_lk_impl=True,
                 with_cp=False,
                 use_sync_bn=False,
                 ffn_factor=4):
        super().__init__()
        self.with_cp = with_cp
        # if deploy:
        #     print('------------------------------- Note: deploy mode')
        # if self.with_cp:
        #     print('****** note with_cp = True, reduce memory consumption but may slow down training ******')

        self.need_contiguous = (not deploy) or kernel_size >= 7

        if kernel_size == 0:
            self.dwconv = nn.Identity()
            self.norm = nn.Identity()
        elif deploy:
            self.dwconv = get_conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=kernel_size // 2,
                                     dilation=1, groups=dim, bias=True,
                                     attempt_use_lk_impl=attempt_use_lk_impl)
            self.norm = nn.Identity()
        elif kernel_size >= 7:
            self.dwconv = DilatedReparamBlock(dim, kernel_size, deploy=deploy,
                                              use_sync_bn=use_sync_bn,
                                              attempt_use_lk_impl=attempt_use_lk_impl)
            self.norm = get_bn(dim, use_sync_bn=use_sync_bn)
        elif kernel_size == 1:
            self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=kernel_size // 2,
                                    dilation=1, groups=1, bias=deploy)
            self.norm = get_bn(dim, use_sync_bn=use_sync_bn)
        else:
            assert kernel_size in [3, 5]
            self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size, stride=1, padding=kernel_size // 2,
                                    dilation=1, groups=dim, bias=deploy)
            self.norm = get_bn(dim, use_sync_bn=use_sync_bn)

        self.se = SEBlock(dim, dim // 4)

        ffn_dim = int(ffn_factor * dim)
        self.pwconv1 = nn.Sequential(
            NCHWtoNHWC(),
            nn.Linear(dim, ffn_dim))
        self.act = nn.Sequential(
            nn.GELU(),
            GRNwithNHWC(ffn_dim, use_bias=not deploy))
        if deploy:
            self.pwconv2 = nn.Sequential(
                nn.Linear(ffn_dim, dim),
                NHWCtoNCHW())
        else:
            self.pwconv2 = nn.Sequential(
                nn.Linear(ffn_dim, dim, bias=False),
                NHWCtoNCHW(),
                get_bn(dim, use_sync_bn=use_sync_bn))

        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones(dim),
                                  requires_grad=True) if (not deploy) and layer_scale_init_value is not None \
                                                         and layer_scale_init_value > 0 else None
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()

    def forward(self, inputs):

        def _f(x):
            if self.need_contiguous:
                x = x.contiguous()
            y = self.se(self.norm(self.dwconv(x)))
            y = self.pwconv2(self.act(self.pwconv1(y)))
            if self.gamma is not None:
                y = self.gamma.view(1, -1, 1, 1) * y
            return self.drop_path(y) + x

        if self.with_cp and inputs.requires_grad:
            return checkpoint.checkpoint(_f, inputs)
        else:
            return _f(inputs)

    def switch_to_deploy(self):
        if hasattr(self.dwconv, 'switch_to_deploy'):
            self.dwconv.switch_to_deploy()
        if hasattr(self.norm, 'running_var') and hasattr(self.dwconv, 'lk_origin'):
            std = (self.norm.running_var + self.norm.eps).sqrt()
            self.dwconv.lk_origin.weight.data *= (self.norm.weight / std).view(-1, 1, 1, 1)
            self.dwconv.lk_origin.bias.data = self.norm.bias + (self.dwconv.lk_origin.bias - self.norm.running_mean) * self.norm.weight / std
            self.norm = nn.Identity()
        if self.gamma is not None:
            final_scale = self.gamma.data
            self.gamma = None
        else:
            final_scale = 1
        if self.act[1].use_bias and len(self.pwconv2) == 3:
            grn_bias = self.act[1].beta.data
            self.act[1].__delattr__('beta')
            self.act[1].use_bias = False
            linear = self.pwconv2[0]
            grn_bias_projected_bias = (linear.weight.data @ grn_bias.view(-1, 1)).squeeze()
            bn = self.pwconv2[2]
            std = (bn.running_var + bn.eps).sqrt()
            new_linear = nn.Linear(linear.in_features, linear.out_features, bias=True)
            new_linear.weight.data = linear.weight * (bn.weight / std * final_scale).view(-1, 1)
            linear_bias = 0 if linear.bias is None else linear.bias.data
            linear_bias += grn_bias_projected_bias
            new_linear.bias.data = (bn.bias + (linear_bias - bn.running_mean) * bn.weight / std) * final_scale
            self.pwconv2 = nn.Sequential(new_linear, self.pwconv2[1])

class C3_UniRepLKNetBlock(C3):
    def __init__(self, c1, c2, n=1, k=7, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(UniRepLKNetBlock(c_, k) for _ in range(n)))

class C2f_UniRepLKNetBlock(C2f):
    def __init__(self, c1, c2, n=1, k=7, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(UniRepLKNetBlock(self.c, k) for _ in range(n))

class Bottleneck_DRB(Bottleneck):
    """Standard bottleneck with DilatedReparamBlock."""

    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):  # ch_in, ch_out, shortcut, groups, kernels, expand
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv2 = DilatedReparamBlock(c2, 7)

class C3_DRB(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_DRB(c_, c_, shortcut, g, k=(1, 3), e=1.0) for _ in range(n)))

class C2f_DRB(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_DRB(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## UniRepLKNetBlock, DilatedReparamBlock end ########################################

######################################## DySample start ########################################

class DySample(nn.Module):
    def __init__(self, in_channels, scale=2, style='lp', groups=4, dyscope=False):
        super().__init__()
        self.scale = scale
        self.style = style
        self.groups = groups
        assert style in ['lp', 'pl']
        if style == 'pl':
            assert in_channels >= scale ** 2 and in_channels % scale ** 2 == 0
        assert in_channels >= groups and in_channels % groups == 0

        if style == 'pl':
            in_channels = in_channels // scale ** 2
            out_channels = 2 * groups
        else:
            out_channels = 2 * groups * scale ** 2

        self.offset = nn.Conv2d(in_channels, out_channels, 1)
        self.normal_init(self.offset, std=0.001)
        if dyscope:
            self.scope = nn.Conv2d(in_channels, out_channels, 1)
            self.constant_init(self.scope, val=0.)

        self.register_buffer('init_pos', self._init_pos())

    def normal_init(self, module, mean=0, std=1, bias=0):
        if hasattr(module, 'weight') and module.weight is not None:
            nn.init.normal_(module.weight, mean, std)
        if hasattr(module, 'bias') and module.bias is not None:
            nn.init.constant_(module.bias, bias)

    def constant_init(self, module, val, bias=0):
        if hasattr(module, 'weight') and module.weight is not None:
            nn.init.constant_(module.weight, val)
        if hasattr(module, 'bias') and module.bias is not None:
            nn.init.constant_(module.bias, bias)

    def _init_pos(self):
        h = torch.arange((-self.scale + 1) / 2, (self.scale - 1) / 2 + 1) / self.scale
        return torch.stack(torch.meshgrid([h, h])).transpose(1, 2).repeat(1, self.groups, 1).reshape(1, -1, 1, 1)

    def sample(self, x, offset):
        B, _, H, W = offset.shape
        offset = offset.view(B, 2, -1, H, W)
        coords_h = torch.arange(H) + 0.5
        coords_w = torch.arange(W) + 0.5
        coords = torch.stack(torch.meshgrid([coords_w, coords_h])
                             ).transpose(1, 2).unsqueeze(1).unsqueeze(0).type(x.dtype).to(x.device)
        normalizer = torch.tensor([W, H], dtype=x.dtype, device=x.device).view(1, 2, 1, 1, 1)
        coords = 2 * (coords + offset) / normalizer - 1
        coords = F.pixel_shuffle(coords.view(B, -1, H, W), self.scale).view(
            B, 2, -1, self.scale * H, self.scale * W).permute(0, 2, 3, 4, 1).contiguous().flatten(0, 1)
        return F.grid_sample(x.reshape(B * self.groups, -1, H, W), coords, mode='bilinear',
                             align_corners=False, padding_mode="border").reshape((B, -1, self.scale * H, self.scale * W))

    def forward_lp(self, x):
        if hasattr(self, 'scope'):
            offset = self.offset(x) * self.scope(x).sigmoid() * 0.5 + self.init_pos
        else:
            offset = self.offset(x) * 0.25 + self.init_pos
        return self.sample(x, offset)

    def forward_pl(self, x):
        x_ = F.pixel_shuffle(x, self.scale)
        if hasattr(self, 'scope'):
            offset = F.pixel_unshuffle(self.offset(x_) * self.scope(x_).sigmoid(), self.scale) * 0.5 + self.init_pos
        else:
            offset = F.pixel_unshuffle(self.offset(x_), self.scale) * 0.25 + self.init_pos
        return self.sample(x, offset)

    def forward(self, x):
        if self.style == 'pl':
            return self.forward_pl(x)
        return self.forward_lp(x)

######################################## DySample end ########################################

######################################## Attentional Scale Sequence Fusion start ########################################
# ASFçš„ç»“æ„æœ¬æ¥æ—¶ç”¨ScalSeqä½œä¸ºä¸Šé‡‡æ ·çš„ï¼Œä½†æ˜¯yamlç”¨çš„æ˜¯Dysampleå’ŒScalSeqç»“åˆçš„DynamicScalSeq

# TFEæ¨¡å—
# è¾“å…¥ä¸ºP3,P4,P5,Hå’ŒWçš„ç»´åº¦ä¸åŒï¼Œå°†å…¶æ•´ç†æˆP4çš„ç»´åº¦ï¼Œå¹¶æœ€åè¿›è¡Œcontactï¼Œå³ï¼ˆb,c,h,wï¼‰-> ï¼ˆb,3c,h,wï¼‰
class Zoom_cat(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        l, m, s = x[0], x[1], x[2]
        tgt_size = m.shape[2:]
        # è¡¨ç¤ºå¯¹å¼ é‡ l è¿›è¡Œè‡ªé€‚åº”æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–æ“ä½œï¼Œå°†å…¶å°ºå¯¸è°ƒæ•´ä¸ºç›®æ ‡å¤§å° tgt_size
        l = F.adaptive_max_pool2d(l, tgt_size) + F.adaptive_avg_pool2d(l, tgt_size)
        # æœ€è¿‘é‚»æ’å€¼
        s = F.interpolate(s, m.shape[2:], mode='nearest')
        lms = torch.cat([l, m, s], dim=1)
        return lms

# ä¸ä¸‹é¢DynamicScalSeqçš„åŒºåˆ«æ˜¯ï¼ŒScalSeqä½¿ç”¨çš„æ˜¯æœ€è¿‘é‚»æ’å€¼DynamicScalSeqä½¿ç”¨çš„æ˜¯iccv2023çš„dysample
class ScalSeq(nn.Module):
    def __init__(self, inc, channel):
        super(ScalSeq, self).__init__()
        if channel != inc[0]:
            self.conv0 = Conv(inc[0], channel, 1)
        self.conv1 = Conv(inc[1], channel, 1)
        self.conv2 = Conv(inc[2], channel, 1)
        self.conv3d = nn.Conv3d(channel, channel, kernel_size=(1, 1, 1))
        self.bn = nn.BatchNorm3d(channel)
        self.act = nn.LeakyReLU(0.1)
        self.pool_3d = nn.MaxPool3d(kernel_size=(3, 1, 1))

    def forward(self, x):
        # æœ€åå¾—åˆ°çš„å®½åº¦å’Œé«˜åº¦å’Œp3çš„ä¸€æ ·
        p3, p4, p5 = x[0], x[1], x[2]
        if hasattr(self, 'conv0'):
            p3 = self.conv0(p3)
        p4_2 = self.conv1(p4)
        # ä¸‹é¢ä¸€è¡Œæ˜¯ç¬¬å››å±‚åˆ°ç¬¬ä¸‰å±‚ä¸Šé‡‡æ ·ï¼Œæ‰€ä»¥ä¸‹é¢çš„DynamicScalSeqä¸­çš„DySampleè®¾ç½®ä¸º2
        p4_2 = F.interpolate(p4_2, p3.size()[2:], mode='nearest')
        p5_2 = self.conv2(p5)
        # ä¸‹é¢ä¸€è¡Œæ˜¯ç¬¬äº”å±‚åˆ°ç¬¬ä¸‰å±‚ä¸Šé‡‡æ ·ï¼Œæ‰€ä»¥ä¸‹é¢çš„DynamicScalSeqä¸­çš„DySampleè®¾ç½®ä¸º4
        p5_2 = F.interpolate(p5_2, p3.size()[2:], mode='nearest')
        p3_3d = torch.unsqueeze(p3, -3)
        p4_3d = torch.unsqueeze(p4_2, -3)
        p5_3d = torch.unsqueeze(p5_2, -3)
        combine = torch.cat([p3_3d, p4_3d, p5_3d], dim=2)
        conv_3d = self.conv3d(combine)
        bn = self.bn(conv_3d)
        act = self.act(bn)
        x = self.pool_3d(act)
        x = torch.squeeze(x, 2)
        return x

# yolov8-ASF-DySample.yaml
class DynamicScalSeq(nn.Module):
    def __init__(self, inc, channel):
        super(DynamicScalSeq, self).__init__()
        if channel != inc[0]:
            self.conv0 = Conv(inc[0], channel, 1)
        self.conv1 = Conv(inc[1], channel, 1)
        self.conv2 = Conv(inc[2], channel, 1)
        # 3då·ç§¯ï¼Œè¾“å…¥å’Œè¾“å‡ºç›¸åŒ
        self.conv3d = nn.Conv3d(channel, channel, kernel_size=(1, 1, 1))
        self.bn = nn.BatchNorm3d(channel)
        self.act = nn.LeakyReLU(0.1)
        self.pool_3d = nn.MaxPool3d(kernel_size=(3, 1, 1))

        # DySampleæ˜¯ICCV2023çš„ä¸Šé‡‡æ ·çš„æ–¹æ³•
        self.dysample1 = DySample(channel, 2, 'lp')
        self.dysample2 = DySample(channel, 4, 'lp')

    def forward(self, x):
        # çœ‹ yolov8-ASF-DySample.yaml é‡Œé¢ DynamicScalSeq çš„ä½¿ç”¨ï¼Œå‘ç°è¾“å…¥æ¥è‡ªä¸‰ä¸ªç»´åº¦çš„é€šé“
        p3, p4, p5 = x[0], x[1], x[2]
        # æ£€æŸ¥å½“å‰å¯¹è±¡æ˜¯å¦åŒ…å«conv0å±æ€§ï¼Œå¦‚æœåŒ…å«ï¼Œåˆ™å°†å¼ é‡p3è¾“å…¥åˆ°conv0ä¸­è¿›è¡Œå·ç§¯æ“ä½œï¼Œç»´åº¦å˜åŒ–
        if hasattr(self, 'conv0'):
            p3 = self.conv0(p3)
        # ç»´åº¦å˜åŒ–
        p4_2 = self.conv1(p4)
        # h  å’Œ w ç»´åº¦æ‰©å……ä¸ºåŸæ¥çš„ä¸¤å€
        p4_2 = self.dysample1(p4_2)
        # ç»´åº¦å˜åŒ–
        p5_2 = self.conv2(p5)
        # h  å’Œ w ç»´åº¦æ‰©å……ä¸ºåŸæ¥çš„å››å€ï¼Œç›¸å½“äºP3çš„ç»´åº¦
        p5_2 = self.dysample2(p5_2)
        # å°†å¼ é‡p3åœ¨å€’æ•°ç¬¬ä¸‰ä¸ªç»´åº¦ä¸Šå¢åŠ ä¸€ä¸ªç»´åº¦
        p3_3d = torch.unsqueeze(p3, -3)
        p4_3d = torch.unsqueeze(p4_2, -3)
        p5_3d = torch.unsqueeze(p5_2, -3)
        # å°†ä¸‰ä¸ªçŸ©é˜µåœ¨å€’æ•°ç¬¬ä¸‰ä¸ªç»´åº¦è¿›è¡Œcontact
        combine = torch.cat([p3_3d, p4_3d, p5_3d], dim=2)
        conv_3d = self.conv3d(combine)
        bn = self.bn(conv_3d)
        act = self.act(bn)
        x = self.pool_3d(act)
        x = torch.squeeze(x, 2)
        return x


class Add(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        # æ¥æ”¶ä¸€ä¸ªè¾“å…¥å¼ é‡ xï¼Œç„¶åä½¿ç”¨ torch.stack(x, dim=0) å°†è¾“å…¥å¼ é‡ä¸­çš„å¼ é‡åœ¨æŒ‡å®šç»´åº¦ï¼ˆè¿™é‡Œæ˜¯ç»´åº¦0ï¼‰ä¸Šè¿›è¡Œå †å ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„å¼ é‡ã€‚
        # æœ€åï¼Œä½¿ç”¨ torch.sum(..., dim=0) å¯¹å †å åçš„å¼ é‡åœ¨ç»´åº¦0ä¸Šè¿›è¡Œæ±‚å’Œï¼Œå³å¯¹æ‰€æœ‰å¼ é‡è¿›è¡Œé€å…ƒç´ ç›¸åŠ ï¼Œæœ€ç»ˆè¿”å›æ±‚å’Œç»“æœ
        return torch.sum(torch.stack(x, dim=0), dim=0)

#
class asf_channel_att(nn.Module):
    def __init__(self, channel, b=1, gamma=2):
        super(asf_channel_att, self).__init__()
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1

        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = y.squeeze(-1)
        y = y.transpose(-1, -2)
        y = self.conv(y).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return x * y.expand_as(x)


class asf_local_att(nn.Module):
    def __init__(self, channel, reduction=16):
        super(asf_local_att, self).__init__()

        self.conv_1x1 = nn.Conv2d(in_channels=channel, out_channels=channel // reduction, kernel_size=1, stride=1,
                                  bias=False)

        self.relu = nn.ReLU()
        self.bn = nn.BatchNorm2d(channel // reduction)

        self.F_h = nn.Conv2d(in_channels=channel // reduction, out_channels=channel, kernel_size=1, stride=1,
                             bias=False)
        self.F_w = nn.Conv2d(in_channels=channel // reduction, out_channels=channel, kernel_size=1, stride=1,
                             bias=False)

        self.sigmoid_h = nn.Sigmoid()
        self.sigmoid_w = nn.Sigmoid()

    def forward(self, x):
        _, _, h, w = x.size()

        x_h = torch.mean(x, dim=3, keepdim=True).permute(0, 1, 3, 2)
        x_w = torch.mean(x, dim=2, keepdim=True)

        x_cat_conv_relu = self.relu(self.bn(self.conv_1x1(torch.cat((x_h, x_w), 3))))

        x_cat_conv_split_h, x_cat_conv_split_w = x_cat_conv_relu.split([h, w], 3)

        s_h = self.sigmoid_h(self.F_h(x_cat_conv_split_h.permute(0, 1, 3, 2)))
        s_w = self.sigmoid_w(self.F_w(x_cat_conv_split_w))

        out = x * s_h.expand_as(x) * s_w.expand_as(x)
        return out

# CPAM åœ¨è¿™é‡Œçš„ä½œç”¨ç›¸å½“äºc2f  å…·ä½“æ“ä½œæ²¡çœ‹
class asf_attention_model(nn.Module):
    # Concatenate a list of tensors along dimension
    def __init__(self, ch=256):
        super().__init__()
        self.channel_att = asf_channel_att(ch)
        self.local_att = asf_local_att(ch)

    def forward(self, x):
        input1, input2 = x[0], x[1]
        input1 = self.channel_att(input1)
        x = input1 + input2
        x = self.local_att(x)
        return x

######################################## Attentional Scale Sequence Fusion end ########################################


######################################## SPD-Conv start ########################################
# å·ç§¯æ ¸ä¸º1ï¼Œå‚æ•°é‡ä¼šä¸‹é™ï¼›ä¸º3ï¼Œå‚æ•°é‡é«˜äºCONV
# ä¸‹é‡‡æ ·é«˜åº¦å’Œå®½åº¦é™ä¸ºä¸€åŠï¼Œæ‰€ä»¥Spdconvå°±ç›´æ¥æŒ‰é¡ºåºå–4*4ä¸­2*2ä¸ªç‚¹
class SPDConv(nn.Module):
    # Changing the dimension of the Tensor
    # incï¼šè¾“å…¥é€šé“æ•°     oucï¼šè¾“å‡ºé€šé“æ•°  dimensionï¼šåˆ‡åˆ†çš„ç»´åº¦ï¼Œé»˜è®¤ä¸º1ï¼Œè¡¨ç¤ºç©ºé—´ç»´åº¦
    def __init__(self, inc, ouc, dimension=1):
        super().__init__()
        self.d = dimension
        self.conv = Conv(inc * 4, ouc, k=3)

    def forward(self, x):
        # æœ€åé¢çš„1æ˜¯æŒ‡åœ¨ç©ºé—´ç»´åº¦è¿›è¡Œåˆ†å‰²ï¼Œä¹Ÿå°±æ˜¯ä¸Šé¢çš„ self.d
        # x[..., ::2, ::2]ï¼šè¿™æ˜¯ Python ä¸­çš„åˆ‡ç‰‡æ“ä½œï¼Œ... è¡¨ç¤ºçœç•¥çš„ç»´åº¦ï¼Œ::2 è¡¨ç¤ºä»¥æ­¥é•¿ä¸º2è¿›è¡Œåˆ‡ç‰‡ã€‚å› æ­¤ï¼Œè¿™éƒ¨åˆ†ä»£ç è¡¨ç¤ºå¯¹ x å¼ é‡åœ¨ç©ºé—´ç»´åº¦ä¸ŠæŒ‰ç…§æ­¥é•¿ä¸º2è¿›è¡Œåˆ‡åˆ†ï¼Œä¿ç•™ç´¢å¼•ä¸ºå¶æ•°çš„å…ƒç´ ã€‚ä¹Ÿå°±æ˜¯2*2çŸ©é˜µä¸­çš„ç¬¬äºŒè¡Œç¬¬äºŒåˆ—
        # x[..., 1::2, ::2]ï¼šç±»ä¼¼åœ°ï¼Œè¿™éƒ¨åˆ†ä»£ç è¡¨ç¤ºå¯¹ x å¼ é‡åœ¨ç©ºé—´ç»´åº¦ä¸ŠæŒ‰ç…§æ­¥é•¿ä¸º2è¿›è¡Œåˆ‡åˆ†ï¼Œä¿ç•™ç´¢å¼•ä¸ºå¥‡æ•°çš„å…ƒç´ ã€‚ä¹Ÿå°±æ˜¯2*2çŸ©é˜µä¸­çš„ç¬¬ä¸€è¡Œç¬¬äºŒåˆ—
        # x[..., ::2, 1::2] ä¹Ÿå°±æ˜¯2*2çŸ©é˜µä¸­çš„ç¬¬äºŒè¡Œç¬¬ä¸€åˆ—        x[..., 1::2, 1::2] ä¹Ÿå°±æ˜¯2*2çŸ©é˜µä¸­çš„ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—
        x = torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1)
        # è¿›è¡Œé™ç»´
        x = self.conv(x)
        return x

######################################## SPD-Conv end ########################################


######################################## ContextGuidedBlock start ########################################
# æ™®é€šçš„é€šé“æ³¨æ„åŠ›æœºåˆ¶
class FGlo(nn.Module):
    """
    the FGlo class is employed to refine the joint feature of both local feature and surrounding context.
    """

    def __init__(self, channel, reduction=16):
        super(FGlo, self).__init__()
        # å¯¹hå’Œwè¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # å®šä¹‰FCæ¨¡å—
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        # è·å–é€šé“çš„æƒé‡
        y = self.avg_pool(x).view(b, c)
        # ä½¿ç”¨FCé™ç»´ï¼Œåœ¨å‡ç»´
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

# ContextGuidedBlockå’ŒContextGuidedBlock_Downçš„åŒºåˆ«æ˜¯ï¼šContextGuidedBlockä½¿ç”¨äº†æ®‹å·®æ¨¡å—ï¼Œè¾“å‡º+è¾“å…¥=æœ€åè¾“å‡º
# å…¶ä»–éƒ½ä¸€æ ·
class ContextGuidedBlock(nn.Module):
    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16, add=True):
        """
        args:
           nIn: number of input channels
           nOut: number of output channels,
           add: if true, residual learning
        """
        super().__init__()
        n = int(nOut / 2)
        self.conv1x1 = Conv(nIn, n, 1, 1)  # 1x1 Conv is employed to reduce the computation
        # æ™®é€šå·ç§¯
        self.F_loc = nn.Conv2d(n, n, 3, padding=1, groups=n)
        # ç©ºæ´å·ç§¯
        self.F_sur = nn.Conv2d(n, n, 3, padding=autopad(3, None, dilation_rate), dilation=dilation_rate,
                               groups=n)  # surrounding context
        self.bn_act = nn.Sequential(
            nn.BatchNorm2d(nOut),
            Conv.default_act
        )
        self.add = add
        self.F_glo = FGlo(nOut, reduction)

    def forward(self, input):
        output = self.conv1x1(input)
        loc = self.F_loc(output)
        sur = self.F_sur(output)

        joi_feat = torch.cat([loc, sur], 1)

        joi_feat = self.bn_act(joi_feat)

        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature
        # if residual version
        if self.add:
            output = input + output
        return output


class ContextGuidedBlock_Down(nn.Module):
    """
    the size of feature map divided 2, (H,W,C)---->(H/2, W/2, 2C)
    """

    def __init__(self, nIn, dilation_rate=2, reduction=16):
        """
        args:
           nIn: the channel of input feature map
           nOut: the channel of output feature map, and nOut=2*nIn
        """
        super().__init__()
        nOut = 2 * nIn
        self.conv1x1 = Conv(nIn, nOut, 3, s=2)  # size/2, channel: nIn--->nOut

        # groups=nOutï¼šåˆ†ç»„å·ç§¯çš„ç»„æ•°ï¼Œè¡¨ç¤ºæ¯ä¸ªé€šé“ç»„å†…çš„å·ç§¯æ ¸æ˜¯ç‹¬ç«‹çš„
        self.F_loc = nn.Conv2d(nOut, nOut, 3, padding=1, groups=nOut)
        self.F_sur = nn.Conv2d(nOut, nOut, 3, padding=autopad(3, None, dilation_rate), dilation=dilation_rate,
                               groups=nOut)

        self.bn = nn.BatchNorm2d(2 * nOut, eps=1e-3)
        self.act = Conv.default_act
        self.reduce = Conv(2 * nOut, nOut, 1, 1)  # reduce dimension: 2*nOut--->nOut

        self.F_glo = FGlo(nOut, reduction)

    def forward(self, input):
        # é€šé“æ•°æ‰©å¤§åŸæ¥çš„ä¸¤å€
        output = self.conv1x1(input)

        loc = self.F_loc(output)
        sur = self.F_sur(output)

        # åœ¨ç©ºé—´ç»´åº¦ä¸Šå°†locå’Œsurè¿æ¥èµ·æ¥
        joi_feat = torch.cat([loc, sur], 1)  # the joint feature
        joi_feat = self.bn(joi_feat)
        joi_feat = self.act(joi_feat)
        # å·ç§¯è¿›è¡Œé€šé“é™ç»´
        joi_feat = self.reduce(joi_feat)  # channel= nOut

        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature

        return output


class C3_ContextGuided(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(ContextGuidedBlock(c_, c_) for _ in range(n)))

# ç›´æ¥ç”¨ContextGuidedBlockè¿™ä¸ªæ¨¡å—ä»£æ›¿bottleneckï¼Œè€Œä¸æ˜¯bottlenecké‡Œé¢çš„conv
class C2f_ContextGuided(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(ContextGuidedBlock(self.c, self.c) for _ in range(n))

######################################## ContextGuidedBlock end ########################################

######################################## YOLOV7 start ########################################

class V7DownSampling(nn.Module):
    def __init__(self, inc, ouc) -> None:
        super(V7DownSampling, self).__init__()

        ouc = ouc // 2
        self.maxpool = nn.Sequential(
            # å…ˆå±€éƒ¨æ± åŒ–ï¼Œå†é™ç»´
            nn.MaxPool2d(kernel_size=2, stride=2),
            Conv(inc, ouc, k=1)
        )
        self.conv = nn.Sequential(
            # å…ˆé™ç»´ï¼Œå†ä¸‹é‡‡æ ·
            Conv(inc, ouc, k=1),
            Conv(ouc, ouc, k=3, s=2),
        )

    def forward(self, x):
        return torch.cat([self.maxpool(x), self.conv(x)], dim=1)

######################################## YOLOV7 end ########################################

######################################## HWD start ########################################

class HWD(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(HWD, self).__init__()
        from pytorch_wavelets import DWTForward
        self.wt = DWTForward(J=1, mode='zero', wave='haar')
        self.conv = Conv(in_ch * 4, out_ch, 1, 1)

    def forward(self, x):
        yL, yH = self.wt(x)
        y_HL = yH[0][:, :, 0, ::]
        y_LH = yH[0][:, :, 1, ::]
        y_HH = yH[0][:, :, 2, ::]
        x = torch.cat([yL, y_HL, y_LH, y_HH], dim=1)
        x = self.conv(x)

        return x

######################################## HWD end ########################################


######################################## ScConv begin ########################################

# CVPR2023 https://openaccess.thecvf.com/content/CVPR2023/papers/Li_SCConv_Spatial_and_Channel_Reconstruction_Convolution_for_Feature_Redundancy_CVPR_2023_paper.pdf

# çŸ¥ä¹ä¸“æ é“¾æ¥ æœ‰ScConvçš„å›¾ç‰‡  https://zhuanlan.zhihu.com/p/649680775
# æ—¨åœ¨æœ‰æ•ˆåœ°é™åˆ¶ç‰¹å¾å†—ä½™ï¼Œå¢å¼ºäº†ç‰¹å¾è¡¨ç¤ºçš„èƒ½åŠ›

class GroupBatchnorm2d(nn.Module):
    def __init__(self, c_num: int,
                 group_num: int = 16,
                 eps: float = 1e-10
                 ):
        super(GroupBatchnorm2d, self).__init__()
        assert c_num >= group_num
        self.group_num = group_num
        self.gamma = nn.Parameter(torch.randn(c_num, 1, 1))
        self.beta = nn.Parameter(torch.zeros(c_num, 1, 1))
        self.eps = eps

    def forward(self, x):
        N, C, H, W = x.size()
        x = x.view(N, self.group_num, -1)
        mean = x.mean(dim=2, keepdim=True)
        std = x.std(dim=2, keepdim=True)
        x = (x - mean) / (std + self.eps)
        x = x.view(N, C, H, W)
        return x * self.gamma + self.beta


class SRU(nn.Module):
    def __init__(self,
                 oup_channels: int,
                 group_num: int = 16,
                 gate_treshold: float = 0.5
                 ):
        super().__init__()

        self.gn = GroupBatchnorm2d(oup_channels, group_num=group_num)
        self.gate_treshold = gate_treshold
        self.sigomid = nn.Sigmoid()

    def forward(self, x):
        # å‡è®¾è¾“å…¥ï¼šx 2 * 128 * 20 *20
        # è°ƒç”¨äº†åˆ†ç»„å½’ä¸€åŒ–æ“ä½œï¼Œå°†è¾“å…¥ x è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
        (b,c,h,w)=x.size()
        print(1,b,c,h,w)
        # gn_x   2 * 128 * 20 * 20
        gn_x = self.gn(x)
        (b,c,h,w)=gn_x.size()
        print(2,b,c,h,w)
        # w_gamma  128 * 1 * 1
        # self.gn.gamma æ˜¯åˆ†ç»„å½’ä¸€åŒ–å±‚ä¸­çš„å¯å­¦ä¹ å‚æ•°ï¼Œå®ƒè¡¨ç¤ºæ¯ä¸ªé€šé“çš„æ‹‰ä¼¸ç³»æ•°
        w_gamma = self.gn.gamma / sum(self.gn.gamma)
        (b,c,h)=w_gamma.size()
        print(3,b,c,h)
        # reweigts  2 128 20 20
        reweigts = self.sigomid(gn_x * w_gamma)
        (b,c,h,w)=reweigts.size()
        print(3,b,c,h,w)
        # Gate  é—¨æ§åˆ¶ç­–ç•¥
        # info_mask æ˜¯ä¸€ä¸ªå¸ƒå°”æ©ç ï¼Œå…¶ä¸­å…ƒç´ ä¸º True è¡¨ç¤ºå¯¹åº”ä½ç½®çš„æƒé‡å¤§äºç­‰äºé—¨æ§é˜ˆå€¼ï¼Œå¦åˆ™ä¸º False
        info_mask = reweigts >= self.gate_treshold
        # noninfo_mask æ˜¯ info_mask çš„é€»è¾‘å–åï¼Œå³å¯¹åº”ä½ç½®çš„æƒé‡å°äºé—¨æ§é˜ˆå€¼æ—¶ä¸º Trueï¼Œå¦åˆ™ä¸º False
        noninfo_mask = reweigts < self.gate_treshold
        # info_mask å¯¹è¾“å…¥ x è¿›è¡Œç­›é€‰å¾—åˆ°çš„éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†å¯¹åº”çš„æƒé‡å¤§äºç­‰äºé—¨æ§é˜ˆå€¼
        # x_1  2 128 20 20
        x_1 = info_mask * x
        (b,c,h,w)=x_1.size()
        print(4,b,c,h,w)
        x_2 = noninfo_mask * x
        # è¿™è¡Œä»£ç è°ƒç”¨äº† self.reconstruct æ–¹æ³•ï¼Œå°†é€šè¿‡é—¨æ§æœºåˆ¶åˆ†å¼€çš„ä¸¤éƒ¨åˆ†ç‰¹å¾é‡æ–°ç»„åˆæˆä¸€ä¸ªç‰¹å¾å¼ é‡ xã€‚
        # åœ¨è¿™ä¸ªæ–¹æ³•ä¸­ï¼Œx_1 å’Œ x_2 çš„å¯¹åº”ä½ç½®ä¼šç›¸åŠ ï¼Œç„¶åå†è¿›è¡Œè¿æ¥ï¼Œå½¢æˆæœ€ç»ˆçš„è¾“å‡ºç‰¹å¾å¼ é‡
        x = self.reconstruct(x_1, x_2)
        return x

    def reconstruct(self, x_1, x_2):
        # è¿™è¡Œä»£ç å°†å¼ é‡ x_1 æ²¿ç€é€šé“ç»´åº¦ï¼ˆå³ç¬¬ 1 ç»´ï¼‰åˆ†å‰²æˆä¸¤ä¸ªéƒ¨åˆ† x_11 å’Œ x_12ï¼Œæ¯ä¸ªéƒ¨åˆ†çš„é€šé“æ•°ä¸ºåŸå§‹å¼ é‡çš„ä¸€åŠ
        x_11, x_12 = torch.split(x_1, x_1.size(1) // 2, dim=1)
        x_21, x_22 = torch.split(x_2, x_2.size(1) // 2, dim=1)
        return torch.cat([x_11 + x_22, x_12 + x_21], dim=1)


class CRU(nn.Module):
    '''
    alpha: 0<alpha<1
    '''

    def __init__(self,
                 op_channel: int,
                 alpha: float = 1 / 2,
                 squeeze_radio: int = 2,
                 group_size: int = 2,
                 group_kernel_size: int = 3,
                 ):
        super().__init__()
        self.up_channel = up_channel = int(alpha * op_channel)
        self.low_channel = low_channel = op_channel - up_channel
        self.squeeze1 = nn.Conv2d(up_channel, up_channel // squeeze_radio, kernel_size=1, bias=False)
        self.squeeze2 = nn.Conv2d(low_channel, low_channel // squeeze_radio, kernel_size=1, bias=False)
        # up
        self.GWC = nn.Conv2d(up_channel // squeeze_radio, op_channel, kernel_size=group_kernel_size, stride=1,
                             padding=group_kernel_size // 2, groups=group_size)
        self.PWC1 = nn.Conv2d(up_channel // squeeze_radio, op_channel, kernel_size=1, bias=False)
        # low
        self.PWC2 = nn.Conv2d(low_channel // squeeze_radio, op_channel - low_channel // squeeze_radio, kernel_size=1,
                              bias=False)
        self.advavg = nn.AdaptiveAvgPool2d(1)

    def forward(self, x):
        # Split  æŒ‰æ¯”ä¾‹ç©ºé—´ç»´åº¦åˆ†å‰²
        up, low = torch.split(x, [self.up_channel, self.low_channel], dim=1)
        # å·ç§¯æ“ä½œ
        up, low = self.squeeze1(up), self.squeeze2(low)
        # Transform
        Y1 = self.GWC(up) + self.PWC1(up)
        Y2 = torch.cat([self.PWC2(low), low], dim=1)
        # Fuse
        # å…ˆæ‹¼æ¥ï¼Œç„¶åæƒé‡ä¸åŸæ¥ç›¸ä¹˜
        out = torch.cat([Y1, Y2], dim=1)
        out = F.softmax(self.advavg(out), dim=1) * out
        out1, out2 = torch.split(out, out.size(1) // 2, dim=1)
        return out1 + out2


class ScConv(nn.Module):
    # https://github.com/cheng-haha/ScConv/blob/main/ScConv.py
    def __init__(self,
                 op_channel: int,
                 group_num: int = 16,
                 gate_treshold: float = 0.5,
                 alpha: float = 1 / 2,
                 squeeze_radio: int = 2,
                 group_size: int = 2,
                 group_kernel_size: int = 3,
                 ):
        super().__init__()
        self.SRU = SRU(op_channel,
                       group_num=group_num,
                       gate_treshold=gate_treshold)
        self.CRU = CRU(op_channel,
                       alpha=alpha,
                       squeeze_radio=squeeze_radio,
                       group_size=group_size,
                       group_kernel_size=group_kernel_size)

    def forward(self, x):
        x = self.SRU(x)
        x = self.CRU(x)
        return x


class Bottleneck_ScConv(Bottleneck):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = ScConv(c2)


class C3_ScConv(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_ScConv(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))


class C2f_ScConv(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_ScConv(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## ScConv end ########################################


######################################## EMSConv+EMSConvP begin ########################################
# å¯¼å€Ÿé‰´ã€ŠScale-Aware Modulation Meet Transformerã€‹ä¸­çš„ MHMC è®¾è®¡çš„æ¨¡å—  yolov8-C2f-EMSC.yaml  yolov8-C2f-EMSCP.yaml
# å€Ÿé‰´ ghostnet å’Œ x/SMT   ghostnetï¼šå‡å°‘å†—ä½™ä¿¡æ¯ï¼Œå…ˆæ˜¯1*1å·ç§¯é™ç»´ï¼Œç„¶åæ•°ä¸ªDWconvï¼Œæœ€åæ‹¼æ¥åˆ°ä¸€èµ·

# EMSConvçš„å›¾ç‰‡åœ¨ original_ultralytics-mainçš„æ–‡ä»¶å¤¹ä¸­
# EMSConv æ¥æ”¶çš„å·ç§¯åªèƒ½æœ‰ä¸¤ä¸ª       EMSConvPæ¥æ”¶çš„å·ç§¯åªèƒ½æœ‰å››ä¸ª   ä½œè€…æŠŠä»–å†™æ­»äº†      è¾“å…¥è¾“å‡ºé€šé“æ•°ä¸å˜
# å°±EMSConvè€Œè¨€ï¼Œè®¡ç®—é‡ä¼šæ¯”convä½ï¼Œä½†æ˜¯ç»“æ„å¤æ‚ï¼Œæ‰€ä»¥FPSä¼šå‡ä½    ç”±äºä¸€äº›åµŒå…¥å¼è®¾å¤‡ï¼Œå†…å­˜é™åˆ¶ï¼Œæ‰€ä»¥éœ€è¦é™ä½å‚æ•°é‡
# ä½¿ç”¨EMSConvå¿…é¡»é€šé“æ•° >= 64ï¼Œæ‰€ä»¥åªåœ¨è¾ƒæ·±çš„å±‚æ‰ä½¿ç”¨   å¯¹ç‰¹å¾å›¾è¿›è¡Œåˆ†ç»„è¿ç®—
class EMSConv(nn.Module):           # è¾“å…¥è¾“å‡ºå¤§å°ä¸å˜
    # Efficient Multi-Scale Conv  ç²¾ç®€å¤šå°ºåº¦å·ç§¯   å‚æ•°é‡è¾ƒæ™®é€šå·ç§¯å‡å°‘
    def __init__(self, channel, kernels=[3, 5]):
        super().__init__()
        # len(kernels) è¿”å› kernels åˆ—è¡¨çš„é•¿åº¦ï¼Œå³åˆ—è¡¨ä¸­å…ƒç´ çš„ä¸ªæ•°,åœ¨è¿™é‡Œå°±æ˜¯2
        self.groups = len(kernels)
        # if not isinstance(kernels, list):
        #     kernels = [kernels]  # å°† kernels è½¬æ¢ä¸ºåˆ—è¡¨
        # self.groups = 2
        min_ch = channel // 4
        assert min_ch >= 16, f'channel must Greater than {64}, but {channel}'

        # æ–°å»ºä¸€ä¸ªç©ºçš„æ¨¡å—ç»„åˆåˆ—è¡¨ï¼Œå«åš self.convs
        self.convs = nn.ModuleList([])
        # ä¸ºæ¯ä¸ª ks åˆ›å»ºä¸€ä¸ªå·ç§¯å±‚ï¼Œç„¶åå°†è¯¥å·ç§¯å±‚æ·»åŠ åˆ° self.convs ä¸­
        # self.convs ä¸­å°±åŒ…å«äº†ä¸¤ä¸ªå·ç§¯å±‚ï¼Œä¸€ä¸ªæ˜¯ kernel size ä¸º 3ï¼Œå¦ä¸€ä¸ªæ˜¯ kernel size ä¸º 5 çš„å·ç§¯å±‚
        # æ„Ÿè§‰appendå°±æ˜¯æ¡‰é¡ºåºæ‹¼æ¥åˆ° self.convs åé¢
        for ks in kernels:
            self.convs.append(Conv(c1=min_ch, c2=min_ch, k=ks))
        self.conv_1x1 = Conv(channel, channel, k=1)

    def forward(self, x):
        # å‡è®¾è¾“å…¥ä¸º 2 256 160 160
        _, c, _, _ = x.size()
        # ç©ºé—´ç»´åº¦ä¸€åˆ†ä¸ºäºŒ   ä¸€åŠçš„é€šé“ä»€ä¹ˆéƒ½ä¸åš
        x_cheap, x_group = torch.split(x, [c // 2, c // 2], dim=1)
        # å°†x_groupçš„ç©ºé—´ç»´åº¦åˆ’åˆ†ä¸ºäºŒéƒ¨åˆ†ï¼Œg = 2
        x_group = rearrange(x_group, 'bs (g ch) h w -> bs ch h w g', g=self.groups)
        # x_group[..., i] è¡¨ç¤ºåœ¨å¼ é‡ x_group çš„æœ€åä¸€ä¸ªç»´åº¦ä¸Šå–ç´¢å¼•ä¸º i çš„åˆ‡ç‰‡,æ¯”å¦‚è¯´ x_group[..., 1] å°±æ˜¯ 2 64 160 160
        # å¯¹è¾“å…¥ x_group ä¸­çš„æ¯ä¸ªåˆ†ç»„åº”ç”¨å¯¹åº”çš„å·ç§¯å±‚ï¼Œå¹¶å°†ç»“æœå †å æˆä¸€ä¸ªå¼ é‡åˆ—è¡¨
        # torch.stack ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œå…¶ä¸­åŒ…å«ç»™å®šå¼ é‡åºåˆ—ä¸­çš„æ‰€æœ‰å¼ é‡ï¼Œè¿™äº›å¼ é‡æŒ‰ç…§æŒ‡å®šçš„è½´ï¼ˆç»´åº¦ï¼‰è¿›è¡Œå †å ï¼Œè¿™é‡Œæ²¡æœ‰æŒ‡å®šç»´åº¦ï¼Œæ‰€ä»¥é»˜è®¤ä¸º0
        x_group = torch.stack([self.convs[i](x_group[..., i]) for i in range(len(self.convs))])
        # æŒ‰ç…§ä»–è¿™é‡Œçš„å†™æ³•ï¼Œå·ç§¯åå¾—åˆ°çš„æ–°ç»´åº¦åœ¨ dim=0 å¤„ï¼Œæ‰€ä»¥ g åœ¨dim=0çš„ä½ç½®
        x_group = rearrange(x_group, 'g bs ch h w -> bs (g ch) h w')
        x = torch.cat([x_cheap, x_group], dim=1)
        # 1*1 äº¤æ¢é€šé“ä¿¡æ¯
        x = self.conv_1x1(x)
        return x


# ä¸ä¸Šé¢çš„åŒºåˆ«æ˜¯å°†è¾“å…¥ç‰¹å¾å›¾åˆ’åˆ†æˆå››ä»½ï¼Œåˆ†åˆ«è¿›è¡Œ1ï¼Œ3ï¼Œ5ï¼Œ7çš„å·ç§¯æ“ä½œ
# ä¸‹é¢è®¾ç½®çš„å·ç§¯æ ¸çš„å¤§å°æ˜¯å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚æ”¹å˜çš„
# è¾“å…¥è¾“å‡ºå¤§å°ä¸å˜
class EMSConvP(nn.Module):
    # Efficient Multi-Scale Conv Plus
    def __init__(self, channel=256, kernels=[1, 3, 5, 7]):
        super().__init__()
        self.groups = len(kernels)
        min_ch = channel // self.groups
        assert min_ch >= 16, f'channel must Greater than {16 * self.groups}, but {channel}'

        self.convs = nn.ModuleList([])
        for ks in kernels:
            self.convs.append(Conv(c1=min_ch, c2=min_ch, k=ks))
        self.conv_1x1 = Conv(channel, channel, k=1)

    def forward(self, x):
        x_group = rearrange(x, 'bs (g ch) h w -> bs ch h w g', g=self.groups)
        x_convs = torch.stack([self.convs[i](x_group[..., i]) for i in range(len(self.convs))])
        x_convs = rearrange(x_convs, 'g bs ch h w -> bs (g ch) h w')
        x_convs = self.conv_1x1(x_convs)

        return x_convs

# åœ¨EMSConvæœ€åé¢åŠ å…¥äº†Conv3*3çš„ä¸‹é‡‡æ ·æ¨¡å—
class EMSConv_down(nn.Module):
    def __init__(self, channel,out, kernels=[3, 5]):
        super().__init__()
        self.groups = len(kernels)
        min_ch = channel // 4
        assert min_ch >= 16, f'channel must Greater than {64}, but {channel}'

        self.convs = nn.ModuleList([])
        for ks in kernels:
            self.convs.append(Conv(c1=min_ch, c2=min_ch, k=ks))
        self.conv_1x1 = Conv(channel, channel, k=1)
        self.conv_3x3 = Conv(channel, out, 3, 2)

    def forward(self, x):
        # å‡è®¾è¾“å…¥ä¸º 2 256 160 160
        _, c, _, _ = x.size()
        # ç©ºé—´ç»´åº¦ä¸€åˆ†ä¸ºäºŒ   ä¸€åŠçš„é€šé“ä»€ä¹ˆéƒ½ä¸åš
        x_cheap, x_group = torch.split(x, [c // 2, c // 2], dim=1)
        # å°†x_groupçš„ç©ºé—´ç»´åº¦åˆ’åˆ†ä¸ºäºŒéƒ¨åˆ†ï¼Œg = 2
        x_group = rearrange(x_group, 'bs (g ch) h w -> bs ch h w g', g=self.groups)
        x_group = torch.stack([self.convs[i](x_group[..., i]) for i in range(len(self.convs))])
        x_group = rearrange(x_group, 'g bs ch h w -> bs (g ch) h w')
        x = torch.cat([x_cheap, x_group], dim=1)
        x = self.conv_1x1(x)
        x = self.conv_3x3(x)
        return x

class Bottleneck_EMSC(Bottleneck):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = EMSConv(c2)


class C3_EMSC(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_EMSC(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))


class C2f_EMSC(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_EMSC(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))


class Bottleneck_EMSCP(Bottleneck):
    def __init__(self, c1, c2, shortcut=True, g=1, k=(3, 3), e=0.5):
        super().__init__(c1, c2, shortcut, g, k, e)
        c_ = int(c2 * e)  # hidden channels
        self.cv1 = Conv(c1, c_, k[0], 1)
        self.cv2 = EMSConvP(c2)


class C3_EMSCP(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Bottleneck_EMSCP(c_, c_, shortcut, g, k=((1, 1), (3, 3)), e=1.0) for _ in range(n)))


class C2f_EMSCP(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Bottleneck_EMSCP(self.c, self.c, shortcut, g, k=(3, 3), e=1.0) for _ in range(n))

######################################## EMSConv+EMSConvP end ########################################


######################################## RCSOSA start ########################################
# Rcs-YOLOä¸­çš„ RCSOSA  å‚æ•°é‡è¾ƒå¤§,å¯¼ç”¨æ¥ä»£æ›¿c2f         yolov8-RCSOSA.yaml
from ultralytics.utils.torch_utils import make_divisible

class SR(nn.Module):
    # Shuffle RepVGG    shuffleèƒ½å¼•å‡ºå¾ˆå¤šåˆ›æ–°
    def __init__(self, c1, c2):
        super().__init__()
        c1_ = int(c1 // 2)
        c2_ = int(c2 // 2)
        self.repconv = RepConv(c1_, c2_, bn=True)

    def forward(self, x):
        # æ²¿ç©ºé—´ç»´åº¦ä¸€åˆ†ä¸ºäºŒ
        x1, x2 = x.chunk(2, dim=1)
        out = torch.cat((x1, self.repconv(x2)), dim=1)
        # ç»è¿‡ä¸Šé¢çš„æ‹¼æ¥ååœ¨æ´—ç‰Œæ“ä½œ
        out = self.channel_shuffle(out, 2)
        return out

    # groupsæ˜¯æŒ‡è¾“å…¥å‘é‡ x çš„ç©ºé—´ç»´åº¦éœ€è¦è¢«åˆ†ä¸º groups ä¸ª
    # ä¾‹å­ å‡è®¾æœ‰ä¸€ä¸ªå‘é‡æ˜¯{[x1  x2], [x3  x4]} ç»è¿‡ channel_shuffle ä¼šå˜æˆ{[x1  x3], [x2  x4]}
    def channel_shuffle(self, x, groups):
        batchsize, num_channels, height, width = x.size()
        channels_per_group = num_channels // groups
        # æ”¹å˜ x çš„æ’åˆ—é¡ºåº
        x = x.view(batchsize, groups, channels_per_group, height, width)
        # å¯¹è¾“å…¥å¼ é‡ x è¿›è¡Œè½¬ç½®æ“ä½œï¼Œå°†ç¬¬1ç»´å’Œç¬¬2ç»´è¿›è¡Œäº¤æ¢ï¼Œç„¶åè°ƒç”¨.contiguous()æ–¹æ³•ä½¿å¾—å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨çš„ã€‚
        x = torch.transpose(x, 1, 2).contiguous()
        x = x.view(batchsize, -1, height, width)
        return x

# yamlä¸­éœ€è¦å¯¹æ˜¯å¦ä½¿ç”¨SEæ³¨æ„åŠ›è¿›è¡Œ true æˆ–è€… false
class RCSOSA(nn.Module):
    # VoVNet with Res Shuffle RepVGG
    def __init__(self, c1, c2, n=1, se=False, g=1, e=0.5):
        super().__init__()
        # n_ ç”±yamlæ–‡ä»¶ä¸­[-1, 6, RCSOSA, [256]] çš„ 6 * depth æ¥æ§åˆ¶
        n_ = n // 2
        # make_divisible(value, 8)çš„ä½œç”¨å°±æ˜¯è¿”å›å¤§äºæˆ–ç­‰äºvalueçš„æœ€å°çš„8çš„å€æ•°  ä¸ºäº†é™ç»´
        c_ = make_divisible(int(c1 * e), 8)
        self.conv1 = RepConv(c1, c_, bn=True)
        self.conv3 = RepConv(int(c_ * 3), c2, bn=True)
        # åˆ›å»ºäº†ä¸€ä¸ªç”± n_ ä¸ªSRæ¨¡å—ç»„æˆçš„Sequentialå®¹å™¨
        self.sr1 = nn.Sequential(*[SR(c_, c_) for _ in range(n_)])
        self.sr2 = nn.Sequential(*[SR(c_, c_) for _ in range(n_)])

        self.se = None
        if se:
            self.se = SEAttention(c2)

    def forward(self, x):
        x1 = self.conv1(x)
        # å †å çš„SRæ¨¡å—
        x2 = self.sr1(x1)
        x3 = self.sr2(x2)
        x = torch.cat((x1, x2, x3), 1)
        return self.conv3(x) if self.se is None else self.se(self.conv3(x))

######################################## RCSOSA end ########################################




######################################## C2f-Faster begin ########################################
# ç»“æ„çœ‹imagesæ–‡ä»¶å¤¹
from timm.models.layers import DropPath

# PConv     å„ç§ç»´åº¦éƒ½ä¸å˜
class Partial_conv3(nn.Module):
    def __init__(self, dim, n_div=4, forward='split_cat'):
        super().__init__()
        self.dim_conv3 = dim // n_div
        self.dim_untouched = dim - self.dim_conv3
        # ç»´åº¦éƒ½ä¸å˜
        self.partial_conv3 = nn.Conv2d(self.dim_conv3, self.dim_conv3, 3, 1, 1, bias=False)

        # æ ¹æ® forward çš„å‚æ•°é€‰ä¸åŒå€¼
        if forward == 'slicing':
            self.forward = self.forward_slicing
        elif forward == 'split_cat':
            self.forward = self.forward_split_cat
        # å¦‚æœforwardå‚æ•°çš„å€¼ä¸æ˜¯è¿™ä¸¤è€…ä¹‹ä¸€ï¼Œåˆ™ä¼šå¼•å‘NotImplementedErrorå¼‚å¸¸
        else:
            raise NotImplementedError

    # forward_slicing æ˜¯ç”¨äºæ¨ç†é˜¶æ®µçš„
    def forward_slicing(self, x):
        # x.clone()æ–¹æ³•ç”¨äºåˆ›å»ºè¾“å…¥å¼ é‡ x çš„æ·±å±‚å‰¯æœ¬ã€‚è¿™æ„å‘³ç€å®ƒä¼šå¤åˆ¶å¼ é‡çš„æ•°æ®å’Œæ¢¯åº¦ä¿¡æ¯ï¼Œä½†æ˜¯ä¸ä¼šå…±äº«å­˜å‚¨ç©ºé—´ã€‚
        # è¿™æ ·åšçš„ç›®çš„æ˜¯ä¿ç•™åŸå§‹è¾“å…¥å¼ é‡çš„å€¼ï¼Œä»¥ä¾¿åœ¨åç»­çš„è®¡ç®—ä¸­ä½¿ç”¨
        x = x.clone()
        # å°†è¾“å…¥å¼ é‡çš„å‰ self.dim_conv3 ä¸ªé€šé“ï¼ˆå³éƒ¨åˆ†é€šé“ï¼‰ä¼ é€’ç»™ partial_conv3 æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œè€Œå‰©ä½™çš„é€šé“ä¿æŒä¸å˜ã€‚
        # æœ€åï¼Œè¿”å›å¤„ç†åçš„å¼ é‡
        x[:, :self.dim_conv3, :, :] = self.partial_conv3(x[:, :self.dim_conv3, :, :])
        return x

    def forward_split_cat(self, x):
        # for training/inference
        # æ ¹æ®self.dim_conv3, self.dim_untouchedçš„å€¼å¯¹è¾“å…¥ x è¿›è¡Œç©ºé—´ç»´åº¦çš„åˆ‡å‰²ï¼Œé»˜è®¤æ˜¯ 1ï¼š3 è¿›è¡Œåˆ‡å‰²
        x1, x2 = torch.split(x, [self.dim_conv3, self.dim_untouched], dim=1)
        # å¯¹åˆ‡å‰²å‡ºæ¥çš„x1è¿›è¡Œ3*3çš„å·ç§¯
        x1 = self.partial_conv3(x1)
        x = torch.cat((x1, x2), 1)
        return x


class Faster_Block(nn.Module):
    def __init__(self,
                 inc,
                 dim,
                 n_div=4,
                 mlp_ratio=2,
                 drop_path=0.1,
                 layer_scale_init_value=0.0,
                 pconv_fw_type='split_cat'
                 ):
        super().__init__()
        self.dim = dim
        self.mlp_ratio = mlp_ratio
        # ç»´åº¦ä¸å˜
        # åœ¨ç»™å®šçš„æ¦‚ç‡é˜ˆå€¼ä¸‹ï¼ŒDropPathæ¨¡å—ä¼šä»¥æ¦‚ç‡drop_pathä¸¢å¼ƒè¾“å…¥å¼ é‡çš„æŸäº›è·¯å¾„ã€‚
        # å…·ä½“æ¥è¯´ï¼Œå¯¹äºè¾“å…¥å¼ é‡çš„æ¯ä¸ªå…ƒç´ ï¼Œä»¥æ¦‚ç‡drop_pathå°†å…¶ç½®ä¸ºé›¶ï¼Œä»¥æ¦‚ç‡1-drop_pathä¿æŒä¸å˜ã€‚
        # ç„¶åï¼Œä¸ºäº†ä¿æŒæœŸæœ›å€¼ä¸å˜ï¼Œå‰©ä½™çš„éé›¶å…ƒç´ ä¼šæŒ‰ç…§1/(1-drop_path)è¿›è¡Œç¼©æ”¾ã€‚
        # è¿™æ ·åšçš„æ•ˆæœç±»ä¼¼äºåœ¨ç½‘ç»œä¸­æ·»åŠ äº†ä¸€äº›é¢å¤–çš„éšæœºæ€§ï¼Œæœ‰åŠ©äºå‡å°‘è¿‡æ‹Ÿåˆå¹¶æé«˜æ³›åŒ–æ€§èƒ½
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.n_div = n_div

        mlp_hidden_dim = int(dim * mlp_ratio)

        mlp_layer = [
            Conv(dim, mlp_hidden_dim, 1),
            nn.Conv2d(mlp_hidden_dim, dim, 1, bias=False)
        ]

        self.mlp = nn.Sequential(*mlp_layer)

        self.spatial_mixing = Partial_conv3(
            dim,
            n_div,
            pconv_fw_type
        )

        self.adjust_channel = None
        if inc != dim:      # å‡å¦‚è¾“å…¥!=è¾“å‡ºé€šé“æ•°
            self.adjust_channel = Conv(inc, dim, 1)

        if layer_scale_init_value > 0:
            # å®šä¹‰ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•° self.layer_scale
            self.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)
            self.forward = self.forward_layer_scale
        else:
            self.forward = self.forward

    def forward(self, x):
        # ç”¨äºåˆ¤æ–­è¾“å…¥æ˜¯å¦ç­‰äºè¾“å‡ºé€šé“æ•°ï¼Œå‡å¦‚ä¸ç›¸ç­‰ï¼Œç›´æ¥é€šè¿‡ä¸€ä¸ª 1 * 1 çš„å·ç§¯è¿›è¡Œç»´åº¦å˜åŒ–
        if self.adjust_channel is not None:
            x = self.adjust_channel(x)
        shortcut = x
        # å¯¹xè¿›è¡ŒPConvçš„æ“ä½œ
        x = self.spatial_mixing(x)
        x = shortcut + self.drop_path(self.mlp(x))
        return x

    # ä¸forwardçš„åŒºåˆ«
    # 1.å¼ºåˆ¶è¦æ±‚è¾“å…¥é€šé“æ•°=è¾“å‡ºé€šé“æ•°
    # 2.å¤šäº†ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°å»è°ƒæ•´PConvä¹‹è·¯çš„æƒé‡
    def forward_layer_scale(self, x):
        shortcut = x
        x = self.spatial_mixing(x)
        x = shortcut + self.drop_path(
            self.layer_scale.unsqueeze(-1).unsqueeze(-1) * self.mlp(x))
        return x


class C3_Faster(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Faster_Block(c_, c_) for _ in range(n)))

# æ®è¯´è½»é‡åŒ–çš„è¯ C2f_Faster æŒºå¥½ç”¨çš„
class C2f_Faster(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Faster_Block(self.c, self.c) for _ in range(n))


######################################## C2f-Faster end ########################################

######################################## C2f-Faster-EMA begin ########################################
# ç»“æ„çœ‹imagesæ–‡ä»¶å¤¹
# EMA æ·»åŠ åœ¨ Droppath åé¢
class Faster_Block_EMA(nn.Module):
    def __init__(self,
                 inc,
                 dim,
                 n_div=4,
                 mlp_ratio=2,
                 drop_path=0.1,
                 layer_scale_init_value=0.0,
                 pconv_fw_type='split_cat'
                 ):
        super().__init__()
        self.dim = dim
        self.mlp_ratio = mlp_ratio
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.n_div = n_div

        mlp_hidden_dim = int(dim * mlp_ratio)

        mlp_layer = [
            Conv(dim, mlp_hidden_dim, 1),
            nn.Conv2d(mlp_hidden_dim, dim, 1, bias=False)
        ]

        self.mlp = nn.Sequential(*mlp_layer)

        self.spatial_mixing = Partial_conv3(
            dim,
            n_div,
            pconv_fw_type
        )
        self.attention = EMA(dim)

        self.adjust_channel = None
        if inc != dim:
            self.adjust_channel = Conv(inc, dim, 1)

        if layer_scale_init_value > 0:
            self.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True)
            self.forward = self.forward_layer_scale
        else:
            self.forward = self.forward

    def forward(self, x):
        if self.adjust_channel is not None:
            x = self.adjust_channel(x)
        shortcut = x
        x = self.spatial_mixing(x)
        x = shortcut + self.attention(self.drop_path(self.mlp(x)))
        return x

    def forward_layer_scale(self, x):
        shortcut = x
        x = self.spatial_mixing(x)
        x = shortcut + self.drop_path(self.layer_scale.unsqueeze(-1).unsqueeze(-1) * self.mlp(x))
        return x


class C3_Faster_EMA(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(Faster_Block_EMA(c_, c_) for _ in range(n)))


class C2f_Faster_EMA(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(Faster_Block_EMA(self.c, self.c) for _ in range(n))

######################################## C2f-Faster-EMA end ########################################

######################################## RepViT start ########################################
from ultralytics.nn.backbone.repvit import Conv2d_BN, RepVGGDW, SqueezeExcite

# fnï¼šä¸€ä¸ªæ¨¡å—ä½œä¸ºè¾“å…¥å‚æ•°
# å®ç°åŠŸèƒ½:åœ¨å‰å‘ä¼ æ’­ä¸­å°†è¾“å…¥ x å’Œå­æ¨¡å— fn çš„è¾“å‡ºç›¸åŠ 
class Residual(nn.Module):
    def __init__(self, fn):
        super(Residual, self).__init__()
        self.fn = fn

    def forward(self, x):
        return self.fn(x) + x

class RepViTBlock(nn.Module):
    def __init__(self, inp, oup, use_se=True):  # use_se è¡¨ç¤ºæ˜¯å¦ä½¿ç”¨SEæ¨¡å—ï¼Œç”¨äºå¢å¼ºé€šé“ä¹‹é—´çš„äº¤äº’
        super(RepViTBlock, self).__init__()
        # è‹¥ inp == oup ï¼Œself.identityè¾“å‡ºä¸º True
        self.identity = inp == oup
        # éšè—å±‚çš„å±‚æ•°
        hidden_dim = 2 * inp

        self.token_mixer = nn.Sequential(
            # RepVGGDWæ˜¯ä¸€ä¸ªæ·±åº¦å¯åˆ†ç¦»çš„é‡å‚æ•°å·ç§¯
            RepVGGDW(inp),
            # SqueezeExciteæ˜¯åº“é‡Œé¢çš„ä¸€ä¸ªSEæ³¨æ„åŠ›æ¨¡å—ï¼Œ0.25æ˜¯æŒ‡ç”¨äºè®¡ç®—å‡å°‘è¾“å…¥é€šé“æ•°çš„æ¯”ä¾‹ï¼Œå³é€šé“å‹ç¼©ï¼Œé»˜è®¤æ˜¯1/16
            SqueezeExcite(inp, 0.25) if use_se else nn.Identity(),
        )
        self.channel_mixer = Residual(nn.Sequential(
            # pw
            Conv2d_BN(inp, hidden_dim, 1, 1, 0),
            nn.GELU(),
            # pw-linear
            Conv2d_BN(hidden_dim, oup, 1, 1, 0, bn_weight_init=0),
        ))

    def forward(self, x):
        return self.channel_mixer(self.token_mixer(x))

# EMAæ³¨æ„åŠ›ä»£æ›¿äº† SE æ³¨æ„åŠ›
class RepViTBlock_EMA(RepViTBlock):
    def __init__(self, inp, oup, use_se=True):
        super().__init__(inp, oup, use_se)

        self.token_mixer = nn.Sequential(
            RepVGGDW(inp),
            EMA(inp) if use_se else nn.Identity(),
        )


class C3_RVB(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(RepViTBlock(c_, c_, False) for _ in range(n)))


class C2f_RVB(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(RepViTBlock(self.c, self.c, False) for _ in range(n))


class C3_RVB_SE(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(RepViTBlock(c_, c_) for _ in range(n)))


class C2f_RVB_SE(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(RepViTBlock(self.c, self.c) for _ in range(n))


class C3_RVB_EMA(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(RepViTBlock_EMA(c_, c_) for _ in range(n)))


class C2f_RVB_EMA(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(RepViTBlock_EMA(self.c, self.c) for _ in range(n))

######################################## RepViT end ########################################

######################################## Dynamic Group Convolution Shuffle Transformer start ########################################

class DGCST(nn.Module):
    # Dynamic Group Convolution Shuffle Transformer
    def __init__(self, c1, c2) -> None:
        super().__init__()

        self.c = c2 // 4
        self.gconv = Conv(self.c, self.c, g=self.c)
        self.conv1 = Conv(c1, c2, 1)
        self.conv2 = nn.Sequential(
            Conv(c2, c2, 1),
            Conv(c2, c2, 1)
        )
        #x + self.conv2(x)

    def forward(self, x):
        # è¿›è¡Œç»´åº¦å˜åŒ–
        x = self.conv1(x)
        # æŒ‰ 1 ï¼š 3 çš„æ¯”ä¾‹å»åˆ†å‰²
        x1, x2 = torch.split(x, [self.c, x.size(1) - self.c], 1)
        # ç»„å·ç§¯
        x1 = self.gconv(x1)

        # ä¸‹é¢æ˜¯DGCStçš„shuffleæ“ä½œä»£ç 
        b, n, h, w = x1.size()
        #print(1,  b, n, h, w)
        b_n = b * n // 2
        y = x1.reshape(b_n, 2, h * w)
        # äº¤æ¢ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´
        y = y.permute(1, 0, 2)
        # n11, h11, w11 = y.size()
        # print(2, n11, h11, w11)
        y = y.reshape(2, -1, n // 2, h, w)
        # d1,b1, n1, h1, w1 = y.size()
        # print(4, d1, b1, n1, h1, w1)
        # å¾ˆå¥‡æ€ªï¼Œè¿™é‡Œy[0]å’Œy[1]æ˜¯æŒ‡ç¬¬äºŒç»´åº¦å’Œç¬¬ä¸‰ç»´åº¦
        # 2 2 4 160 160 å°±å˜æˆäº† 2 8 160 160
        y = torch.cat((y[0], y[1]), 1)

        x = torch.cat([y, x2], 1)
        return x + self.conv2(x)

######################################## Dynamic Group Convolution Shuffle Transformer end ########################################


######################################## C3 C2f Dilation-wise Residual start ########################################

class DWR(nn.Module):
    def __init__(self, dim) -> None:
        super().__init__()

        self.conv_3x3 = Conv(dim, dim // 2, 3)

        self.conv_3x3_d1 = Conv(dim // 2, dim, 3, d=1)
        self.conv_3x3_d3 = Conv(dim // 2, dim // 2, 3, d=3)
        self.conv_3x3_d5 = Conv(dim // 2, dim // 2, 3, d=5)

        self.conv_1x1 = Conv(dim * 2, dim, k=1)

    def forward(self, x):
        conv_3x3 = self.conv_3x3(x)
        x1, x2, x3 = self.conv_3x3_d1(conv_3x3), self.conv_3x3_d3(conv_3x3), self.conv_3x3_d5(conv_3x3)
        x_out = torch.cat([x1, x2, x3], dim=1)
        x_out = self.conv_1x1(x_out) + x
        return x_out


class C3_DWR(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(DWR(c_) for _ in range(n)))


class C2f_DWR(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(DWR(self.c) for _ in range(n))

######################################## C3 C2f Dilation-wise Residual end ########################################

######################################## iRMB and iRMB with CascadedGroupAttention and iRMB with DRB and iRMB with SWC start ########################################

class iRMB(nn.Module):
    def __init__(self, dim_in, dim_out, norm_in=True, has_skip=True, exp_ratio=1.0,
                 act=True, v_proj=True, dw_ks=3, stride=1, dilation=1, se_ratio=0.0, dim_head=16, window_size=7,
                 attn_s=True, qkv_bias=False, attn_drop=0., drop=0., drop_path=0., v_group=False, attn_pre=False):
        super().__init__()
        # ç”¨äºè§„èŒƒåŒ–è¾“å…¥æ•°æ®
        self.norm = nn.BatchNorm2d(dim_in) if norm_in else nn.Identity()
        # æ ¹æ®æ¡ä»¶é€‰æ‹©ä½¿ç”¨é»˜è®¤æ¿€æ´»å‡½æ•°æˆ–è€…æ’ç­‰å‡½æ•°
        self.act = Conv.default_act if act else nn.Identity()
        dim_mid = int(dim_in * exp_ratio)
        # åˆ¤æ–­æ˜¯å¦å­˜åœ¨è·³è·ƒè¿æ¥ï¼Œå½“è¾“å…¥ç»´åº¦ç­‰äºè¾“å‡ºç»´åº¦ä¸”æ­¥é•¿ä¸º1ï¼Œå¹¶ä¸”è®¾ç½®äº† has_skip å‚æ•°ï¼Œåˆ™ self.has_skip = True
        self.has_skip = (dim_in == dim_out and stride == 1) and has_skip
        self.attn_s = attn_s
        # ä¸€ä¸ªè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®ç°
        if self.attn_s:
            # ç¡®ä¿è¾“å…¥ç»´åº¦dim_inèƒ½å¤Ÿè¢«dim_headæ•´é™¤ï¼Œå¦åˆ™ä¼šå¼•å‘AssertionError
            assert dim_in % dim_head == 0, 'dim should be divisible by num_heads'
            # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
            self.dim_head = dim_head
            # çª—å£å¤§å°ï¼Œç”¨äºå±€éƒ¨æ³¨æ„åŠ›
            self.window_size = window_size
            # æ³¨æ„åŠ›å¤´çš„æ•°é‡
            self.num_head = dim_in // dim_head
            # ç¼©æ”¾å› å­ï¼Œç”¨äºè°ƒæ•´æ³¨æ„åŠ›æƒé‡
            self.scale = self.dim_head ** -0.5
            # æ˜¯å¦åœ¨æ³¨æ„åŠ›è®¡ç®—å‰åº”ç”¨é¢„å¤„ç†æ“ä½œ
            self.attn_pre = attn_pre
            # å®šä¹‰äº†qkæ¨¡å—ï¼Œä½¿ç”¨1x1å·ç§¯å°†è¾“å…¥dim_inè½¬æ¢ä¸ºç»´åº¦ä¸ºdim_in*2çš„ç‰¹å¾å›¾ï¼Œç”¨äºè®¡ç®—æ³¨æ„åŠ›æƒé‡
            self.qk = nn.Conv2d(dim_in, int(dim_in * 2), 1, bias=qkv_bias)
            # å®šä¹‰äº†væ¨¡å—ï¼Œä½¿ç”¨1x1å·ç§¯å°†è¾“å…¥dim_inè½¬æ¢ä¸ºç»´åº¦ä¸ºdim_midçš„ç‰¹å¾å›¾ï¼Œå¹¶åº”ç”¨æ¿€æ´»å‡½æ•°act
            self.v = nn.Sequential(
                nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                self.act
            )
            # å®šä¹‰äº†attn_dropæ¨¡å—ï¼Œç”¨äºè¿›è¡Œæ³¨æ„åŠ›æƒé‡çš„dropoutæ“ä½œ
            self.attn_drop = nn.Dropout(attn_drop)
        else:
            if v_proj:
                # è¿™ä¸ªå·ç§¯å±‚çš„è¾“å…¥ç»´åº¦æ˜¯ dim_inï¼Œè¾“å‡ºç»´åº¦æ˜¯ dim_midï¼Œå·ç§¯æ ¸çš„å¤§å°æ˜¯ 1x1ã€‚å¦‚æœè®¾ç½®äº† v_group å‚æ•°ï¼Œåˆ™å·ç§¯å±‚çš„åˆ†ç»„æ•°ä¸º self.num_headï¼Œå¦åˆ™ä¸º 1
                self.v = nn.Sequential(
                    nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                    self.act
                )
            else:
                self.v = nn.Identity()
        # ä¸€ä¸ªå±€éƒ¨å·ç§¯å±‚ï¼Œè¾“å…¥ç»´åº¦ä¸º dim_midï¼Œè¾“å‡ºç»´åº¦ä¹Ÿä¸º dim_midï¼Œå·ç§¯æ ¸å¤§å°ä¸º dw_ksï¼Œæ­¥é•¿ä¸º strideï¼Œæ‰©å¼ ï¼ˆdilationï¼‰ä¸º dilationï¼Œåˆ†ç»„æ•°ä¸º dim_mid
        self.conv_local = Conv(dim_mid, dim_mid, k=dw_ks, s=stride, d=dilation, g=dim_mid)
        # ä¸€ä¸ªSEAttentionæ¨¡å—ï¼Œç”¨äºæ‰§è¡ŒSEæ³¨æ„åŠ›æœºåˆ¶
        # æœ se_ratio å¤§äº0ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªå…·æœ‰é™ç»´æ¯”ä¾‹ reduction çš„SEAttentionæ¨¡å—ï¼Œå¦åˆ™è®¾ç½®ä¸º nn.Identity()
        self.se = SEAttention(dim_mid, reduction=se_ratio) if se_ratio > 0.0 else nn.Identity()
        # self.proj_drop æ˜¯ä¸€ä¸ªç”¨äºæŠ•å½±ï¼ˆprojectionï¼‰çš„dropoutå±‚ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
        self.proj_drop = nn.Dropout(drop)
        # self.proj æ˜¯ä¸€ä¸ªæŠ•å½±å·ç§¯å±‚ï¼Œå°†è¾“å…¥çš„ç‰¹å¾æ˜ å°„åˆ°ç›®æ ‡ç»´åº¦ dim_out
        self.proj = nn.Conv2d(dim_mid, dim_out, kernel_size=1)
        # æ˜¯ä¸€ä¸ªDropPathæ¨¡å—ï¼Œç”¨äºæ‰§è¡ŒéšæœºDropPathæ“ä½œã€‚å¦‚æœ drop_path å‚æ•°å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªDropPathæ¨¡å—ï¼Œå¦åˆ™è®¾ç½®ä¸º nn.Identity()
        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.norm(x)
        B, C, H, W = x.shape
        if self.attn_s:
            # padding
            # æ®çª—å£å¤§å°ï¼ˆwindow_sizeï¼‰å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œå¡«å……ï¼Œç¡®ä¿ç‰¹å¾å›¾å¤§å°èƒ½å¤Ÿè¢«çª—å£å¤§å°æ•´é™¤ã€‚
            # å¡«å……çš„æ•°é‡è®¡ç®—ä¸º (window_size - size % window_size) % window_sizeï¼Œå…¶ä¸­ size æ˜¯ç‰¹å¾å›¾çš„é«˜åº¦æˆ–å®½åº¦ã€‚
            if self.window_size <= 0:
                window_size_W, window_size_H = W, H
            else:
                window_size_W, window_size_H = self.window_size, self.window_size
            pad_l, pad_t = 0, 0
            pad_r = (window_size_W - W % window_size_W) % window_size_W
            pad_b = (window_size_H - H % window_size_H) % window_size_H
            x = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))
            n1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W
            x = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()
            # attention
            b, c, h, w = x.shape
            qk = self.qk(x)
            qk = rearrange(qk, 'b (qk heads dim_head) h w -> qk b heads (h w) dim_head', qk=2, heads=self.num_head,
                           dim_head=self.dim_head).contiguous()
            q, k = qk[0], qk[1]
            attn_spa = (q @ k.transpose(-2, -1)) * self.scale
            attn_spa = attn_spa.softmax(dim=-1)
            attn_spa = self.attn_drop(attn_spa)
            if self.attn_pre:
                x = rearrange(x, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ x
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
                x_spa = self.v(x_spa)
            else:
                v = self.v(x)
                v = rearrange(v, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ v
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
            # unpadding
            x = rearrange(x_spa, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()
            if pad_r > 0 or pad_b > 0:
                x = x[:, :, :H, :W].contiguous()
        else:
            x = self.v(x)

        x = x + self.se(self.conv_local(x)) if self.has_skip else self.se(self.conv_local(x))

        x = self.proj_drop(x)
        x = self.proj(x)

        x = (shortcut + self.drop_path(x)) if self.has_skip else x
        return x


class iRMB_Cascaded(nn.Module):
    def __init__(self, dim_in, dim_out, norm_in=True, has_skip=True, exp_ratio=1.0,
                 act=True, v_proj=True, dw_ks=3, stride=1, dilation=1, num_head=16, se_ratio=0.0,
                 attn_s=True, qkv_bias=False, drop=0., drop_path=0., v_group=False):
        super().__init__()
        self.norm = nn.BatchNorm2d(dim_in) if norm_in else nn.Identity()
        self.act = Conv.default_act if act else nn.Identity()
        dim_mid = int(dim_in * exp_ratio)
        self.has_skip = (dim_in == dim_out and stride == 1) and has_skip
        self.attn_s = attn_s
        self.num_head = num_head
        if self.attn_s:
            self.attn = LocalWindowAttention(dim_mid)
        else:
            if v_proj:
                self.v = nn.Sequential(
                    nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                    self.act
                )
            else:
                self.v = nn.Identity()
        self.conv_local = Conv(dim_mid, dim_mid, k=dw_ks, s=stride, d=dilation, g=dim_mid)
        self.se = SEAttention(dim_mid, reduction=se_ratio) if se_ratio > 0.0 else nn.Identity()

        self.proj_drop = nn.Dropout(drop)
        self.proj = nn.Conv2d(dim_mid, dim_out, kernel_size=1)
        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.norm(x)
        B, C, H, W = x.shape
        if self.attn_s:
            x = self.attn(x)
        else:
            x = self.v(x)

        x = x + self.se(self.conv_local(x)) if self.has_skip else self.se(self.conv_local(x))

        x = self.proj_drop(x)
        x = self.proj(x)

        x = (shortcut + self.drop_path(x)) if self.has_skip else x
        return x


class iRMB_DRB(nn.Module):
    def __init__(self, dim_in, dim_out, norm_in=True, has_skip=True, exp_ratio=1.0,
                 act=True, v_proj=True, dw_ks=3, stride=1, dilation=1, se_ratio=0.0, dim_head=16, window_size=7,
                 attn_s=True, qkv_bias=False, attn_drop=0., drop=0., drop_path=0., v_group=False, attn_pre=False):
        super().__init__()
        self.norm = nn.BatchNorm2d(dim_in) if norm_in else nn.Identity()
        self.act = Conv.default_act if act else nn.Identity()
        dim_mid = int(dim_in * exp_ratio)
        self.has_skip = (dim_in == dim_out and stride == 1) and has_skip
        self.attn_s = attn_s
        if self.attn_s:
            assert dim_in % dim_head == 0, 'dim should be divisible by num_heads'
            self.dim_head = dim_head
            self.window_size = window_size
            self.num_head = dim_in // dim_head
            self.scale = self.dim_head ** -0.5
            self.attn_pre = attn_pre
            self.qk = nn.Conv2d(dim_in, int(dim_in * 2), 1, bias=qkv_bias)
            self.v = nn.Sequential(
                nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                self.act
            )
            self.attn_drop = nn.Dropout(attn_drop)
        else:
            if v_proj:
                self.v = nn.Sequential(
                    nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                    self.act
                )
            else:
                self.v = nn.Identity()
        self.conv_local = DilatedReparamBlock(dim_mid, dw_ks)
        self.se = SEAttention(dim_mid, reduction=se_ratio) if se_ratio > 0.0 else nn.Identity()

        self.proj_drop = nn.Dropout(drop)
        self.proj = nn.Conv2d(dim_mid, dim_out, kernel_size=1)
        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.norm(x)
        B, C, H, W = x.shape
        if self.attn_s:
            # padding
            if self.window_size <= 0:
                window_size_W, window_size_H = W, H
            else:
                window_size_W, window_size_H = self.window_size, self.window_size
            pad_l, pad_t = 0, 0
            pad_r = (window_size_W - W % window_size_W) % window_size_W
            pad_b = (window_size_H - H % window_size_H) % window_size_H
            x = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))
            n1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W
            x = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()
            # attention
            b, c, h, w = x.shape
            qk = self.qk(x)
            qk = rearrange(qk, 'b (qk heads dim_head) h w -> qk b heads (h w) dim_head', qk=2, heads=self.num_head,
                           dim_head=self.dim_head).contiguous()
            q, k = qk[0], qk[1]
            attn_spa = (q @ k.transpose(-2, -1)) * self.scale
            attn_spa = attn_spa.softmax(dim=-1)
            attn_spa = self.attn_drop(attn_spa)
            if self.attn_pre:
                x = rearrange(x, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ x
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
                x_spa = self.v(x_spa)
            else:
                v = self.v(x)
                v = rearrange(v, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ v
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
            # unpadding
            x = rearrange(x_spa, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()
            if pad_r > 0 or pad_b > 0:
                x = x[:, :, :H, :W].contiguous()
        else:
            x = self.v(x)

        x = x + self.se(self.conv_local(x)) if self.has_skip else self.se(self.conv_local(x))

        x = self.proj_drop(x)
        x = self.proj(x)

        x = (shortcut + self.drop_path(x)) if self.has_skip else x
        return x


class iRMB_SWC(nn.Module):
    def __init__(self, dim_in, dim_out, norm_in=True, has_skip=True, exp_ratio=1.0,
                 act=True, v_proj=True, dw_ks=3, stride=1, dilation=1, se_ratio=0.0, dim_head=16, window_size=7,
                 attn_s=True, qkv_bias=False, attn_drop=0., drop=0., drop_path=0., v_group=False, attn_pre=False):
        super().__init__()
        self.norm = nn.BatchNorm2d(dim_in) if norm_in else nn.Identity()
        self.act = Conv.default_act if act else nn.Identity()
        dim_mid = int(dim_in * exp_ratio)
        self.has_skip = (dim_in == dim_out and stride == 1) and has_skip
        self.attn_s = attn_s
        if self.attn_s:
            assert dim_in % dim_head == 0, 'dim should be divisible by num_heads'
            self.dim_head = dim_head
            self.window_size = window_size
            self.num_head = dim_in // dim_head
            self.scale = self.dim_head ** -0.5
            self.attn_pre = attn_pre
            self.qk = nn.Conv2d(dim_in, int(dim_in * 2), 1, bias=qkv_bias)
            self.v = nn.Sequential(
                nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                self.act
            )
            self.attn_drop = nn.Dropout(attn_drop)
        else:
            if v_proj:
                self.v = nn.Sequential(
                    nn.Conv2d(dim_in, dim_mid, kernel_size=1, groups=self.num_head if v_group else 1, bias=qkv_bias),
                    self.act
                )
            else:
                self.v = nn.Identity()
        self.conv_local = ReparamLargeKernelConv(dim_mid, dim_mid, dw_ks, stride=stride, groups=(dim_mid // 16))
        self.se = SEAttention(dim_mid, reduction=se_ratio) if se_ratio > 0.0 else nn.Identity()

        self.proj_drop = nn.Dropout(drop)
        self.proj = nn.Conv2d(dim_mid, dim_out, kernel_size=1)
        self.drop_path = DropPath(drop_path) if drop_path else nn.Identity()

    def forward(self, x):
        shortcut = x
        x = self.norm(x)
        B, C, H, W = x.shape
        if self.attn_s:
            # padding
            if self.window_size <= 0:
                window_size_W, window_size_H = W, H
            else:
                window_size_W, window_size_H = self.window_size, self.window_size
            pad_l, pad_t = 0, 0
            pad_r = (window_size_W - W % window_size_W) % window_size_W
            pad_b = (window_size_H - H % window_size_H) % window_size_H
            x = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))
            n1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W
            x = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()
            # attention
            b, c, h, w = x.shape
            qk = self.qk(x)
            qk = rearrange(qk, 'b (qk heads dim_head) h w -> qk b heads (h w) dim_head', qk=2, heads=self.num_head,
                           dim_head=self.dim_head).contiguous()
            q, k = qk[0], qk[1]
            attn_spa = (q @ k.transpose(-2, -1)) * self.scale
            attn_spa = attn_spa.softmax(dim=-1)
            attn_spa = self.attn_drop(attn_spa)
            if self.attn_pre:
                x = rearrange(x, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ x
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
                x_spa = self.v(x_spa)
            else:
                v = self.v(x)
                v = rearrange(v, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head).contiguous()
                x_spa = attn_spa @ v
                x_spa = rearrange(x_spa, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, h=h,
                                  w=w).contiguous()
            # unpadding
            x = rearrange(x_spa, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()
            if pad_r > 0 or pad_b > 0:
                x = x[:, :, :H, :W].contiguous()
        else:
            x = self.v(x)

        x = x + self.se(self.conv_local(x)) if self.has_skip else self.se(self.conv_local(x))

        x = self.proj_drop(x)
        x = self.proj(x)

        x = (shortcut + self.drop_path(x)) if self.has_skip else x
        return x


class C3_iRMB(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(iRMB(c_, c_) for _ in range(n)))


class C2f_iRMB(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(iRMB(self.c, self.c) for _ in range(n))


class C3_iRMB_Cascaded(C3):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(iRMB_Cascaded(c_, c_) for _ in range(n)))


class C2f_iRMB_Cascaded(C2f):
    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(iRMB_Cascaded(self.c, self.c) for _ in range(n))


class C3_iRMB_DRB(C3):
    def __init__(self, c1, c2, n=1, kernel_size=None, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(iRMB_DRB(c_, c_, dw_ks=kernel_size) for _ in range(n)))


class C2f_iRMB_DRB(C2f):
    def __init__(self, c1, c2, n=1, kernel_size=None, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(iRMB_DRB(self.c, self.c, dw_ks=kernel_size) for _ in range(n))


class C3_iRMB_SWC(C3):
    def __init__(self, c1, c2, n=1, kernel_size=None, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(*(iRMB_SWC(c_, c_, dw_ks=kernel_size) for _ in range(n)))


class C2f_iRMB_SWC(C2f):
    def __init__(self, c1, c2, n=1, kernel_size=None, shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(iRMB_SWC(self.c, self.c, dw_ks=kernel_size) for _ in range(n))

######################################## iRMB and iRMB with CascadedGroupAttention and iRMB with DRB and iRMB with SWC end ########################################

######################################## iCGNet_GELAN begin ########################################

class FGlo(nn.Module):
    """
    the FGlo class is employed to refine the joint feature of both local feature and surrounding context.
    """

    def __init__(self, channel, reduction=16):
        super(FGlo, self).__init__()
        # å¯¹hå’Œwè¿›è¡Œå…¨å±€å¹³å‡æ± åŒ–
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        # å®šä¹‰FCæ¨¡å—
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction),
            nn.ReLU(inplace=True),
            nn.Linear(channel // reduction, channel),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        # è·å–é€šé“çš„æƒé‡
        y = self.avg_pool(x).view(b, c)
        # ä½¿ç”¨FCé™ç»´ï¼Œåœ¨å‡ç»´
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

# convæ›¿æ¢æˆoperaçš„ opera_ContextGuidedBlock
class opera_ContextGuidedBlock(nn.Module):
    def __init__(self, nIn, nOut, dilation_rate=2, reduction=16, add=True):
        super().__init__()
        n = int(nOut / 2)
        self.conv1x1 = Conv(nIn, n, 1, 1)  # 1x1 Conv is employed to reduce the computation
        # 4.7   opreaæ›¿æ¢æ™®é€šå·ç§¯
        self.F_loc = OREPA(n, n, 3, padding=1, groups=n)
        # 4.7   opreaæ›¿æ¢ç©ºæ´å·ç§¯
        self.F_sur = OREPA(n, n, 3, padding=autopad(3, None, dilation_rate), dilation=dilation_rate,
                               groups=n)  # surrounding context
        self.bn_act = nn.Sequential(
            nn.BatchNorm2d(nOut),
            Conv.default_act
        )
        self.add = add
        self.F_glo = FGlo(nOut, reduction)

    def forward(self, input):
        output = self.conv1x1(input)


        loc = self.F_loc(output)
        sur = self.F_sur(output)

        joi_feat = torch.cat([loc, sur], 1)

        joi_feat = self.bn_act(joi_feat)

        output = self.F_glo(joi_feat)  # F_glo is employed to refine the joint feature
        # if residual version
        if self.add:
            output = input + output
        return output


#
# èåˆ ContextGuidedBlock çš„GELAN å…ˆæ›¿æ¢RepNCSP
class CGNet_GELAN(nn.Module):
    # csp-elan  384 256 128 64 1
    def __init__(self, c1, c2, c3, c4):  # ch_in, ch_out, number, shortcut, groups, c5æ˜¯æŒ‡RepNCSPçš„é‡å¤æ¬¡æ•°
        super().__init__()
        self.c = c3//2
        self.cv1 = Conv(c1, c3, 1, 1)

        # åœ¨ç‰¹å¾é‡‘å­—å¡”ä¸­ c3/2 = c4
        # self.m = nn.Sequential(*(ContextGuidedBlock(c3 // 2, c4) for _ in range(c5)))
        self.cv2 = nn.Sequential(opera_ContextGuidedBlock(c3 // 2, c4), Conv(c4, c4, 3, 1))
        self.cv3 = nn.Sequential(opera_ContextGuidedBlock(c4, c4), Conv(c4, c4, 3, 1))

        #self.cv2 = OREPA(c_, c2, k[1], groups=g)
        #self.cv2 = nn.Sequential(RepNCSP(c3//2, c4, c5), Conv(c4, c4, 3, 1))
        #self.cv3 = nn.Sequential(RepNCSP(c4, c4, c5), Conv(c4, c4, 3, 1))
        self.cv4 = Conv(c3+(2*c4), c2, 1, 1)

    def forward(self, x):
        y = list(self.cv1(x).chunk(2, 1))
        # y[-1]å…ˆç»è¿‡self.cv2ï¼ˆç®—æ˜¯ä¸€ä¸ªé›†æˆçš„æ¨¡å—ï¼Œæ‰€ä»¥ç»è¿‡RepNCSPä¸ä¼šè¿æ¥åˆ°contactï¼Œåªæœ‰ç»è¿‡Convæ‰ä¼šcontactï¼‰ï¼Œå†ç»è¿‡self.cv3
        y.extend((m(y[-1])) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))

    def forward_split(self, x):
        y = list(self.cv1(x).split((self.c, self.c), 1))
        y.extend(m(y[-1]) for m in [self.cv2, self.cv3])
        return self.cv4(torch.cat(y, 1))


######################################## CGNet_GELAN end ########################################


######################################## MS-Block start ########################################

# å¯¹è¾“å…¥è¿›è¡Œä¸‰ä¸ªå·ç§¯çš„æ“ä½œ
class MSBlockLayer(nn.Module):
    def __init__(self, inc, ouc, k) -> None:   # inc : æ¨¡å—çš„è¾“å…¥ä¹Ÿæ˜¯è¾“å‡ºé€šé“æ•°  ouc: ä¸­é—´é€šé“æ•°ï¼Œåˆ†ç»„å·ç§¯çš„ç»„æ•°  kï¼šå·ç§¯æ ¸å¤§å°
        super().__init__()

        self.in_conv = Conv(inc, ouc, 1)
        self.mid_conv = Conv(ouc, ouc, k, g=ouc)
        self.out_conv = Conv(ouc, inc, 1)

    def forward(self, x):
        return self.out_conv(self.mid_conv(self.in_conv(x)))

#
class MSBlock(nn.Module):
    def __init__(self, inc, ouc, kernel_sizes, in_expand_ratio=3., mid_expand_ratio=2., layers_num=3,
                 in_down_ratio=2.) -> None:
        super().__init__()

        # å®šä¹‰ç¬¬ä¸€ä¸ªå‡ç»´å·ç§¯çš„è¾“å‡ºé€šé“æ•°
        in_channel = int(inc * in_expand_ratio // in_down_ratio)
        print(inc)
        # ä¸­é—´ç‰¹å¾å±‚çš„è¾“å…¥è¾“å‡ºé€šé“æ•°ï¼Œå› ä¸ºæ˜¯in_channel/3å¾—åˆ°çš„ï¼Œæ‰€ä»¥ä¸­é—´ç‰¹å¾å±‚æœ‰ä¸‰ä¸ªåˆ†æ”¯
        self.mid_channel = in_channel // len(kernel_sizes)
        groups = int(self.mid_channel * mid_expand_ratio)
        self.in_conv = Conv(inc, in_channel)

        self.mid_convs = []
        # ä¾‹å¦‚kernel_size=[1,3,3]
        for kernel_size in kernel_sizes:
            # kernel_size = 1æ˜¯æŒ‡å›¾ç‰‡ä¸­æœ€å·¦è¾¹çš„åˆ†æ”¯ï¼Œä¹Ÿå°±æ˜¯è‡ªå·±
            if kernel_size == 1:
                self.mid_convs.append(nn.Identity())
                # è¿™ä¸ªå…³é”®å­—å‘Šè¯‰ç¨‹åºè·³è¿‡å½“å‰å¾ªç¯çš„å‰©ä½™éƒ¨åˆ†ï¼Œç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªå¾ªç¯
                continue
            # layers_numï¼šMSBlockLayer çš„é‡å¤æ¬¡æ•°ï¼Œé»˜è®¤é‡å¤ä¸‰æ¬¡ï¼ˆæ„Ÿè§‰æœ‰ç‚¹å¤šï¼‰
            mid_convs = [MSBlockLayer(self.mid_channel, groups, k=kernel_size) for _ in range(int(layers_num))]
            # è¿™è¡Œä»£ç å°†ä¸­é—´å·ç§¯å±‚åˆ—è¡¨ mid_convs ä¸­çš„æ‰€æœ‰ MSBlockLayer å®ä¾‹ç»„åˆæˆä¸€ä¸ªåºåˆ—ï¼Œå¹¶å°†è¯¥åºåˆ—ä½œä¸ºä¸€ä¸ªæ•´ä½“æ·»åŠ åˆ° self.mid_convs åˆ—è¡¨ä¸­ã€‚
            # è¿™æ ·åšçš„ç›®çš„æ˜¯å°†å¤šä¸ªå·ç§¯å±‚ç»„åˆæˆä¸€ä¸ªæ›´å¤§çš„ç½‘ç»œå—ï¼Œä»¥ä¾¿åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ã€‚
            # nn.Sequential(*mid_convs) æ¥å—ä¸€ä¸ªç”±å¤šä¸ªæ¨¡å—ç»„æˆçš„åˆ—è¡¨ mid_convsï¼Œå¹¶æŒ‰é¡ºåºå°†å®ƒä»¬è¿æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªæ–°çš„åºåˆ—æ¨¡å—ã€‚
            # ç„¶åï¼Œè¯¥åºåˆ—æ¨¡å—è¢«æ·»åŠ åˆ° self.mid_convs åˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿åç»­åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
            self.mid_convs.append(nn.Sequential(*mid_convs))
        # å‡è®¾ self.mid_convs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé‡Œé¢åŒ…å«äº†ä¸€äº› nn.Module å¯¹è±¡ï¼Œé€šè¿‡å°†å®ƒè½¬æ¢ä¸º ModuleListï¼Œ
        # ä½ å°±å¯ä»¥å°†è¿™äº›æ¨¡å—çš„å‚æ•°çº³å…¥æ¨¡å‹çš„å‚æ•°ç®¡ç†ä¸­ï¼Œæ–¹ä¾¿åç»­çš„è®­ç»ƒå’Œä¼˜åŒ–è¿‡ç¨‹
        self.mid_convs = nn.ModuleList(self.mid_convs)
        self.out_conv = Conv(in_channel, ouc, 1)

        #å¹¶æ²¡æœ‰å®šä¹‰æ‰€ä½¿ç”¨çš„attention
        self.attention = None

    def forward(self, x):
        # è¿›è¡Œç©ºé—´ç»´åº¦çš„å‡ç»´
        out = self.in_conv(x)
        # channels æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æ¯ä¸ªé€šé“çš„ä¸­é—´ç‰¹å¾
        channels = []
        for i, mid_conv in enumerate(self.mid_convs):
            # åœ¨ç©ºé—´ç»´åº¦ä¸Šä»¥ self.mid_channel ä¸ºå¤§å°å¯¹è¾“å…¥è¿›è¡Œåˆ†éš”ï¼Œå¾—åˆ°x1ï¼Œx2ï¼Œx3
            channel = out[:, i * self.mid_channel:(i + 1) * self.mid_channel, ...]
            if i >= 1:
                # å½“å‰ä¸­é—´ç‰¹å¾å±‚ä¸å‰ä¸€ä¸­é—´ç‰¹å¾å±‚è¿›è¡Œç›¸åŠ ï¼Œå¾—åˆ°æ–°çš„ç‰¹å¾å±‚
                channel = channel + channels[i - 1]
            # å¯¹ç›¸åŠ å¾—åˆ°çš„ä¸­é—´ç‰¹å¾å±‚è¿›è¡Œå·ç§¯æ“ä½œ
            channel = mid_conv(channel)
            # è¿›è¡Œæ‹¼æ¥
            channels.append(channel)
        out = torch.cat(channels, dim=1)
        # é™ç»´
        out = self.out_conv(out)
        if self.attention is not None:
            out = self.attention(out)
        return out


class C3_MSBlock(C3):
    def __init__(self, c1, c2, n=1, kernel_sizes=[1, 3, 3], in_expand_ratio=3., mid_expand_ratio=2., layers_num=3,
                 in_down_ratio=2., shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        c_ = int(c2 * e)  # hidden channels
        self.m = nn.Sequential(
            *(MSBlock(c_, c_, kernel_sizes, in_expand_ratio, mid_expand_ratio, layers_num, in_down_ratio) for _ in
              range(n)))


class C2f_MSBlock(C2f):
    def __init__(self, c1, c2, n=1, kernel_sizes=[1, 3, 3], in_expand_ratio=3., mid_expand_ratio=2., layers_num=3,
                 in_down_ratio=2., shortcut=False, g=1, e=0.5):
        super().__init__(c1, c2, n, shortcut, g, e)
        self.m = nn.ModuleList(
            MSBlock(self.c, self.c, kernel_sizes, in_expand_ratio, mid_expand_ratio, layers_num, in_down_ratio) for _ in
            range(n))

######################################## MS-Block end ########################################


